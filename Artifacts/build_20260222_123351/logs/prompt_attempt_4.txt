You are a Swift/SwiftUI developer. A previous attempt to complete a task failed.

TESTING & DEBUGGING REFERENCE:
For building, testing, and debugging iOS/macOS apps, reference this workflow guide:
/Users/emmanuel/Dev/Tools/Eocon-Foundation-V1/.Foundation/Docs/swiftDocs/Testing/XCODEBUILD_MCP_WORKFLOW.md

This guide covers:
- XcodeBuild MCP server tools for programmatic Xcode interaction
- Building for simulator, booting simulators, installing/launching apps
- UI automation: screenshots, accessibility hierarchy, tap simulation
- Debugging UI issues (button taps, gestures, navigation)

=== ORIGINAL TASK ===
I have the following verification comments after thorough review and exploration of the codebase. Implement the comments by following the instructions in the comments verbatim.

---
## Comment 1: Tier1Engine.classify does not return the TierClassification result, causing a compile-time error.

In \`app/decodingOppression/decodingOppression/NLP/Tier1Engine.swift\` update \`classify(clause:)\` to return the \`TierClassification\` produced by \`clauseAnalyzer.analyze\` (e.g., \`return await clauseAnalyzer.analyze(clause: clause)\`).

### Referred Files
- /Users/emmanuel/Documents/Theory/Redefining_racism/app/decodingOppression/decodingOppression/NLP/Tier1Engine.swift
---
## Comment 2: Project build phases are empty, so sources and data (KeywordTaxonomies.json) are not included in the app bundle.

In \`app/decodingOppression/decodingOppression.xcodeproj/project.pbxproj\` add all app Swift sources (including the NLP files) to the \`PBXSourcesBuildPhase\` of the \`decodingOppression\` target and add resources such as \`app/decodingOppression/decodingOppression/Data/KeywordTaxonomies.json\` to the \`PBXResourcesBuildPhase\`. Ensure target membership is set so the app builds and the taxonomy file is copied into the bundle.

### Referred Files
- /Users/emmanuel/Documents/Theory/Redefining_racism/app/decodingOppression/decodingOppression.xcodeproj/project.pbxproj
- /Users/emmanuel/Documents/Theory/Redefining_racism/app/decodingOppression/decodingOppression/Data/KeywordTaxonomies.json
---
=== END ORIGINAL TASK ===

=== REFERENCE CONTEXT ===

=== SWIFT DOCUMENTATION ===

--- FILE: Create-practical-workflows-in-xcode-cloud.md ---
Romain: Hi, my name is Romain, and I'm an engineer working on Xcode Cloud. Xcode Cloud is a powerful tool, flexible enough to be integrated right into a team's development process. It can increase their productivity and help them deliver better apps to their customers. In this session, I'm going to talk about creating workflows in Xcode Cloud, based on situations you could meet in the real world. Teams come in all shapes and sizes, with diverse and unique development processes. To help us design some practical workflows, we'll imagine three hypothetical case studies. Each of these case study will resemble common situations that people might see when adopting Xcode Cloud.

We'll start by looking at a solo developer working on a single app. Then, we'll work our way up to a large team dealing with legacy code and a complex development process. In this session, we'll demonstrate some of the many options available to create and customize workflows that are well-suited to any team.

Let's start off with our first case study. Say we have a solo developer. They have one app that is available on both iOS and macOS. Most of their coding work is done on the main branch, where all the new code changes are pushed. Sure, they'll occasionally use a different branch when experimenting with new APIs and platform features, but for the most part, they use the main branch. Their code relies on a couple of dependencies and they chose Cocoapods to download and integrate them into their project. Lastly, they deploy builds of their app through TestFlight to some friends and family members who can test and provide feedback on the app. Every now and then, they manually release a new version of their app onto the App Store. This developer is a one-person show. They're managing everything from building their app, to distributing it on the App Store all by themselves. For them, simplicity is going to be key. They'll need something they can rely on and maintain. With Xcode Cloud, their entire process of pushing new code, building their app, and distributing to their testers can be achieved in one small, but mighty workflow. Before I dive into what this workflow would look like, let's pause and refresh on what an Xcode Cloud workflow is. For example, building your application, running tests, distributing to your testers, and so on. The "where" is the version of Xcode and the version of macOS you want to use, plus any other configuration such as environment variables. Together, they form the environment you want your workflow to run in. Finally, the "when" is when do you want these actions to happen.

Do you want it to start when you're pushing code to a specific branch? Or say, every day at 4:00 p.m.? Defining one or more start conditions sets the criteria for when you want a workflow to run. Great. With that now fresh in our memory, let's apply the "what", "where", and "when" to our solo developer's situation. I'm going to create an Xcode Cloud workflow they can use to automate all of their process. Here, I have the project already set up in Xcode Cloud. In the report navigator, I select the cloud icon and right click on my product name, then select "manage workflows".

This opens my workflow editor where I will click on the plus sign to open the menu, and select the first item, to create a new workflow for my app. In the name field, I will enter "CI Workflow". I'm going to skip the description field, but feel free to add details. Now, let's take a look at the environment section. Over in this section, I can see that the latest version of Xcode and the latest version of MacOS are selected by default. This looks good to me, so I won't make any changes there. Each new workflow comes with a default start condition. Let's take a look.

This start condition will create a build every time changes are pushed to the default branch; in this case, main. This is almost correct, but this solo developer wants to start builds when code is pushed to any branch, not just the main one. I will change the source branch option to "Any Branch".

The goal of this workflow is to build and distribute the application. We need an archive action, that prepares the build for distribution. I'm going to click on the plus side next to the Actions section and select Archive.

As you can see, the iOS platform is already selected, so I'm just going to select the "TestFlight and App Store" option in the Deployment Preparation section.

Now that our workflow will produce a build that can be distributed, I'm going to add a post-action that uploads the build to App Store Connect. To do that, I will click on the plus sign in the post actions section and select "TestFlight External Testing".

From there, I'm going to click on the plus side in the Groups section and select the "Friends and Family" group of testers.

And just like that, we're almost done. Our solo developer works on an application that targets iOS and macOS, so I want my workflow to archive and release the app on both platforms. To do that, I'm going to add another archive action.

Select the macOS platform and the TestFlight App Store deployment preparation.

Then, I'm going to add another TestFlight External Testing post-action.

Select the "Archive - macOS" artifact.

And choose the same group for testers.

Finally, I'm going to press Save to create the workflow.

We mentioned earlier that this developer uses Cocoapods to include the dependencies their app needs. Out of the box, Xcode Cloud has support for the Swift Package Manager. It is built right into Xcode. But, other dependency managers can also be used in Xcode Cloud. All it takes is a small amount of configuration. There is some documentation on how to use some of the more popular dependency managers. That documentation can be used as a guide on how to use others. For this solo developer, the documentation suggests using a post-clone custom script to install and run the Cocoapods tool.

Custom build scripts provide a way to run additional actions at specific points of an Xcode Cloud build. In this example, the post-clone script is run after all the source code has been cloned into the temporary build environment. We'll see another example of custom script in a later case study. And that is the solo developer's workflow, ready to go! The next time they push some code, an Xcode Cloud build will start, and the app will be archived for both iOS and macOS. Then, a new build will get into the hands of a group of friends and family, who will test and provide feedback on the app. Now, let's take it up a notch and look at our second use case: a medium sized team.

Let's imagine a team composed of developers, project managers, and QA engineers, all spread across the world. They build an iOS app, available on iPhone and iPad. Each developer works in their own branches. They use pull requests to merge their changes back into a branch named "beta". Their internal QA team installs and tests builds made from that specific branch to ensure the app and its features are working as intended.

When they want to release a new version of the app, they merge the beta branch into a release branch and push a new tag to mark the release. To catch bugs and avoid regression, the app is very well tested with both unit and UI tests. They use TestFlight to deploy the app to internal and external testers at different points during development. Finally, they communicate and collaborate on their work over Slack. This is a pretty common example. They're using their tools to work and collaborate in parallel, while holding themselves to a high quality bar. Their process can be implemented with three Xcode Cloud workflows. First, I will create a pull request workflow, that helps getting changes into the beta branch. Then, I will create a beta workflow, that gets internal builds into the hands of the QA team. Lastly, I will create a final workflow to release new versions of the app to external testers and onto the App Store. Let's look at each of the workflows, in that order. The team uses pull requests to manage incorporating all new code changes into their app. When a developer opens a pull request, their teammates review their code, and they run their tests to make sure the app is working as expected. This first workflow is to make sure that when a new pull request is opened, or an existing one is updated, the tests are run. Let's create this workflow. First, I'm going to click on the plus sign to create a new workflow for my product. In the name field, I will type "Pull Requests". For this workflow, the team wants Xcode Cloud to start builds when a pull request is opened, but only when it targets the "beta" branch. For each of those builds, they want to run the tests. Let's start by adding a new start condition. I will click on the plus sign in the start conditions section and select the "Pull Request Changes" item. By default, it will start a build for any branches, so in the target branch section, I will select "custom branches" Then I will click on the plus sign button and type in "beta".

We only require the one start condition, so I can go ahead and remove the "branch changes" one.

Now I will add a new action to run the tests. I will click on the plus sign next to the Actions section and select "Test".

The app targets both iOS and iPadOS, so the team wants to make sure that the tests pass on devices of different screen sizes and capabilities. In the destination section, I will select one small iPhone, The iPhone 13, one large iPhone, The iPhone 14 Pro Max...

…one small iPad, The iPad mini...

..and finally, you guessed it, one large iPad, the iPad Pro.

Then I'm going to press save.

Awesome. Now, after a successful build on Xcode Cloud, and hopefully an exhaustive code review from their teammates, the developer will be able to merge their pull requests, and get their changes into the beta branch. Conveniently, this brings us to our next workflow: The beta build workflow.

The beta branch is where all upcoming changes are put. When a developer merges a pull request, the QA team wants to get a build of the app with those new changes included so that they can perform some verification tests. This will be the basis of this new workflow, deploying a build to the QA team. Let's jump back into Xcode and create this workflow.

This beta workflow is a mix of the workflows that I previously created.

The team wants to release a build every time a change is merged into the "beta" branch. They need a workflow that runs the tests, archive the app, and uploads it to App Store Connect. To achieve that, let's create a new workflow.

Name it “Beta Release".

And update the start condition accordingly. I will select the existing Branch Changes start condition and change the branch from main to beta.

Then, I'm going to add an archive build action.

And select the "TestFlight Internal Testing" under deployment preparation.

Here, we're using internal distribution, since we don't want this build to be deployed to production by mistake. Now, I'm going to add a post build action, that uploads to App Store Connect.

This post action will distribute the application to an internal group of testers. I will add a TestFlight Internal Testing post action.

Click on the plus sign under the groups section, and select "QA Team." All right, we could stop here, but we don't want to risk deploying a broken build. As a safety net, we're going to run the tests as part of that workflow as well. I'm going to repeat the testing action from the pull request workflow. Once again, I'm going to select one small iPhone, one large iPhone, one small iPad...

…and one large iPad.

I can now press Save to finalize the creation of this workflow.

The beta build workflow gets builds into the hands of the QA team, but there is one more thing the team wants to do. They would like to use an alternative app icon for their beta build. That way, they can quickly determine between which of the builds are internal, and which are App Store ready. This is another perfect situation where Xcode Cloud's custom scripts can help.

Earlier, we saw a custom script that ran after the source code has been cloned. Here, we'll use a pre-build script to change the icon.

Using the environment variables available in a script, I'm going to make sure that we only swap the icons in the build phase of our beta workflow. If you want to learn more about how to achieve this, please refer to the session from WWDC21, "Customize your advanced Xcode Cloud workflows" where this exact use case is covered in details. We've only used two of the three types of custom scripts available. If you're wondering what other script you can use, and which environment variables are available to you, our documentation explains all of this in details. And that's it for the beta build workflow. Let's now look at the final workflow for this team, the release workflow. After a number of changes have landed in the beta branch and been verified by QA, the team will prepare a new release.

Their process requires one of the developers to merge the beta branch into a release branch, then create a tag to mark the release.

The name of the tag has to start with the word release, then a version. Once this is done, the apps get built, uploaded to App Store Connect, and distributed to a group of internal stakeholders, and some eager customers.

Our third and final workflow is very similar to the beta workflow, except that we want builds to be created when a new release tag has been pushed. Let's go back to Xcode and create this workflow. The steps required for this workflow are almost exactly the same as the one I created in the beta workflow. I could go through all the same steps, like creating the start condition, archiving the app and so on. Instead, I want to direct you to an Xcode Cloud feature that will allow us to duplicate the beta workflow. First, I'm going to right-click on the beta workflow, select duplicate, then rename the workflow from Beta to Release.

Then, I'm going to add a new start condition. So I will click on the plus sign in the start conditions section and select "Tag Changes".

Similar to the branch changes, I don't want to create builds every time a tag is pushed, but only when the name of the tag starts with the word "release." I'm going to select "Custom Tags" in the "tag" section. Enter "release/"...

...and select "tags beginning with release/" in the menu. With this start condition created, I will go back to the existing "branch changes" start condition and delete it. Now, let's move on to the existing Archive action. The beta workflow was created to deploy builds internally, specifically to the QA team. Here, the team wants to prepare a release for external testing and the App Store. In the Deployment Preparation section, I'm going to select the "Testflight and App Store" option. That said, we still want to deploy the build to an internal team of stakeholders. I'm going to select the existing post build action and remove the QA team Group.

Then, I'm going to click on the plus sign and select the "Executive Stakeholders" group. As a last step, I'm going to add another post action, but this time select TestFlight External Testing. I will click on the plus sign in the Post Actions section and select TestFlight External Testing. Then, I will click on the plus sign in the Groups section, and select the Early Adopters group.

Now, the release workflow is almost ready. We mentioned earlier that this team uses Slack to communicate and collaborate with each other. For this team, getting updates about their builds in Slack will match perfectly their development process. This is especially important if the build failed and cannot be released. Let's add a final step to our workflow. In the release workflow, I will click on the plus sign in the post actions section and select Notify. Xcode Cloud supports sending notifications via email and Slack. Here, I'm going to click on the plus sign under Slack and select the “Releases Feed" channel, then press OK. That concludes our second use case. Those three workflows cover all of the team's development process. They build the app and run tests continuously, allowing developers to contribute with confidence. They archive and distribute the app frequently, making sure that various groups of people are able to provide feedback. This is a fairly common situation, where teams can release apps of great quality with the help of tools that adapt to any processes. Let's further support this statement by looking at a third and final use case, in which a bigger team wants to migrate to Xcode Cloud. For our final case study, say we have a large team of developers. This team shares a lot of similarities with the medium-sized team we just looked at, with some twists. The team is bigger and the code base, much more complex. They have an app on iOS and iPadOS, which has been around since the beginning of the App Store. It has been redesigned and updated many times since then, and developers are dealing with a lot of legacy code and complexity, especially when doing QA. They have a lot of tests. Recently, they adopted a test-driven development approach and lots of new tests are being added with every new code change.

A lot of people are involved in successful updates to the app, so they often distribute new builds to various TestFlight groups, both internal and external, to gather feedback. The team includes many developers working around the world, using Slack to communicate and collaborate. But here is the interesting twist. They already rely on continuous integration and continuous deployment to get their work done. Currently, they use an in-house solution that one of the team member maintains and operates. Access to and knowledge of this system is limited, which makes issues hard to investigate and even harder to resolve. For these reasons, and more, they're considering switching to Xcode Cloud to replace this in-house system. Additionally, they use a project management tool, to track, coordinate, and prioritize the work they're doing. They have also created various dashboards and status pages. That way, people not directly involved in the development of the app can track the progress of the project. Xcode Cloud is a great fit for this kind of team, but migrating a project of this complexity onto a new CI system is hard and can feel overwhelming. In this situation, my recommendation is to break this migration down into different milestones. Each milestone involves moving a workload from the existing system into Xcode Cloud over a period of time. The main focus here is to allow a successful migration, while keeping the team productive and happy. Instead of looking at workflow configurations, let's look at what those milestones might be. My recommendation is to split this migration into distinct milestones. The first step is creating a workflow that builds a version of the app that can be released to the App Store. The second is to get tests working reliably. The third is establishing the remaining workflows that match and improve the team's development process. We'll look at each of these steps in details, beginning with creating a release workflow.

When migrating to Xcode Cloud, my recommendation is to start by creating a workflow that archives and uploads an App Store-ready build of the app. This is just like some of the example workflows we've already created and it can be achieved with zero disruption for the rest of the team. By starting there, you'll be able to use the cloud code signing feature built right into Xcode Cloud. No need to mess with certificates and provisioning profiles to sign your build. This is also a good way to see what is needed in order to build the app successfully with respect to dependencies and other configuration changes. Once this workflow is ready, it can be included into the team's regular development process as the part that creates your App Store-ready build. This is the first piece of work to have moved off of the existing system and onto Xcode Cloud. Next, its time to focus on getting tests working reliably. Testing can be very tricky in a continuous integration system. Often, tests will have been tailored to run in the CI environment that was being used at the time they were created. When running in a new CI environment, they may not run as reliably, causing builds to fail. During the creation of Xcode Cloud, we thought about this specific problem, and built a feature we believe can really help teams get their tests running smoothly. This feature makes Xcode Cloud ignore failures in some of the actions in a workflow and let the build finish.

To activate this feature, select the "Not Required To Pass" option under "requirement" in the workflow's action. By specifying an action as Not Required To Pass, the result of this action won't affect the final result of the Xcode Cloud build. The tests could fail, but the build will succeed nonetheless. You will still see a green checkmark in Xcode Cloud, and in the commit status in your Source Code Management for your build.

This is useful because it means you can continually run your tests off of the critical path. That way, you get to aggregate data to assess how they are performing and if they're reliable enough. Let's look at how to use this feature to get tests working in Xcode Cloud.

The team has quite a few tests, covering many aspects of their app. In this situation, my recommendation is to start by creating a new workflow that runs for all pull requests. In this workflow, a build action will run all the tests but be marked as Not Required To Pass. Whether the tests pass or fail won't block the pull request from being merged just yet. Right now, the idea is to assess how reliable the tests are. Let's remember, the tests are still running in the existing solution, so the pull requests are still merged with confidence, after a week or so. The team can look at the test result data from their pull request builds and see which tests have been passing reliably in Xcode Cloud. These tests can be moved into a new Test Plan, called Reliable Tests. Then, the team can edit the existing pull request workflow and add a new test action that will run the tests from that specific test plan. This time, the test action will be required to pass before the pull request can be merged. For more informations about using test plans on Xcode Cloud, you can refer to the "Author fast and reliable tests for Xcode Cloud" from WWDC22. You can also refer to our documentation on improving code assessments using test plans. The remaining tests, the ones that are not currently performing reliably, can be further investigated to figure out what changes need to be made to make them reliable. As time goes on and changes are made, those tests can be trusted again. They can finally be moved into the reliable test plan and be used to validate changes again. This approach allows you to move your tests to the critical path, step by step, and provide validation and test coverage in your Xcode Cloud workflows.

Once you're happy the tests are running, you now have App Store-ready builds and tests, running reliably in Xcode Cloud. Those two workloads can be removed from the existing CI solution. We are now left with the third and final step: building out the rest of the workflows needed for the team's development process. Some of those workflows will be similar to what we've seen already in the case studies in this session. Through the various customizations you can make to start conditions and actions, you can create some really powerful workflows to help automating your CI and CD process. We also mentioned this team has created tools and dashboards outside their CI system. Those tools help them keep on top of their development process, and can also be integrated with Xcode Cloud. For example, you can use the webhook feature. After a webhook as been configured, whenever a build completes, a request will be sent to your server with information about the build, the workflow that started it, and so on. From there, if the build was created from the beta workflow and it succeeded, you could create a new ticket in your task management system, to track the QA process on this specific build.

If you wish to learn more about webhooks and specifically when those requests are sent and what information are available to you, you can refer to our documentation. Another approach is to use the Xcode Cloud public API. It allows you to fetch information about the recent builds, among other things, and display them on a dashboard or a status page.

Once again, you can refer to the documentation to learn how to use the Xcode Cloud's public API and integrate it into your workflows. Xcode Cloud's public API and the webhook mechanism are features that are extra useful for teams, no matter their size. When combining all the options available, the possibilities are endless. You can refer to the session from WWDC22, "Deep dive into Xcode Cloud for teams" for more examples. In this session, we looked at various types of simple but powerful workflows that can be created to help your team be more productive. We showed some examples on how to customize your build process using build scripts at various points in the build. Finally, we showed that some of the features allow you to build tools on top of Xcode Cloud and integrate with external tools. We hope that these three cases studies helped you realize how Xcode Cloud can fit your team and improve your day-to-day work. Thank you for watching.

## Pre-build script that replaces the app icon for beta builds
```
#!/bin/sh
# ci_pre_xcodebuild.sh
#

if [[ "$CI_XCODEBUILD_ACTION" == "archive" && "$CI_WORKFLOW" == "Beta" ]]; then
    echo "Replacing app icon with beta icon"
    mv BetaAppIcon.appiconset ../App/Assets.xcassets/AppIcon.appiconset
fi
```
--- END FILE ---

--- FILE: testing-asynchronous-code.md ---
# Testing asynchronous code

<!--
This source file is part of the Swift.org open source project

Copyright (c) 2024 Apple Inc. and the Swift project authors
Licensed under Apache License v2.0 with Runtime Library Exception

See https://swift.org/LICENSE.txt for license information
See https://swift.org/CONTRIBUTORS.txt for Swift project authors
-->

Validate whether your code causes expected events to happen.

## Overview

The testing library integrates with Swift concurrency, meaning that in many
situations you can test asynchronous code using standard Swift
features.  Mark your test function as `async` and, in the function
body, `await` any asynchronous interactions:

```swift
@Test func priceLookupYieldsExpectedValue() async {
  let mozarellaPrice = await unitPrice(for: .mozarella)
  #expect(mozarellaPrice == 3)
}
```

In more complex situations you can use ``Confirmation`` to discover whether an
expected event happens.

### Confirm that an event happens

Call ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-5mqz2`` in your
asynchronous test function to create a `Confirmation` for the expected event. In
the trailing closure parameter, call the code under test. Swift Testing passes a
`Confirmation` as the parameter to the closure, which you call as a function in
the event handler for the code under test when the event you're testing for
occurs:

```swift
@Test("OrderCalculator successfully calculates subtotal for no pizzas")
func subtotalForNoPizzas() async {
  let calculator = OrderCalculator()
  await confirmation() { confirmation in
    calculator.successHandler = { _ in confirmation() }
    _ = await calculator.subtotal(for: PizzaToppings(bases: []))
  }
}
```

If you expect the event to happen more than once, set the
`expectedCount` parameter to the number of expected occurrences. The
test passes if the number of occurrences during the test matches the
expected count, and fails otherwise.

You can also pass a range to ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-l3il``
if the exact number of times the event occurs may change over time or is random:

```swift
@Test("Customers bought sandwiches")
func boughtSandwiches() async {
  await confirmation(expectedCount: 0 ..< 1000) { boughtSandwich in
    var foodTruck = FoodTruck()
    foodTruck.orderHandler = { order in
      if order.contains(.sandwich) {
        boughtSandwich()
      }
    }
    await FoodTruck.operate()
  }
}
```

In this example, there may be zero customers or up to (but not including) 1,000
customers who order sandwiches. Any [range expression](https://developer.apple.com/documentation/swift/rangeexpression)
which includes an explicit lower bound can be used:

| Range Expression | Usage |
|-|-|
| `1...` | If an event must occur _at least_ once |
| `5...` | If an event must occur _at least_ five times |
| `1 ... 5` | If an event must occur at least once, but not more than five times |
| `0 ..< 100` | If an event may or may not occur, but _must not_ occur more than 99 times |

### Confirm that an event doesn't happen

To validate that a particular event doesn't occur during a test,
create a `Confirmation` with an expected count of `0`:

```swift
@Test func orderCalculatorEncountersNoErrors() async {
  let calculator = OrderCalculator()
  await confirmation(expectedCount: 0) { confirmation in
    calculator.errorHandler = { _ in confirmation() }
    calculator.subtotal(for: PizzaToppings(bases: []))
  }
}
```
--- END FILE ---

--- FILE: Natural-Language.md ---
# Natural Language

**Analyze natural language text and deduce its language-specific metadata.**

## Availability

- **iOS** 12.0+
- **iPadOS** 12.0+
- **Mac Catalyst** 13.0+
- **macOS** 10.14+
- **tvOS** 12.0+
- **visionOS** 1.0+
- **watchOS** 5.0+


## Overview

The Natural Language framework provides a variety of natural language processing (NLP) functionality with support for many different languages and scripts. Use this framework to segment natural language text into paragraphs, sentences, or words, and tag information about those segments, such as part of speech, lexical class, lemma, script, and language.

![Image](media-3597579)

Use this framework to perform tasks like:

- *Language identification*, automatically detecting the language of a piece of text.

- *Tokenization*, breaking up a piece of text into linguistic units or tokens.

- *Parts-of-speech tagging*, marking up individual words with their part of speech.

- *Lemmatization*, deducing a word’s stem based on its morphological analysis.

- *Named entity recognition*, identifying tokens as names of people, places, or organizations.

You can also use this framework with Create ML to train and deploy custom natural language models. For more information, see [doc://com.apple.documentation/documentation/CreateML/creating-a-text-classifier-model](https://developer.apple.com/documentation/CreateML/creating-a-text-classifier-model) and [doc://com.apple.naturallanguage/documentation/NaturalLanguage/creating-a-word-tagger-model](https://developer.apple.com/documentation/NaturalLanguage/creating-a-word-tagger-model).

## Topics

### Tokenization

- [Tokenizing natural language text](https://developer.apple.com/documentation/naturallanguage/tokenizing-natural-language-text) — Enumerate the words in a string.
- [NLTokenizer](https://developer.apple.com/documentation/naturallanguage/nltokenizer) — A tokenizer that segments natural language text into semantic units.
### Language identification

- [Identifying the language in text](https://developer.apple.com/documentation/naturallanguage/identifying-the-language-in-text) — Detect the language in a piece of text by using a language recognizer.
- [NLLanguageRecognizer](https://developer.apple.com/documentation/naturallanguage/nllanguagerecognizer) — The language of a body of text.
- [NLLanguage](https://developer.apple.com/documentation/naturallanguage/nllanguage) — The languages that the Natural Language framework supports.
### Linguistic tags

- [Identifying parts of speech](https://developer.apple.com/documentation/naturallanguage/identifying-parts-of-speech) — Classify nouns, verbs, adjectives, and other parts of speech in a string.
- [Identifying people, places, and organizations](https://developer.apple.com/documentation/naturallanguage/identifying-people-places-and-organizations) — Use a linguistic tagger to perform named entity recognition on a string.
- [NLTagger](https://developer.apple.com/documentation/naturallanguage/nltagger) — A tagger that analyzes natural language text.
### Text embedding

- [Finding similarities between pieces of text](https://developer.apple.com/documentation/naturallanguage/finding-similarities-between-pieces-of-text) — Calculate the semantic distance between words or sentences.
- [NLEmbedding](https://developer.apple.com/documentation/naturallanguage/nlembedding) — A map of strings to vectors, which locates neighboring, similar strings.
### Contextual embedding

- [NLContextualEmbedding](https://developer.apple.com/documentation/naturallanguage/nlcontextualembedding) — A model that computes sequences of embedding vectors for natural language utterances.
- [NLContextualEmbeddingKey](https://developer.apple.com/documentation/naturallanguage/nlcontextualembeddingkey) — Contextual embedding keys.
- [NLScript](https://developer.apple.com/documentation/naturallanguage/nlscript) — The writing scripts that the Natural Language framework supports.
### Natural language models

- [Creating a text classifier model](https://developer.apple.com/documentation/CreateML/creating-a-text-classifier-model) — Train a machine learning model to classify natural language text.
- [Creating a word tagger model](https://developer.apple.com/documentation/naturallanguage/creating-a-word-tagger-model) — Train a machine learning model to tag individual words in natural language text.
- [NLModel](https://developer.apple.com/documentation/naturallanguage/nlmodel) — A custom model trained to classify or tag natural language text.

---

*Source: [https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage](https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage)*
--- END FILE ---

--- FILE: Creating-a-text-classifier-model.md ---
# Creating a text classifier model

**Train a machine learning model to classify natural language text.**


## Overview

A text classifier is a machine learning model that’s been trained to recognize patterns in natural language text, like the sentiment expressed by a sentence.

![Image](creating-a-text-classifer-model-1)

You train a text classifier by showing it lots of examples of text you’ve already labeled—for example, movie reviews that you’ve already labeled as positive, negative, or neutral.

![Image](creating-a-text-classifer-model-2)


### Import your data

Start by gathering textual data and importing it into an [doc://com.apple.createml/documentation/CreateML/MLDataTable](https://developer.apple.com/documentation/CreateML/MLDataTable) instance. You can create a data table from JSON and CSV formats. Or, if your textual data is in a collection of files, you can sort them into folders, using the folder names as labels, similar to the image data source used in [doc://com.apple.createml/documentation/CreateML/creating-an-image-classifier-model](https://developer.apple.com/documentation/CreateML/creating-an-image-classifier-model).

As an example, consider a JSON file containing movie reviews that you’ve categorized by sentiment. Each entry contains a pair of keys, the `text` and the `label`. The values of those keys are the input samples used to train your model. The JSON snippet below shows three pairs of sentences with their sentiment labels.

```javascript
// JSON file
[
    {
        "text": "The movie was fantastic!",
        "label": "positive"
    }, {
        "text": "Very boring. Fell asleep.",
        "label": "negative"
    }, {
        "text": "It was just OK.",
        "label": "neutral"
    } ...
]
```

In a macOS playground, create a data frame using the [doc://com.apple.documentation/documentation/TabularData](https://developer.apple.com/documentation/TabularData) framework.

```swift
import TabularData

let data = try DataFrame(contentsOfJSONFile: URL(fileURLWithPath: "<#/path/to/read/data.json#>"))
```

The resulting data frame has two columns, named *text* and *label*, derived from the keys in the JSON file. The column names can be anything, as long as they are meaningful to you, because you’ll use them as parameters in other methods.


### Prepare your data for training and evaluation

The data you use to train your model needs to be different from the data you use to evaluate your model. Use the [doc://com.apple.documentation/documentation/TabularData/DataFrameProtocol/stratifiedSplit(on:by:randomSeed:)-9iauf](https://developer.apple.com/documentation/TabularData/DataFrameProtocol/stratifiedSplit(on:by:randomSeed:)-9iauf) method to split your data into two data frames, one for training and the other for testing. The training data frame contains the majority of your data, and the testing data frame contains the remaining 20 percent.

```swift
let (trainingData, testingData) = data.stratifiedSplit(on: "text", by: 0.8)
```


### Choose model training parameters

You can use model training parameters to control the learning process. Choose a classifier algorithm type by specifying the [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType) parameter. The [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/maxEnt(revision:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/maxEnt(revision:)) algorithm trains quickly and performs well for a wide variety of data. It’s a good starting point algorithm while you’re exploring your data and models.

You can choose a transfer learning algorithm, [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/transferLearning(_:revision:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/transferLearning(_:revision:)).  Transfer learning models use a pre-trained model as a feature extractor, you specify the [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/FeatureExtractorType](https://developer.apple.com/documentation/CreateML/MLTextClassifier/FeatureExtractorType). Transfer learning models can take longer to train, but can improve accuracy because the baseline model has already been trained on a large amount of text in a specific language.

If your data contains multiple languages, choose either the maximum entropy algorithm [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/maxEnt(revision:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/maxEnt(revision:)) or the transfer learning algorithm [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/transferLearning(_:revision:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/transferLearning(_:revision:)) and set its [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/FeatureExtractorType](https://developer.apple.com/documentation/CreateML/MLTextClassifier/FeatureExtractorType) to the Bidirectional Encoder Representations from Transformers (BERT) embedding feature extractor [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/FeatureExtractorType/bertEmbedding](https://developer.apple.com/documentation/CreateML/MLTextClassifier/FeatureExtractorType/bertEmbedding).

If your data contains a single language, use the conditional random fields algorithm [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/crf(revision:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/crf(revision:)) or the transfer learning algorithm [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/transferLearning(_:revision:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType/transferLearning(_:revision:)) and set its [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/FeatureExtractorType](https://developer.apple.com/documentation/CreateML/MLTextClassifier/FeatureExtractorType) to the Embeddings from Language Models (ELMo) embedding feature extractor [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/FeatureExtractorType/elmoEmbedding](https://developer.apple.com/documentation/CreateML/MLTextClassifier/FeatureExtractorType/elmoEmbedding).

Use the [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelParameters-swift.struct/ValidationData-swift.enum](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelParameters-swift.struct/ValidationData-swift.enum) parameter to specify the evaluation data that’s held out from training your model. During the training process, use the validation data to estimate your model’s ability to correctly classify new examples. Depending on the validation accuracy, the classifier algorithm might adjust values within the model — or stop the training process, if the accuracy is high enough. Since the split of your data is random, you might get a different result each time you train your model.

```swift
let parameters = MLTextClassifier.ModelParameters(
    validation: .split(strategy: .automatic),
    algorithm: .transferLearning(.bertEmbedding, revision: 1),
    language: .english
)
```


### Create and train a text classifier

Create an instance of [doc://com.apple.createml/documentation/CreateML/MLTextClassifier](https://developer.apple.com/documentation/CreateML/MLTextClassifier) with your training data frame and the column names.

```swift
let sentimentClassifier = try MLTextClassifier(
    trainingData: trainingData,
    textColumn: "text",
    labelColumn: "label",
    parameters: parameters
)
```

To measure how accurately the model (`sentimentClassifier`) performs on the training and validation data, use the [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics/classificationError](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics/classificationError) properties of the model’s [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/trainingMetrics](https://developer.apple.com/documentation/CreateML/MLTextClassifier/trainingMetrics) and [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/validationMetrics](https://developer.apple.com/documentation/CreateML/MLTextClassifier/validationMetrics) properties.

```swift
// Calculate training accuracy as a percentage.
let trainingAccuracy = (1.0 - sentimentClassifier.trainingMetrics.classificationError) * 100

// Calculate validation accuracy as a percentage.
let validationAccuracy = (1.0 - sentimentClassifier.validationMetrics.classificationError) * 100
```


### Evaluate a classifier’s accuracy

Next, evaluate your trained model’s performance by testing it against sentences it’s never seen before. Pass your testing data frame to the [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/evaluation(on:)-8ch4k](https://developer.apple.com/documentation/CreateML/MLTextClassifier/evaluation(on:)-8ch4k) method, which returns an [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics) instance.

```swift
let evaluationMetrics = sentimentClassifier.evaluation(on: testingData, textColumn: "text", labelColumn: "label")
```

To get the evaluation accuracy, use the [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics/classificationError](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics/classificationError) property of the returned [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics) instance.

```swift
// Calculate evaluation accuracy as a percentage.
let evaluationAccuracy = (1.0 - evaluationMetrics.classificationError) * 100
```

If the evaluation performance isn’t good enough, you may need to retrain with more data or make other adjustments. For information about improving model performance, see [doc://com.apple.createml/documentation/CreateML/improving-your-model-s-accuracy](https://developer.apple.com/documentation/CreateML/improving-your-model-s-accuracy).


### Save a Core ML model

When your model is performing well enough, you’re ready to save it so you can use it in your app. Use the [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/write(to:metadata:)](https://developer.apple.com/documentation/CreateML/MLTextClassifier/write(to:metadata:)) method to write the Core ML model file to disk. Provide any information about the model, like its author, version, or description in an [doc://com.apple.createml/documentation/CreateML/MLModelMetadata](https://developer.apple.com/documentation/CreateML/MLModelMetadata) instance.

```swift
let metadata = MLModelMetadata(author: "John Appleseed",
                               shortDescription: "A model trained to classify movie review sentiment",
                               version: "1.0")

try sentimentClassifier.write(to: URL(fileURLWithPath: "<#/path/to/save/SentimentClassifier.mlmodel#>"),
                              metadata: metadata)
```

Specify the file name using the `fileURLWithPath:` parameter, in the above code, `SentimentClassifier.mlmodel`.


### Add a Core ML model to your app

With your app open in Xcode, drag the `SentimentClassifier.mlmodel` file into the navigation pane. Xcode compiles the model and generates a `SentimentClassifier` class for use in your app. Select the `SentimentClassifier.mlmodel` file in Xcode to view additional information about the model.

Create an [doc://com.apple.documentation/documentation/NaturalLanguage/NLModel](https://developer.apple.com/documentation/NaturalLanguage/NLModel) in the Natural Language framework from the `SentimentClassifier` to ensure that the tokenization is consistent between training and deployment. Then use [doc://com.apple.documentation/documentation/NaturalLanguage/NLModel/predictedLabel(for:)](https://developer.apple.com/documentation/NaturalLanguage/NLModel/predictedLabel(for:)) to generate predictions on new text inputs.

```swift
import NaturalLanguage
import CoreML

let mlModel = try SentimentClassifier(configuration: MLModelConfiguration()).model

let sentimentPredictor = try NLModel(mlModel: mlModel)
sentimentPredictor.predictedLabel(for: "It was the best I've ever seen!")
```

---

*Source: [https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/creating-a-text-classifier-model](https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/creating-a-text-classifier-model)*
--- END FILE ---

--- FILE: NLTagger.md ---
# NLTagger

**A tagger that analyzes natural language text.**

## Availability

- **iOS** 12.0+
- **iPadOS** 12.0+
- **Mac Catalyst** 13.1+
- **macOS** 10.14+
- **tvOS** 12.0+
- **visionOS** 1.0+
- **watchOS** 5.0+


## Overview

[doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTagger](https://developer.apple.com/documentation/NaturalLanguage/NLTagger) supports many different languages and scripts. Use it to segment natural language text into paragraph, sentence, or word units and to tag each unit with information like part of speech, lexical class, lemma, script, and language.

When you create a linguistic tagger, you specify what kind of information you’re interested in by passing one or more [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTagScheme](https://developer.apple.com/documentation/NaturalLanguage/NLTagScheme) values. Set the [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTagger/string](https://developer.apple.com/documentation/NaturalLanguage/NLTagger/string) property to the natural language text you want to analyze, and the linguistic tagger processes it according to the specified tag schemes. You can then enumerate over the tags in a specified range, using the methods described in Enumerating linguistic tags, to get the information requested for a given scheme and unit.


> **IMPORTANT**:  Don’t use an instance of [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTagger](https://developer.apple.com/documentation/NaturalLanguage/NLTagger) simultaneously from multiple threads.


## Topics

### Creating a tagger

- [init(tagSchemes:)](https://developer.apple.com/documentation/naturallanguage/nltagger/init(tagschemes:)) — Creates a linguistic tagger instance using the specified tag schemes and options.
- [string](https://developer.apple.com/documentation/naturallanguage/nltagger/string) — The string being analyzed by the linguistic tagger.
### Getting the tag schemes

- [availableTagSchemes(for:language:)](https://developer.apple.com/documentation/naturallanguage/nltagger/availabletagschemes(for:language:)) — Retrieves the tag schemes available for a particular unit (like word or sentence) and language on the current device.
- [requestAssets(for:tagScheme:completionHandler:)](https://developer.apple.com/documentation/naturallanguage/nltagger/requestassets(for:tagscheme:completionhandler:)) — Asks the Natural Language framework to load any missing assets for a tag scheme onto the device for the given language.
- [NLTagger.AssetsResult](https://developer.apple.com/documentation/naturallanguage/nltagger/assetsresult) — The response to an asset request.
- [tagSchemes](https://developer.apple.com/documentation/naturallanguage/nltagger/tagschemes) — The tag schemes configured for this linguistic tagger.
- [NLTagScheme](https://developer.apple.com/documentation/naturallanguage/nltagscheme) — Constants for the tag schemes specified when initializing a linguistic tagger.
### Determining the dominant language and orthography

- [dominantLanguage](https://developer.apple.com/documentation/naturallanguage/nltagger/dominantlanguage) — The dominant language of the string set for the linguistic tagger.
- [setLanguage(_:range:)](https://developer.apple.com/documentation/naturallanguage/nltagger/setlanguage(_:range:)) — Sets the language for a range of text within the tagger’s string.
- [setOrthography(_:range:)](https://developer.apple.com/documentation/naturallanguage/nltagger/setorthography(_:range:)) — Sets the orthography for the specified range.
### Enumerating linguistic tags

- [enumerateTags(in:unit:scheme:options:using:)](https://developer.apple.com/documentation/naturallanguage/nltagger/enumeratetags(in:unit:scheme:options:using:)) — Enumerates a block over the tagger’s string, given a range, token unit, and tag scheme.
- [NLTagger.Options](https://developer.apple.com/documentation/naturallanguage/nltagger/options) — Constants for linguistic tagger enumeration specifying which tokens to omit and whether to join names.
- [NLTag](https://developer.apple.com/documentation/naturallanguage/nltag) — A token type, lexical class, name, lemma, language, or script returned by a linguistic tagger for natural language text.
### Getting linguistic tags

- [tags(in:unit:scheme:options:)](https://developer.apple.com/documentation/naturallanguage/nltagger/tags(in:unit:scheme:options:)) — Finds an array of linguistic tags and token ranges for a given string range and linguistic unit.
- [tag(at:unit:scheme:)](https://developer.apple.com/documentation/naturallanguage/nltagger/tag(at:unit:scheme:)) — Finds a tag for a given linguistic unit, for a single scheme, at the specified character position.
- [tagHypotheses(at:unit:scheme:maximumCount:)](https://developer.apple.com/documentation/naturallanguage/nltagger/taghypotheses(at:unit:scheme:maximumcount:)) — Finds multiple possible tags for a given linguistic unit, for a single scheme, at the specified character position.
### Determining the range of a unit token

- [tokenRange(at:unit:)](https://developer.apple.com/documentation/naturallanguage/nltagger/tokenrange(at:unit:)) — Returns the range of the linguistic unit containing the specified character index.
- [tokenRange(for:unit:)](https://developer.apple.com/documentation/naturallanguage/nltagger/tokenrange(for:unit:)) — Finds the entire range of all tokens of the specified linguistic unit contained completely or partially within the specified range.
- [NLTokenUnit](https://developer.apple.com/documentation/naturallanguage/nltokenunit) — Constants representing linguistic units.
### Using models with a tagger

- [setModels(_:forTagScheme:)](https://developer.apple.com/documentation/naturallanguage/nltagger/setmodels(_:fortagscheme:)) — Assigns models for a tag scheme.
- [models(forTagScheme:)](https://developer.apple.com/documentation/naturallanguage/nltagger/models(fortagscheme:)) — Returns the models that apply to the given tag scheme.
### Using gazetteers with a tagger

- [setGazetteers(_:for:)](https://developer.apple.com/documentation/naturallanguage/nltagger/setgazetteers(_:for:)) — Attaches gazetteers to a tag scheme, typically one gazetteer per language or one language-independent gazetteer.
- [gazetteers(for:)](https://developer.apple.com/documentation/naturallanguage/nltagger/gazetteers(for:)) — Retrieves the gazetteers attached to a tag scheme.
- [NLGazetteer](https://developer.apple.com/documentation/naturallanguage/nlgazetteer) — A collection of terms and their labels, which take precedence over a word tagger.

---

*Source: [https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage/NLTagger](https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage/NLTagger)*
--- END FILE ---

--- FILE: Tokenizing-natural-language-text.md ---
# Tokenizing natural language text

**Enumerate the words in a string.**


## Overview

When you work with natural language text, it’s often useful to tokenize the text into individual words. Using [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer) to enumerate words, rather than simply splitting components by whitespace, ensures correct behavior in multiple scripts and languages. For example, neither Chinese nor Japanese uses spaces to delimit words.

The example and accompanying steps below show how you use [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer) to enumerate over the words in natural language text.

```swift
let text = """
All human beings are born free and equal in dignity and rights.
They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.
"""

let tokenizer = NLTokenizer(unit: .word)
tokenizer.string = text

tokenizer.enumerateTokens(in: text.startIndex..<text.endIndex) { tokenRange, _ in
    print(text[tokenRange])
    return true
}
```

1. Create an instance of [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer), specifying [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenUnit/word](https://developer.apple.com/documentation/NaturalLanguage/NLTokenUnit/word) as the unit to tokenize.

2. Set the [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer/string](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer/string) property of the tokenizer to the natural language text.

3. Enumerate over the entire range of the string by calling the [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer/enumerateTokensInRange:usingBlock:](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer/enumerateTokensInRange:usingBlock:) method, specifying the entire range of the string to process.

4. In the enumeration block, take a substring of the original text at `tokenRange` to obtain each word.

5. Run this code to print out each word in text on a new line.

---

*Source: [https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage/tokenizing-natural-language-text](https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage/tokenizing-natural-language-text)*
--- END FILE ---

--- FILE: Technologies.md ---
# Technologies


---

*Source: [https://developer.apple.com/documentation/com.apple.welcome-experience-documentation/documentation/technologies](https://developer.apple.com/documentation/com.apple.welcome-experience-documentation/documentation/technologies)*
--- END FILE ---

--- FILE: generating-content-and-performing-tasks.md ---
# Generating content and performing tasks with Foundation Models

**Enhance the experience in your app by prompting an on-device large language model.**


## Overview

The Foundation Models framework lets you tap into the on-device large models at the core of Apple Intelligence. You can enhance your app by using generative models to create content or perform tasks. The framework supports language understanding and generation based on model capabilities.

For design guidance, see Human Interface Guidelines > Technologies > [https://developer.apple.com/design/human-interface-guidelines/generative-ai](https://developer.apple.com/design/human-interface-guidelines/generative-ai).


## Understand model capabilities

When considering features for your app, it helps to know what the on-device language model can do. The on-device model supports text generation and understanding that you can use to:

The on-device language model may not be suitable for handling all requests, like:

The model can complete complex generative tasks when you use guided generation or tool calling. For more on handling complex tasks, or tasks that require extensive world-knowledge, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation) and [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling).


## Check for availability

Before you use the on-device model in your app, check that the model is available by creating an instance of [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel) with the [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/default](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel/default) property.

Model availability depends on device factors like:

- The device must support Apple Intelligence.

- The device must have Apple Intelligence turned on in Settings.


> **NOTE**: It can take some time for the model to download and become available when a person turns on Apple Intelligence.


Always verify model availability first, and plan for a fallback experience in case the model is unavailable.

```swift
struct GenerativeView: View {
    // Create a reference to the system language model.
    private var model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            // Show your intelligence UI.
        case .unavailable(.deviceNotEligible):
            // Show an alternative UI.
        case .unavailable(.appleIntelligenceNotEnabled):
            // Ask the person to turn on Apple Intelligence.
        case .unavailable(.modelNotReady):
            // The model isn't ready because it's downloading or because of other system reasons.
        case .unavailable(let other):
            // The model is unavailable for an unknown reason.
        }
    }
}
```


## Create a session

After confirming that the model is available, create a [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession) object to call the model. For a single-turn interaction, create a new session each time you call the model:

```swift
// Create a session with the system model.
let session = LanguageModelSession()
```

For a multiturn interaction — where the model retains some knowledge of what it produced — reuse the same session each time you call the model.


## Provide a prompt to the model

A [doc://com.apple.foundationmodels/documentation/FoundationModels/Prompt](https://developer.apple.com/documentation/FoundationModels/Prompt) is an input that the model responds to. Prompt engineering is the art of designing high-quality prompts so that the model generates a best possible response for the request you make. A prompt can be as short as “hello”, or as long as multiple paragraphs. The process of designing a prompt involves a lot of exploration to discover the best prompt, and involves optimizing prompt length and writing style.

When thinking about the prompt you want to use in your app, consider using conversational language in the form of a question or command. For example, “What’s a good month to visit Paris?” or “Generate a food truck menu.”

Write prompts that focus on a single and specific task, like “Write a profile for the dog breed Siberian Husky”. When a prompt is long and complicated, the model takes longer to respond, and may respond in unpredictable ways. If you have a complex generation task in mind, break the task down into a series of specific prompts.

You can refine your prompt by telling the model exactly how much content it should generate. A prompt like, “Write a profile for the dog breed Siberian Husky” often takes a long time to process as the model generates a full multi-paragraph essay. If you specify “using three sentences”, it speeds up processing and generates a concise summary. Use phrases like “in a single sentence” or “in a few words” to shorten the generation time and produce shorter text.

```swift
// Generate a longer response for a specific command.
let simple = "Write me a story about pears."

// Quickly generate a concise response.
let quick = "Write the profile for the dog breed Siberian Husky using three sentences."
```


## Provide instructions to the model

[doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions](https://developer.apple.com/documentation/FoundationModels/Instructions) help steer the model in a way that fits the use case of your app. The model obeys prompts at a lower priority than the instructions you provide. When you provide instructions to the model, consider specifying details like:

- What the model’s role is; for example, “You are a mentor,” or “You are a movie critic”.

- What the model should do, like “Help the person extract calendar events,” or “Help the person by recommending search suggestions”.

- What the style preferences are, like “Respond as briefly as possible”.

- What the possible safety measures are, like “Respond with ‘I can’t help with that’ if you’re asked to do something dangerous”.

Use content you trust in instructions because the model follows them more closely than the prompt itself. When you initialize a session with instructions, it affects all prompts the model responds to in that session. Instructions can also include example responses to help steer the model. When you add examples to your prompt, you provide the model with a template that shows the model what a good response looks like.


## Generate a response

To call the model with a prompt, call [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re) on your session. The response call is asynchronous because it may take a few seconds for the on-device foundation model to generate the response.

```swift
let instructions = """
    Suggest five related topics. Keep them concise (three to seven words) and make sure they \
    build naturally from the person's topic.
    """

let session = LanguageModelSession(instructions: instructions)

let prompt = "Making homemade bread"
let response = try await session.respond(to: prompt)
```


> **NOTE**: A session can only handle a single request at a time, and causes a runtime error if you call it again before the previous request finishes. Check [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/isResponding](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/isResponding) to verify the session is done processing the previous request before sending a new one.


Instead of working with raw string output from the model, the framework offers guided generation to generate a custom Swift data structure you define. For more information about guided generation, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation).

When you make a request to the model, you can provide custom tools to help the model complete the request. If the model determines that a [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) can assist with the request, the framework calls your [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) to perform additional actions like retrieving content from your local database. For more information about tool calling, see [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling)


## Consider context size limits per session

The *context window size* is a limit on how much data the model can process for a session instance. A token is a chunk of text the model processes, and the system model supports up to 4,096 tokens. A single token corresponds to three or four characters in languages like English, Spanish, or German, and one token per character in languages like Japanese, Chinese, or Korean. In a single session, the sum of all tokens in the instructions, all prompts, and all outputs count toward the context window size.

If your session processes a large amount of tokens that exceed the context window, the framework throws the error [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)). When you encounter the error, start a new session and try shortening your prompts. If you need to process a large amount of data that won’t fit in a single context window limit, break your data into smaller chunks, process each chunk in a separate session, and then combine the results.


## Tune generation options and optimize performance

To get the best results for your prompt, experiment with different generation options. [doc://com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions](https://developer.apple.com/documentation/FoundationModels/GenerationOptions) affects the runtime parameters of the model, and you can customize them for every request you make.

```swift
// Customize the temperature to increase creativity.
let options = GenerationOptions(temperature: 2.0)

let session = LanguageModelSession()

let prompt = "Write me a story about coffee."
let response = try await session.respond(
    to: prompt,
    options: options
)
```

When you test apps that use the framework, use Xcode Instruments to understand more about the requests you make, like the time it takes to perform a request. When you make a request, you can access the [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript](https://developer.apple.com/documentation/FoundationModels/Transcript) entries that describe the actions the model takes during your [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession).

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models)*
--- END FILE ---

--- FILE: README.md ---
# Swift Documentation Collection

A comprehensive collection of Apple Swift and iOS development documentation, organized for easy reference and learning.

## 📚 What's Included

This repository contains extensive documentation covering:

- **AI & Machine Learning**: CoreML, CreateML, FoundationModels, NaturalLanguage, Speech, Vision
- **App Services**: CloudKit, CoreData, HealthKit, Intents, StoreKit, WatchKit, WidgetKit
- **Core Frameworks**: AppKit, Foundation, UIKit
- **Graphics & Media**: ARKit, AVFoundation, CoreAnimation, CoreGraphics, Metal, RealityKit
- **Hardware & Sensors**: CoreBluetooth, CoreLocation, CoreMotion
- **Swift & SwiftUI**: Complete Swift language and SwiftUI framework documentation
- **System & Security**: Network, Security frameworks
- **Testing**: XCTest, Swift Testing, UI Testing, StoreKit Testing, Network Testing
- **Game Development**: SceneKit, SpriteKit

## 🚀 Quick Start

1. **Clone the repository**:
   ```bash
   git clone https://github.com/th30d0re/swiftDocs.git
   cd swiftDocs
   ```

2. **Browse the documentation**:
   - Navigate through the organized folders
   - Use your favorite markdown viewer
   - Search for specific topics using your editor's search functionality

## 📖 Methodology

This documentation collection was created through a systematic approach to gathering, organizing, and maintaining Apple's Swift and iOS development documentation.

### Research and Collection Process

1. **Comprehensive Scraping**: Systematically gathered documentation from Apple's official developer documentation website, covering all major frameworks and APIs.

2. **Automated Organization**: Created folder structures based on framework categories (AI/ML, App Services, Core Frameworks, Graphics & Media, etc.) for easy navigation.

3. **Content Validation**: Verified that all documentation reflects the latest Apple APIs and best practices, removing outdated or deprecated content.

4. **Cross-Referencing**: Linked related documentation and created index files to help developers find relevant information quickly.

### Organization Principles

- **Categorical Grouping**: Documentation organized by framework functionality (e.g., "AI-ML" for machine learning, "Testing" for testing frameworks)
- **Hierarchical Structure**: Main categories contain subfolders for specific frameworks or topics
- **Clear Naming**: Files named descriptively to indicate their content (e.g., `AVFoundation-Audio.md`)
- **Comprehensive Coverage**: Attempts to include all major Apple frameworks and tools

### Maintenance and Updates

- **Regular Sync**: Automated synchronization with Apple's documentation to ensure accuracy
- **Version Tracking**: Git-based version control to track changes over time
- **Scripted Updates**: Automated scripts for updating nested repositories (e.g., Swift Testing)

### Quality Assurance

- **Completeness Checks**: Regular audits to ensure all major frameworks are represented
- **Accuracy Verification**: Cross-checking with official Apple documentation
- **Link Validation**: Ensuring all internal and external links are functional

### Research Methodology

This collection was created by:

1. **Systematic Documentation Crawling**: Automated tools to scrape Apple's developer documentation
2. **Manual Curation**: Human review and organization of scraped content
3. **Community Feedback**: Incorporation of feedback from developers using the documentation
4. **Continuous Improvement**: Regular updates based on usage patterns and missing content

### Source Attribution

All documentation is sourced from:
- Apple Developer Documentation: https://developer.apple.com/documentation/
- Apple Developer Forums and Resources
- Official Apple GitHub Repositories (e.g., swift-testing)

**Note**: This collection is maintained for educational and reference purposes. Please respect Apple's copyright and licensing terms.

## 🔄 Repository Sync

This repository is automatically synced with the [nottinghillai organization](https://github.com/nottinghillai/swiftDocs) using GitHub Actions.

### Manual Sync
If you need to manually sync changes:
```bash
# The sync happens automatically on push to main branch
git add .
git commit -m "Your changes"
git push origin main
```

## 🧪 Swift Testing Repository Management

This repository includes a nested copy of the official Swift Testing repository for reference. Here's how to manage it:

### Updating Swift Testing

To fetch the latest updates from Apple's official Swift Testing repository:

```bash
./update-swift-testing.sh
```

This script will:
- Navigate to the nested repository
- Initialize it as a git repository if needed
- Set up the remote to point to `apple/swift-testing`
- Fetch and pull the latest updates
- Preserve the nested structure

### Manual Management

You can also manage the nested repository manually:

```bash
cd Testing/swift-testing-repo
git status
git pull origin main
git log --oneline
```

### What's Ignored

The `.gitignore` file is configured to:
- ✅ **Keep source code files** (`Sources/`, `Documentation/`)
- ❌ **Ignore git metadata** (`.git/` directory)
- ❌ **Ignore build files** (`.github/`, `CMakeLists.txt`, `Package.swift`, etc.)
- ❌ **Ignore test files** (`Tests/` directory)

This prevents git conflicts while preserving the ability to fetch updates.

## 📁 Repository Structure

```
swiftDocs/
├── AI-ML/                    # Machine Learning frameworks
├── App-Services/             # App services and data
├── Core-Frameworks/          # Core iOS/macOS frameworks
├── Graphics-Media/           # Graphics and media frameworks
├── Hardware-Sensors/         # Hardware and sensor APIs
├── Swift-SwiftUI/            # Swift language and SwiftUI
├── System-Security/          # System and security frameworks
├── Testing/                  # Testing frameworks and tools
│   ├── SwiftTests/           # Swift Testing documentation
│   ├── XCTest/               # XCTest framework docs
│   ├── UITests/              # UI Testing documentation
│   ├── StoreKitTests/        # StoreKit Testing docs
│   ├── NetworkTests/         # Network Testing docs
│   └── swift-testing-repo/   # Nested Swift Testing source
├── Custom/                   # Custom documentation
├── .github/workflows/        # GitHub Actions workflows
├── .gitignore               # Git ignore rules
├── update-swift-testing.sh  # Swift Testing update script
└── README.md                # This file
```

## 🔧 Development

### Adding New Documentation

1. Add your documentation files to the appropriate category folder
2. Follow the existing naming conventions
3. Use clear, descriptive filenames
4. Include proper markdown formatting

### Updating Existing Documentation

1. Edit the relevant markdown files
2. Test your changes locally
3. Commit and push to trigger automatic sync

## 🤝 Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## 📄 License

This documentation collection is for educational and reference purposes. Please respect Apple's copyright and licensing terms for the original documentation.

## 🔗 Links

- **Personal Repository**: https://github.com/th30d0re/swiftDocs
- **Organization Repository**: https://github.com/nottinghillai/swiftDocs
- **Swift Testing Source**: https://github.com/apple/swift-testing

## 📞 Support

If you encounter any issues or have questions:

1. Check the existing documentation
2. Search through the repository
3. Open an issue on GitHub
4. Contact the maintainers

---

**Happy coding! 🚀**
--- END FILE ---

--- FILE: QUICK-START.md ---
# Quick Start Guide - Apple Documentation Collection

**Your complete Apple framework documentation is ready!** 🎉

---

## 📍 Where Is Everything?

**Main Collection**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/`  
**Total Files**: 802 Markdown files  
**Frameworks**: 30 essential Apple frameworks

---

## 🚀 Quick Search Commands

### Search for AI/ML Content
```bash
cd /Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection

# Find Core ML content
grep -r "MLModel\|Core ML" . --include="*.md" | head -20

# Find Vision framework content
grep -r "Vision\|VNImageRequest" . --include="*.md" | head -20

# Find Foundation Models content
grep -r "Foundation Models\|LLM" . --include="*.md" | head -20
```

### Search for SwiftUI Content
```bash
# Find View documentation
grep -r "protocol View\|struct View" . --include="*.md" | head -20

# Find State management
grep -r "@State\|@Binding\|@ObservedObject" . --include="*.md" | head -20

# Find modifiers
grep -r "\.modifier\|ViewModifier" . --include="*.md" | head -20
```

### Search for Swift Language Content
```bash
# Find async/await
grep -r "async\|await\|Task" . --include="*.md" | head -20

# Find protocols
grep -r "protocol\|extension" . --include="*.md" | head -20
```

---

## 📚 Top Frameworks by File Count

| Framework | Files | Best For |
|-----------|-------|----------|
| Foundation | 60 | Data types, collections, system services |
| SwiftUI | 57 | Modern UI development |
| AppKit | 60 | macOS app development |
| AVFoundation | 50 | Audio/video processing |
| UIKit | 46 | iOS app development |
| Core ML | 42 | Machine learning |
| Metal | 42 | GPU programming |
| Core Graphics | 40 | 2D graphics |
| Core Data | 40 | Data persistence |
| Vision | 38 | Computer vision |

---

## 🎯 Recommended Starting Points

### For AI/ML Development
1. **Foundation Models** - `grep -l "Foundation Models" *.md`
2. **Core ML** - `grep -l "Core ML\|MLModel" *.md`
3. **Vision** - `grep -l "Vision\|VNImageRequest" *.md`

### For Swift/SwiftUI Development
1. **SwiftUI Basics** - `grep -l "SwiftUI\|View protocol" *.md`
2. **Swift Language** - `grep -l "Swift.*language\|async" *.md`
3. **Foundation** - `grep -l "Foundation.*framework" *.md`

### For Graphics Development
1. **Metal** - `grep -l "Metal.*framework\|GPU" *.md`
2. **ARKit** - `grep -l "ARKit\|Augmented Reality" *.md`
3. **Core Graphics** - `grep -l "Core Graphics\|CGContext" *.md`

---

## 💡 Pro Tips

### 1. Find All Files About a Topic
```bash
# Find all files mentioning "machine learning"
grep -l "machine learning" *.md

# Find all files with code examples
grep -l "```" *.md
```

### 2. Count References to a Concept
```bash
# Count how many times "async" appears
grep -r "async" . --include="*.md" | wc -l

# Count files mentioning "protocol"
grep -l "protocol" *.md | wc -l
```

### 3. Export to Other Formats
```bash
# Convert a file to HTML
pandoc README.md -o Overview.html

# Convert a file to PDF
pandoc README.md -o Overview.pdf
```

### 4. Create a Custom Index
```bash
# List all files by topic
ls -1 *.md | grep -i "core-ml\|vision\|foundation" > AI-ML-Index.txt

# List all SwiftUI files
ls -1 *.md | grep -i "swiftui\|view\|state" > SwiftUI-Index.txt
```

---

## 🔄 Update Documentation

To get the latest documentation:

```bash
# Navigate to tools
cd /Users/emmanuel/Dev/Tools/getMDfromURL

# Re-run the essentials scraper
python3 scrape_essentials.py

# Copy to research directory
rsync -av output/apple-docs-*/ /Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/
```

---

## 📖 Available Documentation

### AI & Machine Learning ✅
- Foundation Models (21 files)
- Core ML (42 files)
- Vision (38 files)
- Create ML (30 files)
- Speech (25 files)

### Swift & SwiftUI ✅
- Swift (27 files)
- SwiftUI (57 files)

### Core Frameworks ✅
- Foundation (60 files)
- UIKit (46 files)
- AppKit (60 files)

### Graphics & Media ✅
- Metal (42 files)
- AVFoundation (50 files)
- Core Graphics (40 files)
- ARKit (15 files)
- RealityKit (30 files)

### System & More ✅
- Security, Network, CloudKit, HealthKit, StoreKit
- WatchKit, WidgetKit, Core Data, Intents
- SpriteKit, SceneKit, XCTest
- Core Motion, Core Bluetooth, Core Location

---

## 🆘 Need Help?

**Full Documentation**: See `README.md` in the Complete-Collection directory  
**Scraping Summary**: See `SCRAPING-SUMMARY.md` in the parent directory  
**Tools Location**: `/Users/emmanuel/Dev/Tools/getMDfromURL`

---

**Ready to explore! Happy coding! 🚀**
--- END FILE ---

--- FILE: Complete-Apple-Documentation-Index.md ---
# Complete Apple Developer Documentation Collection

**Total Frameworks Discovered:** 116 frameworks

## 🎯 Available Frameworks

### Core Frameworks
- **Foundation** (`foundation`)
- **AVFoundation** (`avfoundation`)
- **UIKit** (`uikit`)
- **AppKit** (`appkit`)
- **SwiftUI** (`swiftui`)

### AI & Machine Learning
- **Core ML** (`coreml`)
- **Create ML** (`createML`)
- **Vision** (`vision`)
- **Speech** (`speech`)
- **Foundation Models** (`foundationmodels`)
- **Accelerate** (`accelerate`)
- **simd** (`simd`)

### Graphics & Media
- **Core Graphics** (`coregraphics`)
- **Core Image** (`coreimage`)
- **Core Video** (`corevideo`)
- **Core Audio** (`coreaudio`)
- **Core MIDI** (`coremidi`)
- **Metal** (`metal`)
- **OpenGL ES** (`opengles`)
- **SceneKit** (`scenekit`)
- **SpriteKit** (`spritekit`)
- **GameplayKit** (`gameplaykit`)
- **Photos** (`photos`)
- **PhotosUI** (`photosui`)
- **Image I/O** (`imageio`)
- **Media Player** (`mediaplayer`)
- **AVKit** (`avkit`)

### Augmented Reality
- **ARKit** (`arkit`)
- **RealityKit** (`realitykit`)

### System & Security
- **Security** (`security`)
- **Apple CryptoKit** (`cryptokit`)
- **Network** (`network`)
- **Network Extension** (`networkextension`)
- **System Configuration** (`systemconfiguration`)
- **OSLog** (`oslog`)
- **os** (`os`)
- **Dispatch** (`dispatch`)

### Location & Maps
- **Core Location** (`corelocation`)
- **MapKit** (`mapkit`)
- **Nearby Interaction** (`nearbyinteraction`)

### App Services
- **CloudKit** (`cloudkit`)
- **HealthKit** (`healthkit`)
- **HomeKit** (`homekit`)
- **WatchKit** (`watchkit`)
- **PassKit (Apple Pay and Wallet)** (`passkit`)
- **StoreKit** (`storekit`)
- **EventKit** (`eventkit`)
- **EventKit UI** (`eventkitui`)
- **Contacts** (`contacts`)
- **Contacts UI** (`contactsui`)
- **Message UI** (`messageui`)
- **Social** (`social`)
- **Accounts** (`accounts`)
- **AdServices** (`adservices`)

### User Interface
- **SwiftUI** (`swiftui`)
- **UIKit** (`uikit`)
- **AppKit** (`appkit`)
- **WatchKit** (`watchkit`)
- **TVUIKit** (`tvuikit`)
- **CarPlay** (`carplay`)
- **Multipeer Connectivity** (`multipeerconnectivity`)
- **WidgetKit** (`widgetkit`)
- **App Intents** (`appintents`)

### Data & Storage
- **Core Data** (`coredata`)
- **CloudKit** (`cloudkit`)
- **File Provider** (`fileprovider`)
- **File Provider UI** (`fileproviderui`)
- **Quick Look** (`quicklook`)
- **Quick Look Thumbnailing** (`quicklookthumbnailing`)
- **Quick Look UI** (`quicklookui`)

### Networking
- **Network** (`network`)
- **Network Extension** (`networkextension`)
- **Multipeer Connectivity** (`multipeerconnectivity`)
- **Core Bluetooth** (`corebluetooth`)
- **Core WLAN** (`corewlan`)
- **Core Telephony** (`coretelephony`)

### System Integration
- **Intents** (`intents`)
- **IntentsUI** (`intentsui`)
- **SiriKit** (`sirikit`)
- **ActivityKit** (`activitykit`)

### Game Development
- **GameKit** (`gamekit`)
- **GameplayKit** (`gameplaykit`)
- **SpriteKit** (`spritekit`)
- **SceneKit** (`scenekit`)
- **Metal** (`metal`)

### Development Tools
- **XCTest** (`xctest`)
- **Xcode** (`xcode`)
- **Swift** (`swift`)
- **Objective-C Runtime** (`objectivec`)

### Hardware & Sensors
- **Core Motion** (`coremotion`)
- **Core Bluetooth** (`corebluetooth`)
- **Core Location** (`corelocation`)
- **Core WLAN** (`corewlan`)

### Other Frameworks
- **Model I/O** (`modelio`)
- **AVRouting** (`avrouting`)

## 🚀 How to Use

### Crawl Individual Framework
```bash
python3 apple_doc_crawler.py <framework-name> -m 50 -d 0.5
```

### Examples
```bash
# AI & Machine Learning
python3 apple_doc_crawler.py "coreml" -m 50
python3 apple_doc_crawler.py "vision" -m 30
python3 apple_doc_crawler.py "foundationmodels" -m 20

# Graphics & Media
python3 apple_doc_crawler.py "metal" -m 40
python3 apple_doc_crawler.py "coregraphics" -m 30
python3 apple_doc_crawler.py "photos" -m 25

# User Interface
python3 apple_doc_crawler.py "swiftui" -m 100
python3 apple_doc_crawler.py "uikit" -m 80

# System & Security
python3 apple_doc_crawler.py "security" -m 30
python3 apple_doc_crawler.py "network" -m 40

# Augmented Reality
python3 apple_doc_crawler.py "arkit" -m 50
python3 apple_doc_crawler.py "realitykit" -m 30
```

### Crawl Multiple Frameworks
```bash
# Create a script to crawl multiple frameworks
cat > crawl_multiple.sh << 'EOF'
#!/bin/bash
frameworks=("coreml" "vision" "foundationmodels" "metal" "swiftui" "arkit")
for framework in "${frameworks[@]}"; do
    echo "Crawling $framework..."
    python3 apple_doc_crawler.py "$framework" -m 50 -d 0.5 -o "output/$framework"
done
EOF
chmod +x crawl_multiple.sh
./crawl_multiple.sh
```

## 📊 Statistics

- **Total Frameworks**: 116
- **Categories**: 15
- **Last Updated**: October 7, 2025

## 🛠️ Tools Used

This comprehensive collection was discovered using:
- `discover_apple_frameworks.py` - Automated framework discovery
- `apple_doc_crawler.py` - Framework-wide documentation crawling
- `apple_doc_fetcher.py` - Individual page fetching
- Apple's JSON API endpoints at `https://developer.apple.com/tutorials/data/documentation/`

## 🎯 Next Steps

1. **Choose frameworks** based on your research interests
2. **Crawl specific frameworks** using the examples above
3. **Organize results** into your research directory structure
4. **Build comprehensive collections** for your areas of focus

---
*Generated by getMDfromURL Apple Documentation Discovery Tools*
--- END FILE ---

--- FILE: FINAL-COLLECTION-SUMMARY.md ---
# 🎉 Complete Apple Documentation Collection - Final Summary

**Date**: October 8, 2025  
**Status**: ✅ **100% SUCCESS - ALL FRAMEWORKS SCRAPED!**

---

## 🏆 Mission Accomplished!

Successfully scraped **ALL 32 FRAMEWORKS** including the two that initially failed!

### 📊 Final Statistics

- **Total Frameworks**: 32 (100% success!)
- **Total Documentation Files**: 848 Markdown files
- **Success Rate**: 100% 🎉
- **Collection Location**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/`
- **Total Content Size**: ~5.2 MB

---

## ✅ All 32 Frameworks Successfully Scraped

### AI & Machine Learning (6 frameworks) ✅
1. ✅ **Foundation Models** (21 files) - Apple's AI foundation models
2. ✅ **Core ML** (42 files) - On-device machine learning
3. ✅ **Vision** (38 files) - Computer vision & image analysis
4. ✅ **Create ML** (30 files) - ML model training
5. ✅ **Speech** (25 files) - Speech recognition & synthesis
6. ✅ **Natural Language** (31 files) - NLP & text analysis ⭐ **FIXED!**

### Swift & SwiftUI (2 frameworks) ✅
7. ✅ **Swift** (27 files) - Swift language documentation
8. ✅ **SwiftUI** (57 files) - Modern declarative UI

### Core Apple Frameworks (3 frameworks) ✅
9. ✅ **Foundation** (60 files) - Essential data types
10. ✅ **UIKit** (46 files) - iOS UI framework
11. ✅ **AppKit** (60 files) - macOS UI framework

### Graphics & Media (6 frameworks) ✅
12. ✅ **Metal** (42 files) - Low-level graphics API
13. ✅ **AVFoundation** (50 files) - Audio/video processing
14. ✅ **Core Graphics** (40 files) - 2D graphics
15. ✅ **Core Animation** (41 files) - Animation framework ⭐ **FIXED!**
16. ✅ **ARKit** (15 files) - Augmented reality
17. ✅ **RealityKit** (30 files) - 3D rendering for AR

### System & Security (2 frameworks) ✅
18. ✅ **Security** (30 files) - Cryptography & secure storage
19. ✅ **Network** (30 files) - Modern networking

### App Services (5 frameworks) ✅
20. ✅ **CloudKit** (30 files) - Cloud data storage
21. ✅ **HealthKit** (25 files) - Health & fitness data
22. ✅ **StoreKit** (25 files) - In-app purchases
23. ✅ **WatchKit** (30 files) - Apple Watch apps
24. ✅ **WidgetKit** (20 files) - Home screen widgets

### Data & Storage (1 framework) ✅
25. ✅ **Core Data** (40 files) - Object persistence

### System Integration (1 framework) ✅
26. ✅ **Intents** (25 files) - Siri integration

### Game Development (2 frameworks) ✅
27. ✅ **SpriteKit** (30 files) - 2D game engine
28. ✅ **SceneKit** (30 files) - 3D game engine

### Development Tools (1 framework) ✅
29. ✅ **XCTest** (30 files) - Unit testing

### Hardware & Sensors (3 frameworks) ✅
30. ✅ **Core Motion** (25 files) - Motion & sensor data
31. ✅ **Core Bluetooth** (25 files) - Bluetooth LE
32. ✅ **Core Location** (25 files) - Location services

---

## 🔧 How We Fixed the Failed Frameworks

### Problem 1: Natural Language (Failed as "natural")
**Solution**: Used the full framework name `naturallanguage` instead of `natural`
```bash
python3 apple_doc_crawler.py "naturallanguage" -m 30 -d 0.5
```
**Result**: ✅ 31 files successfully scraped!

### Problem 2: Core Animation (Failed as "coreanimation")
**Solution**: Used the original framework name `quartzcore` instead of `coreanimation`
```bash
python3 apple_doc_crawler.py "quartzcore" -m 40 -d 0.5
```
**Result**: ✅ 41 files successfully scraped!

---

## 📈 Collection Growth

| Metric | Before Retry | After Retry | Improvement |
|--------|-------------|-------------|-------------|
| Frameworks | 30/32 | 32/32 | +2 frameworks |
| Success Rate | 93.8% | 100% | +6.2% |
| Total Files | 802 | 848 | +46 files |
| Content Size | ~4.6 MB | ~5.2 MB | +0.6 MB |

---

## 🎯 Verified Documentation

### Speech Framework ✅
- **Files**: 19 documentation files
- **URL**: https://developer.apple.com/documentation/speech
- **Content**: Speech recognition, synthesis, analysis
- **Location**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/`

### Vision Framework ✅
- **Files**: 38 documentation files
- **URL**: https://developer.apple.com/documentation/vision
- **Content**: Computer vision, image analysis, object detection
- **Location**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/`

---

## 📁 Complete Collection Structure

```
/Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/
├── README.md                                  # Main index
├── 848 Framework Documentation Files          # All scraped content
│
├── AI & Machine Learning (6 frameworks, 187 files)
│   ├── Foundation Models (21 files)
│   ├── Core ML (42 files)
│   ├── Vision (38 files)
│   ├── Create ML (30 files)
│   ├── Speech (25 files)
│   └── Natural Language (31 files) ⭐ NEW
│
├── Swift & SwiftUI (2 frameworks, 84 files)
│   ├── Swift (27 files)
│   └── SwiftUI (57 files)
│
├── Core Frameworks (3 frameworks, 166 files)
│   ├── Foundation (60 files)
│   ├── UIKit (46 files)
│   └── AppKit (60 files)
│
├── Graphics & Media (6 frameworks, 226 files)
│   ├── Metal (42 files)
│   ├── AVFoundation (50 files)
│   ├── Core Graphics (40 files)
│   ├── Core Animation (41 files) ⭐ NEW
│   ├── ARKit (15 files)
│   └── RealityKit (30 files)
│
└── ... (15 more frameworks, 185 files)
```

---

## 🚀 Quick Start Commands

### Navigate to Collection
```bash
cd /Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection
```

### Search for AI/ML Content
```bash
# Core ML
grep -r "MLModel" . --include="*.md" | head -20

# Vision
grep -r "VNImageRequest" . --include="*.md" | head -20

# Natural Language (NEW!)
grep -r "NLTagger\|Natural Language" . --include="*.md" | head -20

# Speech
grep -r "SFSpeechRecognizer\|Speech" . --include="*.md" | head -20
```

### Search for Graphics & Animation
```bash
# Core Animation (NEW!)
grep -r "CALayer\|CAAnimation" . --include="*.md" | head -20

# Metal
grep -r "MTL\|Metal" . --include="*.md" | head -20

# ARKit
grep -r "ARKit\|ARSession" . --include="*.md" | head -20
```

### Search for Swift/SwiftUI
```bash
# SwiftUI
grep -r "@State\|@Binding" . --include="*.md" | head -20

# Swift language features
grep -r "async\|await" . --include="*.md" | head -20
```

---

## 📊 Coverage by Category

| Category | Frameworks | Files | Percentage |
|----------|-----------|-------|-----------|
| AI & Machine Learning | 6 | 187 | 22% |
| Swift & SwiftUI | 2 | 84 | 10% |
| Core Frameworks | 3 | 166 | 20% |
| Graphics & Media | 6 | 226 | 27% |
| System & Security | 2 | 60 | 7% |
| App Services | 5 | 130 | 15% |
| Other | 8 | 95 | 11% |
| **Total** | **32** | **848** | **100%** |

---

## 🎓 What You Can Do Now

### 1. AI/ML Development
- **Foundation Models**: Build AI-powered apps with Apple's LLMs
- **Core ML**: Integrate machine learning models
- **Vision**: Add computer vision capabilities
- **Natural Language**: Process and analyze text
- **Speech**: Add voice recognition and synthesis

### 2. UI Development
- **SwiftUI**: Build modern, declarative interfaces
- **UIKit**: iOS app development
- **AppKit**: macOS app development
- **Core Animation**: Create smooth animations

### 3. Graphics & Gaming
- **Metal**: GPU programming and compute
- **ARKit/RealityKit**: Augmented reality experiences
- **SpriteKit/SceneKit**: 2D/3D game development

### 4. System Integration
- **CloudKit**: Sync data across devices
- **HealthKit**: Health and fitness apps
- **Intents/Siri**: Voice integration
- **Core Location**: Location-based features

---

## 🔄 Keep Your Documentation Updated

To get the latest documentation in the future:

```bash
# Navigate to tools directory
cd /Users/emmanuel/Dev/Tools/getMDfromURL

# Re-run the essentials scraper
python3 scrape_essentials.py

# Add the newly fixed frameworks
python3 apple_doc_crawler.py "naturallanguage" -m 30
python3 apple_doc_crawler.py "quartzcore" -m 40

# Copy to research directory
rsync -av output/apple-docs-*/ /Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/
```

---

## 📚 Additional Resources

- **Quick Start Guide**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/QUICK-START.md`
- **Scraping Summary**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/SCRAPING-SUMMARY.md`
- **Collection README**: `/Users/emmanuel/Dev/Research/Apple/AI/Docs/Complete-Collection/README.md`
- **Tools Location**: `/Users/emmanuel/Dev/Tools/getMDfromURL`

---

## 🎉 Summary

**You now have the most comprehensive Apple documentation collection possible!**

- ✅ All 32 essential frameworks
- ✅ 848 documentation files
- ✅ 100% success rate
- ✅ Fully searchable and offline
- ✅ Perfect for AI/ML, Swift, SwiftUI, and all Apple development

**Ready for any Apple development project! 🚀**

---

**Generated**: October 8, 2025  
**Status**: ✅ COMPLETE COLLECTION - 100% SUCCESS!  
**Total**: 32 frameworks, 848 files, ~5.2 MB
--- END FILE ---
=== END SWIFT DOCUMENTATION ===
=== END CONTEXT ===

=== BUILD ATTEMPT 3 FAILED ===
The previous implementation failed with the following errors:

Build failed but no detailed error log available.

=== INSTRUCTIONS FOR ATTEMPT 4 ===
1. Analyze what went wrong in the previous approach
2. Try a DIFFERENT approach or fix the specific issues
3. Do NOT repeat the same mistakes
4. If the same approach keeps failing, consider an alternative implementation strategy
5. Write the corrected code directly - do not explain, just implement

Please fix the issues and provide the corrected implementation.