You are a Swift/SwiftUI developer. Your task is to implement the following requirements.

IMPORTANT INSTRUCTIONS:
1. Write clean, production-ready Swift code
2. Follow Apple's Swift API Design Guidelines
3. Use SwiftUI for UI components where appropriate
4. Include proper error handling
5. The code must compile without errors
6. After writing code, the build will be verified automatically
7. If the build fails, you will receive error feedback and should try a DIFFERENT approach

TESTING & DEBUGGING REFERENCE:
For building, testing, and debugging iOS/macOS apps, reference this workflow guide:
/Users/emmanuel/Dev/Tools/Eocon-Foundation-V1/.Foundation/Docs/swiftDocs/Testing/XCODEBUILD_MCP_WORKFLOW.md

This guide covers:
- XcodeBuild MCP server tools for programmatic Xcode interaction
- Building for simulator, booting simulators, installing/launching apps
- UI automation: screenshots, accessibility hierarchy, tap simulation
- Debugging UI issues (button taps, gestures, navigation)

=== TASK/REQUIREMENTS ===
I have the following verification comments after thorough review and exploration of the codebase. Implement the comments by following the instructions in the comments verbatim.

---
## Comment 1: trainEpoch simulates data-aware losses without MLX model loading, LoRA, or training loops.

Fully implement real MLX LoRA fine-tuning in \`TrainingManager.swift\` to resolve this review thread completely.

**Thread Context Summary:**
- Original: Fake random losses; ignored data/config/MLX/checkpoints/resume.
- Partial progress: Loads clauses, builds prompt-completion pairs, 80/20 split, per-epoch dirs, yields epoch/loss progress, saves JSON metadata/adapters.
- Further comment: \`trainEpoch\` still fake; need MLX imports/load/LoRA/tokenize/batches/losses/save.
- Current state: Imports MLX/MLXLLM/MLXLMCommon; \`computeDataAwareTrainingLoss\` fakes 'data-aware' CE loss from char counts/vocab heuristic; empty model dir; no training.

**Root Cause:** Placeholder loss computation skips MLX pipeline.

**Comprehensive Fix Steps:**
1. **Base Model:** Update \`downloadBaseModel()\` to download/convert small LM e.g., Phi-3-mini-4k-instruct (Q4_0) via \`mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct --mlx-path \\\`baseModelURL.path\\\` -q\`. Ensure tokenizer.json/weights.npz/config.json present.
2. **Pairs:** Format causal: \`prompt + tokenizer.eosToken + completion + tokenizer.eosToken\` or use chat template if instruct model.
3. **trainEpoch:**
   - \`let lm = try LM(modelPath: baseModelURL.path)\`; \`let model = lm.model; let tokenizer = lm.tokenizer\`
   - LoRA: \`let loraConfigMLX = LoRA(rank: loraConfig.loraRank, alpha: loraConfig.alpha, dropout: 0.05, targets: [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"])\`; \`let trainer = LoRATrainer(model: model, lora: loraConfigMLX)\` or manual LoRA layers.
   - Optimizer: \`let opt = AdamW(learningRate: loraConfig.learningRate, weightDecay: 0.01)\`
   - Tokenize: \`let trainTokens = trainData.map { tokenizer.encode(prompt + completion).ids }\`; pad to max_len=512, batch_size=4.
   - Train loop: for batches: \`let logits = model(tokens)\`; \`loss = cross_entropy(logits.view(...), labels, ignore prompt tokens)\`; \`loss.backward()\`; \`opt.update()\`; avg trainLoss.
   - Val similar no-grad.
   - Save: \`trainer.save_lora(checkpointURL)\` or \`model.save_weights(checkpointURL)\`
4. **Resume:** Load LoRA deltas into model from \`checkpointURL\`, continue epochs.
5. **Broader Integration:** Wire to \`AdapterManager.loadActiveAdapter\` for inference; post-train validate COI improvement in \`ValidationRunner\`; handle OOM/small data.
6. **Simulator Fallback:** Retain fake losses under \`#if targetEnvironment(simulator)\`.

**presentation_instructions:**
In \`app/decodingOppression/decodingOppression/MLX/TrainingManager.swift\`, \`trainEpoch\`/\`computeDataAwareTrainingLoss\` fake training.
Download Phi-3 to \`baseModelURL\` via mlx_lm.convert.
Replace with: Load \`LM\` from \`baseModelURL.path\`; init LoRA on q_proj/v_proj etc. with \`loraConfig\`; AdamW optimizer.
Tokenize pairs to ids, batch/pad (bs=4, len=512), causal LM loss masking prompt.
Train/Val loops compute real losses; save LoRA to \`checkpointURL\`.
Resume loads prior LoRA.

### Referred Files
- app/decodingOppression/decodingOppression/MLX/TrainingManager.swift
---
=== END TASK ===


=== REFERENCE CONTEXT ===
Use the following documentation and context as reference:

=== SWIFT DOCUMENTATION ===

--- FILE: MODEL_CHAT_ARCHITECTURE.md ---
# Model Chat Architecture

This document explains exactly how ChAI enables users to chat with different model types: **Foundation Models** (Apple's on-device AI), **MLX Models** (local quantized models), and **Cloud Models** (API-based models from various providers).

## Overview

ChAI uses a unified chat interface powered by the `MultiModelManager` that intelligently routes requests to the appropriate model handler based on the selected model type. All chat interactions flow through a single entry point but branch into different execution paths depending on whether the model is local (Foundation/MLX) or cloud-based.

## Architecture Components

### Core Managers

1. **MultiModelManager** - Central orchestrator for all model interactions
2. **FoundationModelManager** - Handles Apple's on-device FoundationModels (iOS 26+)
3. **MLXModelManager** - Manages locally downloaded MLX models
4. **MessagesManager** - Builds conversation context with token awareness
5. **ThreadViewModel** - Coordinates UI and message sending

### Flow Diagram

```
User Input
    ‚Üì
ThreadViewModel.sendMessage()
    ‚Üì
MultiModelManager.respond() or streamResponse()
    ‚Üì
    ‚îú‚îÄ‚Üí [isLocal = true, provider = "MLX"] ‚Üí MLXModelManager
    ‚îú‚îÄ‚Üí [isLocal = true, iOS 26+] ‚Üí FoundationModelManager
    ‚îî‚îÄ‚Üí [isLocal = false] ‚Üí Cloud API (OpenAI, Anthropic, Google, etc.)
```

## Model Type 1: Foundation Models (Apple On-Device)

### What They Are
Apple's on-device AI models introduced in iOS 26, using the FoundationModels framework. These run entirely on-device with no API costs.

### How Chat is Enabled

#### 1. **Initialization** (`FoundationModelManager.swift`)
```swift
init(settings: Settings) {
    if #available(iOS 26.0, *) {
        guard SystemLanguageModel.default.availability == .available else { return }
        let session = try LanguageModelSession(instructions: settings.systemPrompt)
        self.session = session
        self.transcript = session.transcript
    }
}
```

Key points:
- Creates a `LanguageModelSession` with the system prompt
- The session maintains its own internal transcript for conversation context
- Checks model availability before initialization

#### 2. **Message Routing** (`MultiModelManager.swift` - Line 444-534)
```swift
func respond(to text: String, thread: Thread?) async throws -> String {
    guard var model = currentModel else {
        throw MultiModelError.noModelSelected
    }
    
    if model.isLocal {
        if model.provider == "MLX" {
            // Route to MLX
        } else if #available(iOS 26.0, *) {
            // Route to Foundation Models
            guard let thread = thread else {
                throw MultiModelError.noModelSelected
            }
            responseText = try await respondWithLocalModel(to: text, thread: thread)
        }
    }
}
```

Foundation Models **require a thread** for session management.

#### 3. **Non-Streaming Response** (`MultiModelManager.swift` - Line 564-629)
```swift
func respondWithLocalModel(to text: String, thread: Thread) async throws -> String {
    guard let manager = foundationModelManager else {
        throw MultiModelError.noModelSelected
    }
    
    // Handle simple greetings to avoid safety guardrails
    let simpleGreetings = ["hi", "hello", "hey", ...]
    if simpleGreetings.contains(normalizedText) {
        return "Hello! How can I help you today?"
    }
    
    do {
        let response = try await manager.respond(to: text, thread: thread)
        return response
    } catch FoundationModelManager.FoundationModelManagerError.safetyGuardrailsTriggered {
        // Trigger fallback to cloud model
        throw MultiModelError.foundationModelUnavailable(...)
    }
}
```

#### 4. **Streaming Response** (`FoundationModelManager.swift` - Line 136-200)
```swift
func streamResponse(to text: String, thread: Thread? = nil) -> AsyncStream<String>? {
    let responseStream = session.streamResponse(to: text)
    return AsyncStream<String> { continuation in
        Task {
            var previous = ""
            for try await snapshot in responseStream {
                let current = snapshot.content ?? ""
                // Yield only new content since last snapshot
                if current.count > previous.count {
                    let deltaText = String(current[start...])
                    continuation.yield(deltaText)
                }
                previous = current
            }
            continuation.finish()
        }
    }
}
```

#### 5. **Conversation Context**
Foundation Models handle context automatically through `LanguageModelSession.transcript`. The framework maintains conversation history internally, so ChAI doesn't need to manually build message arrays.

### Special Handling

**Safety Guardrails**: Foundation Models have built-in safety guardrails that can reject certain prompts. When triggered:
1. Error is caught in `respondWithLocalModel()`
2. Throws `MultiModelError.foundationModelUnavailable`
3. `ThreadViewModel` triggers automatic fallback to cloud model
4. User sees: "‚ö†Ô∏è Using alternative model (FoundationModels unavailable: safety)"

**Simple Greeting Bypass**: To avoid false positives on safety guardrails, simple greetings like "hi" and "hello" are intercepted and return canned responses.

---

## Model Type 2: MLX Models (Local Quantized Models)

### What They Are
Quantized language models downloaded from HuggingFace that run locally using Apple's MLX framework. These are stored in the device's Application Support directory.

### How Chat is Enabled

#### 1. **Model Discovery & Loading** (`MLXModelManager.swift`)

**Download Process** (Line 119-254):
```swift
func downloadModel(huggingFaceId: String, displayName: String) async throws -> MLXModel {
    // Download to Application Support/ChAi/MLXModels
    let mlxModelsDir = appSupportDir.appendingPathComponent("ChAi/MLXModels")
    let modelDir = mlxModelsDir.appendingPathComponent(directoryName)
    
    // Download using HubApi
    let hubApi = HubApi()
    let modelUrl = try await hubApi.snapshot(from: huggingFaceId, matching: ["*.safetensors", "*.json"])
    
    // Create MLXModel record
    let model = MLXModel(huggingFaceId: huggingFaceId, displayName: displayName, localPath: directoryName)
    modelContext.insert(model)
    
    return model
}
```

**Loading into Memory** (Line 258-339):
```swift
func loadModel(_ model: MLXModel) async throws -> MLXLMCommon.ModelContainer {
    // Check cache first
    let cacheKey = model.id.uuidString as NSString
    if let cached = modelCache.object(forKey: cacheKey) {
        return cached
    }
    
    // Load from disk
    let modelURL = URL(fileURLWithPath: model.resolvedPath)
    let hubApi = HubApi(downloadBase: modelURL)
    let configuration = ModelConfiguration(id: model.huggingFaceId)
    
    let container = try await LLMModelFactory.shared.loadContainer(
        hub: hubApi,
        configuration: configuration,
        progressHandler: progressHandler
    )
    
    // Cache for reuse
    modelCache.setObject(container, forKey: cacheKey)
    return container
}
```

#### 2. **Message Routing** (`MultiModelManager.swift` - Line 444-534)
```swift
if model.isLocal {
    if model.provider == "MLX" {
        guard let mlxModel = mlxModelManager?.downloadedModels.first(where: {
            $0.id.uuidString == model.id
        }) else {
            throw MultiModelError.noModelSelected
        }
        responseText = try await respondWithMLX(to: text, model: mlxModel, thread: thread)
    }
}
```

#### 3. **Building Conversation Context** (`MultiModelManager.swift` - Line 632-691)
Unlike Foundation Models, MLX models require explicit message arrays:

```swift
func buildMLXMessages(for text: String, thread: Thread?, model: MLXModel) async -> [[String: String]] {
    var messages: [[String: String]] = []
    
    // 1. Add system prompt
    if !settings.systemPrompt.isEmpty {
        messages.append(["role": "system", "content": settings.systemPrompt])
    }
    
    // 2. Add conversation history if memory enabled
    if let thread = thread, 
       settings.globalMemoryEnabled,
       settings.isMemoryEnabled(for: thread.id) {
        
        let sortedChats = thread.chats.sorted(by: { $0.timestamp < $1.timestamp })
        
        for chat in sortedChats {
            let role = chat.sender == .user ? "user" : "assistant"
            messages.append(["role": role, "content": chat.content])
        }
    }
    
    // 3. Add current user message
    messages.append(["role": "user", "content": text])
    
    return messages
}
```

#### 4. **Generation** (`MLXModelManager.swift` - Line 343-446)
```swift
func generate(
    model: MLXModel,
    messages: [[String: String]],
    maxTokens: Int,
    temperature: Double,
    topP: Double
) -> AsyncThrowingStream<String, Error> {
    return AsyncThrowingStream { continuation in
        Task {
            // Load model container
            let container = try await loadModel(model)
            
            // Prepare input with chat template
            try await container.perform { (context: ModelContext) in
                let userInput = UserInput(messages: messages)
                let preparedInput = try await context.processor.prepare(input: userInput)
                
                // Generate with streaming
                let result = try MLXLMCommon.generate(
                    input: preparedInput,
                    parameters: generateParameters,
                    context: context
                )
                
                // Stream tokens
                for await generation in result {
                    if case .chunk(let text) = generation {
                        continuation.yield(text)
                    }
                }
            }
            continuation.finish()
        }
    }
}
```

#### 5. **Response Collection** (`MultiModelManager.swift` - Line 710-755)
```swift
func respondWithMLX(to text: String, model: MLXModel, thread: Thread?) async throws -> String {
    let messages = await buildMLXMessages(for: text, thread: thread, model: model)
    
    var fullResponse = ""
    let stream = manager.generate(model: model, messages: messages, ...)
    
    for try await chunk in stream {
        fullResponse += chunk
    }
    
    return fullResponse
}
```

### MLX-Specific Features

**Model Caching**: Loaded models are cached in `NSCache` to avoid reloading on every request.

**Template Fallback**: If the model's Jinja template fails to process the message array:
```swift
catch {
    // Fallback: combine system + user message
    let combinedPrompt = "\(systemContent)\n\n\(lastUserMessage)"
    let fallbackMessages = [["role": "user", "content": combinedPrompt]]
    preparedInput = try await context.processor.prepare(input: fallbackInput)
}
```

**Cancellation Support**: MLX generation can be cancelled mid-stream via `cancelGeneration()`.

---

## Model Type 3: Cloud Models (API-based)

### What They Are
External AI models accessed via REST APIs from providers like OpenAI, Anthropic, Google, Deepseek, Perplexity, etc.

### How Chat is Enabled

#### 1. **Model Registration** (`MultiModelManager.swift` - Line 86-250)
All cloud models are registered in `AIModel.defaultModels`:

```swift
static let defaultModels: [AIModel] = [
    AIModel(id: "gpt-4o", name: "GPT-4o", provider: "OpenAI", isLocal: false, 
            maxTokens: 128000, requiresAPIKey: true),
    AIModel(id: "claude-3-5-sonnet-20241022", name: "Claude 3.5 Sonnet", 
            provider: "Anthropic", isLocal: false, maxTokens: 200000, requiresAPIKey: true),
    AIModel(id: "gemini-2.0-flash-exp", name: "Gemini 2.0 Flash", 
            provider: "Google", isLocal: false, maxTokens: 1048576, requiresAPIKey: true),
    // ... more models
]
```

#### 2. **API Key Management** (`APIKeyManager.swift`)
Each provider requires an API key:

```swift
enum AIProvider: String {
    case openAI = "OpenAI"
    case anthropic = "Anthropic"
    case google = "Google"
    case deepseek = "Deepseek"
    // ... more providers
}

func getAPIKey(for provider: AIProvider) async -> String? {
    // Retrieves encrypted API key from Keychain
}
```

#### 3. **Message Routing** (`MultiModelManager.swift` - Line 444-534)
```swift
if !model.isLocal {
    // Use token-aware context building
    let result = try await respondWithAPIWithTokenTracking(
        to: text, context: context, model: model, thread: thread
    )
    responseText = result.response
    contextTokens = result.tokenCount
}
```

#### 4. **Token-Aware Context Building** (`MultiModelManager.swift` - Line 768-802)
Cloud models have token limits, so context must be carefully managed:

```swift
func respondWithAPIWithTokenTracking(
    to text: String, model: AIModel, thread: Thread?
) async throws -> TokenAwareResponse {
    // Use MessagesManager for intelligent context building
    let messagesManager = MessagesManager()
    let contextResult = await messagesManager.apiMessages(
        for: thread,
        systemPrompt: settings.systemPrompt,
        settings: settings,
        model: model,
        includeHistory: true,
        respectTokenLimits: true
    )
    
    // Returns: messages, tokenCount, wasTruncated, summaryUsed
    let response = try await respondWithAPIUsingMessages(
        text: text, model: model, thread: thread, messages: contextResult.messages
    )
    
    return TokenAwareResponse(
        response: response,
        contextTokens: contextResult.tokenCount,
        wasTruncated: contextResult.wasTruncated,
        summaryUsed: contextResult.summaryUsed
    )
}
```

#### 5. **Context Building Logic** (`MessagesManager.swift` - Line 56-122)
```swift
func apiMessages(for thread: Thread, model: AIModel, ...) async -> (messages, tokenCount, wasTruncated, summaryUsed) {
    var messages: [[String: Any]] = []
    
    // 1. System prompt
    messages.append(["role": "system", "content": systemPrompt])
    
    // 2. Check memory settings
    if !settings.globalMemoryEnabled || !settings.isMemoryEnabled(for: thread.id) {
        // Memory disabled - only send last user message
        messages.append(["role": "user", "content": lastUserMessage])
        return (messages, tokenCount, false, false)
    }
    
    // 3. Apply retention policy
    let sortedChats = thread.chats.sorted(by: { $0.timestamp < $1.timestamp })
    let filteredChats = filterChatsByRetentionPolicy(sortedChats, settings: settings)
    
    // 4. Convert to message format
    for chat in filteredChats {
        let role = chat.sender == .user ? "user" : "assistant"
        messages.append(["role": role, "content": chat.content])
    }
    
    // 5. Apply token-aware truncation
    if respectTokenLimits {
        let result = await applyTokenAwareTruncation(messages, thread, model)
        messages = result.messages
        wasTruncated = result.wasTruncated
        summaryUsed = result.summaryUsed
    }
    
    return (messages, tokenCount, wasTruncated, summaryUsed)
}
```

#### 6. **Provider-Specific API Calls** (`MultiModelManager.swift` - Line 868-924)
```swift
func respondWithDirectAPIUsingMessages(...) async throws -> String {
    let provider = getProviderForModel(model)
    guard let apiKey = await apiKeyManager.getAPIKey(for: provider) else {
        throw MultiModelError.missingAPIKey(provider: provider.displayName)
    }
    
    switch provider {
    case .openAI:
        return try await callOpenAIAPIWithMessages(...)
    case .anthropic:
        return try await callAnthropicAPIWithMessages(...)
    case .google:
        return try await callGoogleAPIWithMessages(...)
    case .deepseek:
        return try await callDeepseekAPIWithMessages(...)
    // ... more providers
    }
}
```

#### 7. **Example: OpenAI API Call** (`MultiModelManager.swift` - Line 1247-1358)
```swift
func callOpenAIAPIWithMessages(
    text: String, apiKey: String, model: AIModel, messages: [[String: Any]]
) async throws -> String {
    var request = URLRequest(url: URL(string: "https://api.openai.com/v1/chat/completions")!)
    request.httpMethod = "POST"
    request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
    request.setValue("application/json", forHTTPHeaderField: "Content-Type")
    
    let body: [String: Any] = [
        "model": model.id,
        "messages": messages,
        "max_tokens": settings.maxTokens,
        "temperature": settings.temperature,
        "top_p": settings.topP
    ]
    request.httpBody = try JSONSerialization.data(withJSONObject: body)
    
    let (data, response) = try await URLSession.shared.data(for: request)
    
    let json = try JSONSerialization.jsonObject(with: data) as? [String: Any]
    let choices = json?["choices"] as? [[String: Any]]
    let message = choices?.first?["message"] as? [String: Any]
    let content = message?["content"] as? String
    
    return content ?? "No response received"
}
```

#### 8. **Streaming Support** (`MultiModelManager.swift` - Line 2111-2302)
```swift
func callOpenAIStreamingAPI(...) -> AsyncThrowingStream<String, Error> {
    return AsyncThrowingStream { continuation in
        Task {
            // Set stream: true in request body
            let body: [String: Any] = ["model": model.id, "messages": messages, "stream": true]
            
            let (bytes, response) = try await URLSession.shared.bytes(for: request)
            
            for try await line in bytes.lines {
                if line.hasPrefix("data: ") {
                    let jsonString = line.dropFirst(6)
                    if jsonString == "[DONE]" { break }
                    
                    let json = try JSONSerialization.jsonObject(with: Data(jsonString.utf8))
                    let delta = json["choices"][0]["delta"]["content"] as? String
                    if let content = delta {
                        continuation.yield(content)
                    }
                }
            }
            continuation.finish()
        }
    }
}
```

### Cloud Model Features

**Token Tracking**: All requests track prompt and completion tokens for analytics and cost estimation.

**Context Truncation**: When context exceeds model limits:
1. Calculate effective limit: `maxTokens - responseTokens - safetyMargin`
2. Truncate oldest messages first
3. Optionally generate summary of truncated content
4. Insert summary as system message

**Provider Detection**: Models are mapped to providers via string matching:
```swift
func getProviderForModel(_ model: AIModel) -> AIProvider {
    switch model.provider.lowercased() {
    case "openai": return .openAI
    case "anthropic": return .anthropic
    case "google": return .google
    // ...
    }
}
```

---

## Unified Chat Flow

### Non-Streaming Path

```
ThreadViewModel.sendMessage(text)
    ‚Üì
MultiModelManager.respond(to: text, thread: thread)
    ‚Üì
    ‚îú‚îÄ‚Üí Foundation: foundationModelManager.respond(to: text, thread: thread)
    ‚îú‚îÄ‚Üí MLX: respondWithMLX(to: text, model: mlxModel, thread: thread)
    ‚îî‚îÄ‚Üí Cloud: respondWithAPIWithTokenTracking(to: text, model: model, thread: thread)
           ‚Üì
           MessagesManager.apiMessages(for: thread, model: model)
           ‚Üì
           callOpenAIAPIWithMessages(...) / callAnthropicAPIWithMessages(...) / etc.
    ‚Üì
Chat(content: response, sender: .agent)
```

### Streaming Path

```
ThreadViewModel.sendMessage(text)
    ‚Üì
MultiModelManager.streamResponse(to: text, thread: thread)
    ‚Üì
    ‚îú‚îÄ‚Üí Foundation: foundationModelManager.streamResponse(to: text, thread: thread)
    ‚îú‚îÄ‚Üí MLX: mlxModelManager.generate(model: mlxModel, messages: messages)
    ‚îî‚îÄ‚Üí Cloud: streamViaDirectAPI(text: text, model: model)
           ‚Üì
           callOpenAIStreamingAPI(...) / callAnthropicStreamingAPI(...) / etc.
    ‚Üì
for await chunk in stream {
    agentChat.appendPlaintextChunk(chunk)
}
```

---

## Key Differences Summary

| Feature | Foundation Models | MLX Models | Cloud Models |
|---------|------------------|------------|--------------|
| **Location** | On-device (iOS 26+) | On-device (local files) | Remote API |
| **Cost** | Free | Free | Pay per token |
| **Context Handling** | Automatic (session transcript) | Manual (message array) | Manual (message array) |
| **Requires Thread** | Yes | No (optional) | No (optional) |
| **Memory Management** | Internal | External (cache) | N/A |
| **Token Limits** | Framework handles | Model-specific | Provider-specific |
| **API Key** | No | No | Yes |
| **Availability Check** | `SystemLanguageModel.default.availability` | File system check | API key check |
| **Streaming** | `session.streamResponse()` | `generate()` returns stream | SSE/streaming API |
| **Cancellation** | AsyncStream termination | `cancelGeneration()` | Task cancellation |
| **Fallback Support** | Yes (to cloud) | No | N/A |
| **Safety Guardrails** | Built-in | None | Provider-specific |

---

## Memory & Context Management

### Foundation Models
- Context managed internally by `LanguageModelSession`
- No manual message building required
- Thread parameter passed for consistency but not used for context

### MLX Models
```swift
// Memory enabled: Include full conversation history
if settings.globalMemoryEnabled && settings.isMemoryEnabled(for: thread.id) {
    messages = buildMLXMessages(for: text, thread: thread, model: mlxModel)
}
// Memory disabled: Only current message
else {
    messages = [
        ["role": "system", "content": systemPrompt],
        ["role": "user", "content": text]
    ]
}
```

### Cloud Models
```swift
// Token-aware context building with truncation
let contextResult = await messagesManager.apiMessages(
    for: thread,
    respectTokenLimits: true  // Enable smart truncation
)

if contextResult.tokenCount > effectiveLimit {
    // Truncate oldest messages
    // Generate summary if needed
    // Insert summary as system message
}
```

---

## Error Handling & Fallback

### Foundation Model Fallback
When Foundation Models fail (safety guardrails, unavailable, etc.):

```swift
// MultiModelManager.respondWithLocalModel()
catch FoundationModelManager.FoundationModelManagerError.safetyGuardrailsTriggered {
    foundationModelFailureCount += 1
    
    if autoFallbackEnabled && foundationModelFailureCount >= maxFoundationFailures {
        // Automatic fallback to cloud model
        return try await attemptFallbackResponse(to: text, thread: thread, originalError: error)
    }
    
    throw MultiModelError.foundationModelUnavailable(reason: "safety", fallbackUsed: true)
}
```

Fallback model selection:
```swift
func selectFallbackModel() -> AIModel? {
    for modelId in preferredFallbackOrder {
        if let model = availableModels.first(where: { $0.id == modelId }) {
            return model
        }
    }
    // Default to first available cloud model
    return availableModels.first(where: { !$0.isLocal && $0.requiresAPIKey })
}
```

---

## Performance Optimizations

### Model Caching
```swift
// MLX: Cache loaded models to avoid reload
let modelCache = NSCache<NSString, MLXLMCommon.ModelContainer>()
```

### Token Estimation
```swift
// Fast token estimation (4 chars ‚âà 1 token)
func estimateTokens(in text: String) -> Int {
    return max(1, text.count / 4)
}
```

### Context Truncation
```swift
// Only rebuild context when model changes or chats added
if thread.needsContextRecalculation(for: model.id) {
    let result = await messagesManager.rebuildContextForModel(model, thread: thread)
    thread.updateContextCache(modelId: model.id, tokenCount: result.tokenCount)
}
```

### View Updates
```swift
// Throttle view updates during streaming
if agentChat.content.count % 50 == 0 {
    scheduleViewUpdate()
}
```

---

## Testing & Debugging

### Model Availability Checks
```swift
// Foundation Models
if #available(iOS 26.0, *) {
    switch SystemLanguageModel.default.availability {
    case .available: // Ready
    case .unavailable(.deviceNotEligible): // Device too old
    case .unavailable(.appleIntelligenceNotEnabled): // Setting disabled
    case .unavailable(.modelNotReady): // Downloading
    }
}

// MLX Models
#if targetEnvironment(simulator)
    return false  // MLX doesn't work on simulator
#else
    return settings.mlxEnabled && !downloadedModels.isEmpty
#endif

// Cloud Models
guard apiKeyManager.hasAPIKey(for: provider) else {
    throw MultiModelError.missingAPIKey(provider: provider.displayName)
}
```

### Logging
Each model type has debug logging:
```swift
print("üîç [Respond] Using Foundation model (iOS 26+)")
print("üîµ [MLX Response] Starting generation with model: \(model.displayName)")
print("üîç [Direct API] Retrieved API key: \(String(apiKey.prefix(10)))...")
```

---

## Conclusion

ChAI's chat architecture provides a unified interface that seamlessly handles three very different model types:

1. **Foundation Models**: Leverage Apple's on-device AI with automatic context management
2. **MLX Models**: Run quantized models locally with full control over message arrays
3. **Cloud Models**: Access external APIs with intelligent token-aware context building

The `MultiModelManager` acts as the central orchestrator, routing requests to the appropriate handler while providing consistent error handling, token tracking, and streaming support across all model types.
--- END FILE ---

--- FILE: MLTrainingSession.md ---
# MLTrainingSession

**The current state of a model‚Äôs asynchronous training session.**

## Availability

- **iOS** 15.0+
- **iPadOS** 15.0+
- **Mac Catalyst** 15.0+
- **macOS** 11.0+
- **tvOS** 16.0+
- **visionOS** 1.0+


## Topics

### Checking a training session‚Äôs progress

- [phase](https://developer.apple.com/documentation/createml/mltrainingsession/phase) ‚Äî The training session‚Äôs current state.
- [MLPhase](https://developer.apple.com/documentation/createml/mlphase) ‚Äî The possible states of a training session.
- [iteration](https://developer.apple.com/documentation/createml/mltrainingsession/iteration) ‚Äî The iteration number of a training session‚Äôs phase.
- [checkpoints](https://developer.apple.com/documentation/createml/mltrainingsession/checkpoints) ‚Äî An array of checkpoints the training session has created so far.
### Removing checkpoints

- [removeCheckpoints(_:)](https://developer.apple.com/documentation/createml/mltrainingsession/removecheckpoints(_:)) ‚Äî Removes the checkpoints that satisfy your closure from the training session.
### Reusing features from a previous session

- [reuseExtractedFeatures(from:)](https://developer.apple.com/documentation/createml/mltrainingsession/reuseextractedfeatures(from:)) ‚Äî Uses the features another session has already extracted from its dataset.
### Inspecting a session

- [date](https://developer.apple.com/documentation/createml/mltrainingsession/date) ‚Äî The time when you created this training session.
- [parameters](https://developer.apple.com/documentation/createml/mltrainingsession/parameters) ‚Äî The parameters you used to create the training session.

---

*Source: [https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLTrainingSession](https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLTrainingSession)*
--- END FILE ---

--- FILE: MLCheckpoint.md ---
# MLCheckpoint

**The state of a model‚Äôs asynchronous training session at a specific point in time during the feature extraction or training phase.**

## Availability

- **iOS** 15.0+
- **iPadOS** 15.0+
- **Mac Catalyst** 15.0+
- **macOS** 11.0+
- **tvOS** 16.0+
- **visionOS** 1.0+


## Topics

### Inspecting a checkpoint

- [phase](https://developer.apple.com/documentation/createml/mlcheckpoint/phase) ‚Äî The training session‚Äôs phase when it created the checkpoint.
- [iteration](https://developer.apple.com/documentation/createml/mlcheckpoint/iteration) ‚Äî The iteration number of a training session‚Äôs phase when it created the checkpoint.
- [date](https://developer.apple.com/documentation/createml/mlcheckpoint/date) ‚Äî The time when the training session created the checkpoint.
- [url](https://developer.apple.com/documentation/createml/mlcheckpoint/url) ‚Äî The location of the checkpoint in the file system.
### Assessing a checkpoint

- [metrics](https://developer.apple.com/documentation/createml/mlcheckpoint/metrics) ‚Äî Measurements of the model‚Äôs performance at the time the session saved the checkpoint.
- [MLProgress.Metric](https://developer.apple.com/documentation/createml/mlprogress/metric) ‚Äî Metrics you use to evaluate a model‚Äôs performance during a training session.
### Encoding and decoding a checkpoint

- [encode(to:)](https://developer.apple.com/documentation/createml/mlcheckpoint/encode(to:)) ‚Äî Encodes the checkpoint into the encoder.
- [init(from:)](https://developer.apple.com/documentation/createml/mlcheckpoint/init(from:)) ‚Äî Creates a new checkpoint by decoding from the decoder.

---

*Source: [https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLCheckpoint](https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLCheckpoint)*
--- END FILE ---

--- FILE: MLTrainingSessionParameters.md ---
# MLTrainingSessionParameters

**The configuration settings for a training session.**

## Availability

- **iOS** 15.0+
- **iPadOS** 15.0+
- **Mac Catalyst** 15.0+
- **macOS** 11.0+
- **tvOS** 16.0+
- **visionOS** 1.0+


## Topics

### Creating a session‚Äôs parameters

- [init(sessionDirectory:reportInterval:checkpointInterval:iterations:)](https://developer.apple.com/documentation/createml/mltrainingsessionparameters/init(sessiondirectory:reportinterval:checkpointinterval:iterations:)) ‚Äî Creates a set of parameters for a training session.
### Configuring the session‚Äôs parameters

- [sessionDirectory](https://developer.apple.com/documentation/createml/mltrainingsessionparameters/sessiondirectory) ‚Äî The location in the file system where the session stores its progress.
- [reportInterval](https://developer.apple.com/documentation/createml/mltrainingsessionparameters/reportinterval) ‚Äî The number of iterations the session completes before it reports its progress.
- [checkpointInterval](https://developer.apple.com/documentation/createml/mltrainingsessionparameters/checkpointinterval) ‚Äî The number of iterations the session completes before it saves a checkpoint.
- [iterations](https://developer.apple.com/documentation/createml/mltrainingsessionparameters/iterations) ‚Äî The maximum number of iterations for the training session.

---

*Source: [https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLTrainingSessionParameters](https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLTrainingSessionParameters)*
--- END FILE ---

--- FILE: Model-Personalization.md ---
# Model Personalization

**Update your model to adapt to new data.**


## Overview

On-device model updates provide your app the flexibility to personalize a user‚Äôs experience. For example, ordering a mocha at your favorite coffee shop every day increases a model‚Äôs ability to recommend that drink on subsequent visits. With the Core ML framework, you can adapt to incoming data with an updatable model at runtime on the user‚Äôs device.

## Topics

### On-device model updates

- [MLTask](https://developer.apple.com/documentation/coreml/mltask) ‚Äî An abstract base class for machine learning tasks.
- [Personalizing a Model with On-Device Updates](https://developer.apple.com/documentation/coreml/personalizing-a-model-with-on-device-updates) ‚Äî Modify an updatable Core ML model by running an update task with labeled data.
- [MLUpdateTask](https://developer.apple.com/documentation/coreml/mlupdatetask) ‚Äî A task that updates a model with additional training data.

---

*Source: [https://developer.apple.com/documentation/com.apple.coreml/documentation/CoreML/model-personalization](https://developer.apple.com/documentation/com.apple.coreml/documentation/CoreML/model-personalization)*
--- END FILE ---

--- FILE: Meet_the_Foundation_Models_framework.md ---
# Meet the Foundation Models framework

Hi, I‚Äôm Erik. And I‚Äôm Yifei. And today, we are so excited to get the privilege of introducing you to the new Foundation Models framework! The Foundation Models framework gives you access to the on-device Large Language Model that powers Apple Intelligence, with a convenient and powerful Swift API. It is available on macOS, iOS, iPadOS, and visionOS! You can use it to enhance existing features in your apps, like providing personalized search suggestions. Or you can create completely new features, like generating an itinerary in a travel app, all on-device. You can even use it to create dialog on-the-fly for characters in a game.

It is optimized for generating content, summarizing text, analyzing user input and so much more.

All of this runs on-device, so all data going into and out of the model stays private. That also means it can run offline! And it‚Äôs built into the operating system, so it won‚Äôt increase your app size. It‚Äôs a huge year, so to help you get the most out of the FoundationModels framework, we‚Äôve prepared a series of videos. In this first video, we‚Äôll be giving you a high level overview of the framework in its entirety. Starting with the details of the model.

We will then introduce guided generation which allows you to get structured output in Swift, and the powerful streaming APIs that turn latency into delight.

We will also talk about tool calling, which allows the model to autonomously execute code you define in your app.

Finally, we will finish up with how we provide multi-turn support with stateful sessions, and how we seamlessly integrate the framework into the Apple developer ecosystem. The most important part of the framework, of course, is the model that powers it. And the best way to get started with prompting the model, is to jump into Xcode.

Testing out a variety of prompts to find what works best is an important part of building with large language models, and the new Playgrounds feature in Xcode is the best way to do that. With just a few lines of code, you can immediately start prompting the on-device model. Here I'll ask it to generate a title for a trip to Japan, and the model's output will appear in the canvas on the right. 

```swift
import FoundationModels
import Playgrounds

#Playground {
    let session = LanguageModelSession()
    let response = try await session.respond(to: "What's a good name for a trip to Japan? Respond only with a title")
}
```

Now, I want to see if this prompt works well for other destinations too. In a #Playground, you can access types defined in your app, so I'll create a for loop over the landmarks featured in mine. Now Xcode will show me the model's response for all of the landmarks.

```swift
import FoundationModels
import Playgrounds

#Playground {
    let session = LanguageModelSession()
    for landmark in ModelData.shared.landmarks {
        let response = try await session.respond(to: "What's a good name for a trip to \(landmark.name)? Respond only with a title")
    }
}
```

The on-device model we just used is a large language model with 3 billion parameters, each quantized to 2 bits. It is several orders of magnitude bigger than any other models that are part of the operating system.

But even so, it‚Äôs important to keep in mind that the on-device model is a device-scale model. It is optimized for use cases like summarization, extraction, classification, and many more. It‚Äôs not designed for world knowledge or advanced reasoning, which are tasks you might typically use server-scale LLMs for.

Device scale models require tasks to be broken down into smaller pieces. As you work with the model, you‚Äôll develop an intuition for its strengths and weaknesses.

For certain common use cases, such as content tagging, we also provide specialized adapters that maximize the model‚Äôs capability in specific domains.

We will also continue to improve our models over time. Later in this video we‚Äôll talk about how you can tell us how you use our models, which will help us to improve them in ways that matter to you.

Now that we've taken a look at the model, the first stop on our journey is Guided Generation. Guided Generation is what makes it possible to build features like the ones you just saw, and it is the beating heart of the FoundationModels framework. Let's take a look at a common problem and talk about how Guided Generation solves it.

By default, language models produce unstructured, natural language as output. It's easy for humans to read, but difficult to map onto views in your app.

A common solution is to prompt the model to produce something that's easy to parse, like JSON or CSV.

However, that quickly turns into a game of whack-a-mole. You have to add increasingly specific instructions about what it it is and isn't supposed to do‚Ä¶ Often that doesn't quite work‚Ä¶ So you end up writing hacks to extract and patch the content. This isn't reliable because the model is probabilistic and there is a non-zero chance of structural mistakes. Guided Generation offers a fundamental solution to this problem.

When you import FoundationModels, you get access to two new macros, @Generable and @Guide. Generable let's you describe a type that you want the model to generate an instance of.

```swift
// Creating a Generable struct

@Generable
struct SearchSuggestions {
    @Guide(description: "A list of suggested search terms", .count(4))
    var searchTerms: [String]
}
```

Additionally, Guides let you provide natural language descriptions of properties, and programmatically control the values that can be generated for those properties.

Once you've defined a Generable type, you can make the model respond to prompts by generating an instance of your type. This is really powerful.

```swift
// Responding with a Generable type

let prompt = """
    Generate a list of suggested search terms for an app about visiting famous landmarks.
    """

let response = try await session.respond(
    to: prompt,
    generating: SearchSuggestions.self
)

print(response.content)
```

Observe how our prompt no longer needs to specify the output format. The framework takes care of that for you.

The most important part, of course, is that we now get back a rich Swift object that we can easily map onto an engaging view.

Generable types can be constructed using primitives, like Strings, Integers, Doubles, Floats, and Decimals, and Booleans. Arrays are also generable. And Generable types can be composed as well. Generable even supports recursive types, which have powerful applications in domains like generative UIs.

```swift
// Composing Generable types

@Generable struct Itinerary {
    var destination: String
    var days: Int
    var budget: Float
    var rating: Double
    var requiresVisa: Bool
    var activities: [String]
    var emergencyContact: Person
    var relatedItineraries: [Itinerary]
}
```

The most important thing to understand about Guided Generation is that it fundamentally guarantees structural correctness using a technique called constrained decoding.

When using Guided Generation, your prompts can be simpler and focused on desired behavior instead of the format.

Additionally, Guided Generation tends to improve model accuracy. And, it allows us to perform optimizations that speed up inference at the same time. This is all made possible by carefully coordinated integration of Apple operating systems, developer tools, and the training of our foundation models. There is still a lot more to cover about guided generation, like how to create dynamic schemas at runtime, so please check out our deep dive video for more details. So that wraps up Guided Generation ‚Äî we‚Äôve seen how Swift‚Äôs powerful type system augments natural language prompts to enable reliable structured output. Our next topic is streaming, and it all builds on top of the @Generable macro you‚Äôre already familiar with.

If you‚Äôve worked with large language models before, you may be aware that they generate text as short groups of characters called tokens.

Typically when streaming output, tokens are delivered in what‚Äôs called a delta, but the FoundationModels framework actually takes a different approach, and I want to show you why.

As deltas are produced, the responsibility for accumulating them usually falls on the developer.

You append each delta as they come in. And the response grows as you do.

But it gets tricky when the result has structure. If you want to show the greeting string after each delta, you have to parse it out of the accumulation, and that‚Äôs not trivial, especially for complicated structures. Delta streaming just isn‚Äôt the right formula when working with structured output.

And as you‚Äôve learned, structured output is at the very core of the FoundationModels framework, which is why we‚Äôve developed a different approach. Instead of raw deltas, we stream snapshots.

As the model produces deltas, the framework transforms them into snapshots. Snapshots represent partially generated responses. Their properties are all optional. And they get filled in as the model produces more of the response.

Snapshots are a robust and convenient representation for streaming structured output.

You're already familiar with the @Generable macro, and as it turns out, it's also where the definitions for partially generated types come from. If you expand the macro, you'll discover it produces a type named `PartiallyGenerated`. It is effectively a mirror of the outer structure, except every property is optional.

```swift
// PartiallyGenerated types

@Generable struct Itinerary {
    var name: String
    var days: [Day]
}
```

The partially generated type comes into play when you call the `streamResponse` method on your session.

```swift
// Streaming partial generations

let stream = session.streamResponse(
    to: "Craft a 3-day itinerary to Mt. Fuji.",
    generating: Itinerary.self
)

for try await partial in stream {
    print(partial)
}
```

Stream response returns an async sequence. And the elements of that sequence are instances of a partially generated type. Each element in the sequence will contain an updated snapshot.

These snapshots work great with declarative frameworks like SwiftUI. First, create state holding a partially generated type.

Then, just iterate over a response stream, store its elements, and watch as your UI comes to life.

```swift
ItineraryView: View {
    let session: LanguageModelSession
    let dayCount: Int
    let landmarkName: String
  
    @State
    private var itinerary: Itinerary.PartiallyGenerated?
  
    var body: some View {
        //...
        Button("Start") {
            Task {
                do {
                    let prompt = """
                        Generate a \(dayCount) itinerary \
                        to \(landmarkName).
                        """
                  
                    let stream = session.streamResponse(
                        to: prompt,
                        generating: Itinerary.self
                    )
                  
                    for try await partial in stream {
                        self.itinerary = partial
                    }
                } catch {
                    print(error)  
                }
            }
        }
    }
}
```

To wrap up, let's review some best practices for streaming.

First, get creative with SwiftUI animations and transitions to hide latency. You have an opportunity turn a moment of waiting into one of delight. Second, you'll need to think carefully about view identity in SwiftUI, especially when generating arrays. Finally, bear in mind that properties are generated in the order they are declared on your Swift struct. This matters both for animations and for the quality of the model's output. For example, you may find that the model produces the best summaries when they're the last property in the struct.

```swift
@Generable struct Itinerary {
  
  @Guide(description: "Plans for each day")
  var days: [DayPlan]
  
  @Guide(description: "A brief summary of plans")
  var summary: String
}
```

There is a lot to unpack here, so make sure to check out our video on integrating Foundation Models into your app for more details. So that wraps up streaming with Foundation Models. Next up, Yifei is going to teach you all about tool calling! Thanks Erik! Tool calling is another one of our key features. It lets the model execute code you define in your app. This feature is especially important for getting the most out of our model, since tool calling gives the model many additional capabilities. It allows the model to identify that a task may require additional information or actions and autonomously make decisions about what tool to use and when, when it‚Äôs difficult to decide programmatically.

The information you provide to the model can be world knowledge, recent events, or personal data. For example, in our travel app, it provides information about various locations from MapKit. This also gives the model the ability to cite sources of truth, which can suppress hallucinations and allow fact-checking the model output.

Finally, it allows the model to take actions, whether it‚Äôs in your app, on the system, or in the real world.

Integrating with various sources of information in your app is a winning strategy for building compelling experiences. Now that you know why tool calling is very useful, let‚Äôs take a look at how it works.

On the left we have a transcript which records everything that has happened so far. If you‚Äôve provided tools to the session, the session will present these tools to the model along with the instructions. Next comes the prompt, where we tell the model which destination we want to visit.

Now, if the model deems that calling a tool can enhance the response, it will produce one or more tool calls. In this example, the model produces two tool calls ‚Äî querying restaurants and hotels.

At this phase, the FoundationModels framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the transcript. Finally, the model will incorporate the tool output along with everything else in the transcript to furnish the final response.

Now that we have a high level understanding of tool calling, let's define a tool.

Here we're defining a simple weather tool, which conforms to the Tool protocol. The weather tool has kind of emerged as the de-facto 'hello world' of tool calling, and it's a great way to get started.

The protocol first requires you to specify a name and a natural language description of the tool.

The framework will automatically provide them for the model to help it understand when to call your tool.

When the model calls your tool, it will run the call method you define.

The argument to the call method can be any Generable type.

The reason your arguments need to be generable is because tool calling is built on guided generation to ensure that the model will never produce invalid tool names or arguments.

After defining your arguments type, you can now write anything you want in the body of your method. Here we're using CoreLocation and WeatherKit to find the temperature of a given city. The output is represented using the ToolOutput type, which can be created from GeneratedContent to represent structured data. Or from a string if your tool's output is natural language. 

```swift
// Defining a tool
import WeatherKit
import CoreLocation
import FoundationModels

struct GetWeatherTool: Tool {
    let name = "getWeather"
    let description = "Retrieve the latest weather information for a city"

    @Generable
    struct Arguments {
        @Guide(description: "The city to fetch the weather for")
        var city: String
    }

    func call(arguments: Arguments) async throws -> ToolOutput {
        let places = try await CLGeocoder().geocodeAddressString(arguments.city)
        let weather = try await WeatherService.shared.weather(for: places.first!.location!)
        let temperature = weather.currentWeather.temperature.value

        let content = GeneratedContent(properties: ["temperature": temperature])
        let output = ToolOutput(content)

        // Or if your tool's output is natural language:
        // let output = ToolOutput("\(arguments.city)'s temperature is \(temperature) degrees.")

        return output
    }
}
```

Now that we have defined a tool, we have to ensure that the model has access to it.

To do so, pass your tool into your session's initializer. Tools must be attached at session initialization, and will be available to the model for the session's lifetime.

```swift
// Attaching tools to a session

let session = LanguageModelSession(
    tools: [GetWeatherTool()],
    instructions: "Help the user with weather forecasts."
)

let response = try await session.respond(
    to: "What is the temperature in Cupertino?"
)

print(response.content)
// It's 71ÀöF in Cupertino!
```

After creating a session with tools, all you need to do is prompt the model as you would normally. Tool calls will happen transparently and autonomously, and the model will incorporate the tools' outputs into its final response. The examples I‚Äôve shown here demonstrate how to define type-safe tools at compile time, which is great for the vast majority of use cases. But tools can also be dynamic in every way! For example, you can define the arguments and behaviors of a tool at runtime by using dynamic generation schemas. If you are curious about that, feel free to check out our deep dive video to learn more.

That wraps up tool calling. We learned why tool calling is useful and how to implement tools to extend the model's capabilities. Next, let's talk about stateful sessions. You've seen the word session pop up in this video many times already. The Foundation Models framework is built around the notion of a stateful session. By default, when you create a session, you will be prompting the on-device general-purpose model. And you can provide custom instructions.

Instructions are an opportunity for you to tell the model its role and provide guidance on how the model should respond. For example, you can specify things like style and verbosity.

```swift
// Supplying custom instructions

let session = LanguageModelSession(
    instructions: """
        You are a helpful assistant who always \
        responds in rhyme.
        """
)
```

Note that providing custom instructions is optional, and reasonable default instructions will be used if you don't specify any.

If you do choose to provide custom instructions, it is important to understand the difference between instructions and prompts. Instructions should come from you, the developer, while prompts can come from the user. This is because the model is trained to obey instructions over prompts. This helps protect against prompt injection attacks, but is by no means bullet proof.

As a general rule, instructions are mostly static, and it‚Äôs best not to interpolate untrusted user input into the instructions.

So this is a basic primer on how to best form your instructions and prompts. To discover even more best practices, check out our video on prompt design and safety.

Now that you have initialized a session, let's talk about multi-turn interactions! When using the respond or streamResponse methods we talked about earlier. Each interaction with the model is retained as context in a transcript, so the model will be able to refer to and understand past multi-turn interactions within a single session. For example, here the model is able to understand when we say "do another one", that we're referring back to writing a haiku.

```swift
// Multi-turn interactions

let session = LanguageModelSession()

let firstHaiku = try await session.respond(to: "Write a haiku about fishing")
print(firstHaiku.content)
// Silent waters gleam,
// Casting lines in morning mist‚Äî
// Hope in every cast.

let secondHaiku = try await session.respond(to: "Do another one about golf")
print(secondHaiku.content)
// Silent morning dew,
// Caddies guide with gentle words‚Äî
// Paths of patience tread.

print(session.transcript)// (Prompt) Write a haiku about fishing
// (Response) Silent waters gleam...
// (Prompt) Do another one about golf
// (Response) Silent morning dew...
```

And the `transcript` property on the session object will allow you to inspect previous interactions or draw UI views to represent them.

One more important thing to know is that while the model is producing output, its `isResponding` property will become `true`. You may need to observe this property and make sure not to submit another prompt until the model finishes responding.

```swift
import SwiftUI
import FoundationModels

struct HaikuView: View {

    @State
    private var session = LanguageModelSession()

    @State
    private var haiku: String?

    var body: some View {
        if let haiku {
            Text(haiku)
        }
        Button("Go!") {
            Task {
                haiku = try await session.respond(
                    to: "Write a haiku about something you haven't yet"
                ).content
            }
        }
        // Gate on `isResponding`
        .disabled(session.isResponding)
    }
}
```

Beyond the default model, we are also providing additional built-in specialized use cases that are backed by adapters.

If you find a built-in use case that fits your need, you can pass it to SystemLanguageModel's initializer. 

```swift
// Using a built-in use case

let session = LanguageModelSession(
    model: SystemLanguageModel(useCase: .contentTagging)
)
```

To understand what built-in use cases are available and how to best utilize them, check out our documentation on the developer website. One specialized adapter I want to talk more about today is the content tagging adapter. The content tagging adapter provides first class support for tag generation, entity extraction, and topic detection. By default, the adapter is trained to output topic tags, and it integrates with guided generation out of the box. So you can simply define a struct with our Generable macro, and pass the user input to extract topics from it.

```swift
// Content tagging use case

@Generable
struct Result {
    let topics: [String]
}

let session = LanguageModelSession(model: SystemLanguageModel(useCase: .contentTagging))
let response = try await session.respond(to: ..., generating: Result.self)
```

But there's more! By providing it with custom instructions and a custom Generable output type, you can even use it to detect things like actions and emotions.

```swift
// Content tagging use case

@Generable
struct Top3ActionEmotionResult {
    @Guide(.maximumCount(3))
    let actions: [String]
    @Guide(.maximumCount(3))
    let emotions: [String]
}

let session = LanguageModelSession(
    model: SystemLanguageModel(useCase: .contentTagging),
    instructions: "Tag the 3 most important actions and emotions in the given input text."
)
let response = try await session.respond(to: ..., generating: Top3ActionEmotionResult.self)
```

Before you create a session, you should also check for availability, since the model can only run on Apple Intelligence-enabled devices in supported regions. To check if the model is currently available, you can access the availability property on the SystemLanguageModel.

Availability is a two case enum that's either available or unavailable. If it's unavailable, you also receive a reason so you can adjust your UI accordingly.

```swift
// Availability checking

struct AvailabilityExample: View {
    private let model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            Text("Model is available").foregroundStyle(.green)
        case .unavailable(let reason):
            Text("Model is unavailable").foregroundStyle(.red)
            Text("Reason: \(reason)")
        }
    }
}
```

Lastly, you could encounter errors when you are calling into the model.

These errors might include guardrail violation, unsupported language, or context window exceeded. To provide the best user experience, you should handle them appropriately, and the deep-dive video will teach you more about them. That‚Äôs it for multi-turn stateful sessions! We learned how to create a session and use it, as well as how our model keeps track of your context. Now that you‚Äôve seen all the cool features of the framework, let‚Äôs talk about developer tooling and experience. To start, you can go to any Swift file in your project and use the new playground macro to prompt the model.

Playgrounds are powerful because they let you quickly iterate on your prompts without having to rebuild and rerun your entire app.

In a playground, your code can access all the types in your project, such as the generable types that are already powering your UI.

Next, we know that when it comes to building app experiences powered by large language models, it is important to understand all the latency under the hood, because large language models take longer to run compared to traditional ML models. Understanding where latency goes can help you tweak the verbosity of your prompts, or determine when to call useful APIs such as prewarming.

And our new Instruments app profiling template is built exactly for that. You can profile the latency of a model request, observe areas of optimizations, and quantify improvements.

Now, as you develop your app, you may have feedback that can help us improve our models and our APIs.

We encourage you to provide your feedback through Feedback Assistant. We even provide an encodable feedback attachment data structure that you can attach as a file to your feedback.

```swift
let feedback = LanguageModelFeedbackAttachment(
  input: [
    // ...
  ],
  output: [
    // ...
  ],
  sentiment: .negative,
  issues: [
    LanguageModelFeedbackAttachment.Issue(
      category: .incorrect,
      explanation: "..."
    )
  ],
  desiredOutputExamples: [
    [
      // ...
    ]
  ]
)
let data = try JSONEncoder().encode(feedback)
```

Finally, if you are an ML practitioner with a highly specialized use case and a custom dataset, you can also train your custom adapters using our adapter training toolkit. But bear in mind, this comes with significant responsibilities because you need to retrain it as Apple improves the model over time. To learn more, you can visit the developer website. 

Now that you've learned many of the cool features provided by the new Foundation Models framework, we can't wait to see all the amazing things you build with it! To discover even more about how you can integrate generative AI into your app, how technologies like guided generation work under the hood, and how you can create the best prompts, we have a whole series of wonderful videos and articles for you.

Thank you so much for joining us today! Happy generating!
--- END FILE ---

--- FILE: Code-along-with-the-foundation-models-framework.md ---
Hello everyone and welcome to the Foundation Models Framework Code Along. My name is Shashank. I'm a technology evangelist here at Apple and today I'm excited to guide you through integrating on-device generative AI features directly into your app. We'll cover everything from basic prompting to generating structured output, streaming responses and more. we have an incredible team of experts in Slido. If you have any questions at any point, please ask there.

Let's start with a quick overview to get everyone on the same page. At WWDC24, we introduced Apple Intelligence, powered by large foundation models built into the core of our operating systems. This brought system level features like Writing Tools and Genmoji. Many of you have asked for access to the underlying models and at WWDC25, we delivered with the Foundation Models Framework. It gives you direct access to the same on-device large language model that powers Apple intelligence, all through a powerful Swift API. For developers, this on-device approach has major advantages. Because everything runs locally, user data remains private. Your features work entirely offline with no accounts to set up or API keys to manage. There's no cost to you or someone using the app for any of these requests. And since it's all part of the OS, there's no impact on your app size.

Today, we're gonna build an app together. We'll start with a simple static app that lists landmarks and transform it into a dynamic travel planner. You learn how to generate rich structure itineraries for a custom UI, stream the results in real time as they're created. You'll also learn how to give the model access to custom tools to find real points of interest. And finally, how to optimize your app for performance.

Let's do a quick tour of the final app you'll be building.

Here is a completed app running on my Mac. And this is what you'll have by the end of our session today. We'll start with a simple, clean list of famous landmarks built using SwiftUI. Let's pick a landmark. How about Serengeti? When we click into the detail view, you see a header image and description.

At the bottom is our generate itinerary button. When I click this, the app will call the on-device model to generate a complete three-day travel plan. Watch the screen closely as this happens.

The UI is building itself in real time. First the title, then the description, then the day-by-day plan. This is the streaming API we'll incorporate in chapter four, and it creates a fantastic dynamic user experience. And this here isn't just a block of code, it's rich structured response, which we learn about in chapter two. We have distinct sections for day with a title, subtitle, and a map. Notice the names here like Hotel 1 and Restaurant 1. These aren't random. Our app is using tool calling to get these names, which we'll cover in chapter five. The foundation model framework lets you create rich, structured, and intelligent experiences that feel seamlessly integrated into your app. This is what we're going to build together today. To get the most out of today's Code Along, you have three key resources. First is the Xcode startup project. It has all the boilerplate UI and assets ready to go. If you're watching on developer.apple.com or the developer app, You'll find this under resources on the bottom of the page. If you're watching this on YouTube, it's linked from the description.

Second is our step by step guide on the web page. This is your source of truth with all the instructions and code snippets. You can simply copy paste these to avoid typos. And finally, you have me on the live stream and team of experts behind the scenes to answer your questions. I'll be building this project right here with you, explaining the why behind each change.

Before we jump into the settings and set up the project, let's quickly go over the system requirements for today's session. Everyone is welcome to watch and follow along. However, if you plan on coding live with me, you'll need an Apple Silicon based Mac running macOS Tahoe and Xcode 26. You'll also need to make sure that Apple intelligence is turned on under settings. I'll be building and running the app directly on my Mac today, but you can also use Xcode 26 with a recent iPhone running iOS 26 as your target.

With that, let's move on to the prerequisite section in our Code Along Guide and get our startup project downloaded and configured.

Here in our guide, you'll see a prerequisite section.

First, please click the link to download the project file.

Here, once you've downloaded, you'll find a zip file that macOS may automatically unzip for you. Inside, you'll find a folder named Foundation Models Code Along. This is a startup project we'll be using today. It contains all the necessary views, models, and placeholder code to get us started. I have my project open here and ready to go.

The first thing we need to do is set the developer team. In the project navigator, select the project file.

Then select targets.

Click on signing and capabilities. And under team, select the dropdown and select your team.

To make sure everything is working correctly, select myMac as the run destination in the Xcode toolbar.

and then click on the Run button. This will build and run the project. Alternatively, you can use Command + R.

What I have here is the app that we're gonna be building and adding our generative AI features into. So this is our starting point and we'll be adding powerful features throughout this session. Now let's do a quick tour of our startup project.

First we have here our Playground.swift file under the playgrounds folder. This is where we'll iterate on our prompts and test out foundation models APIs in isolation without having to build and run our entire app. Once we're happy with a prompt here, we'll move this code into our app.

Next is our view models folder and the most important file for us here is itineraryGenerator.swift. All the core logic for creating and managing foundation model sessions, calling the framework APIs and processing the results will live right here. And finally, we have a views folder.

This is where all our SwiftUI code lives. For this code along, the UI is mostly pre-built to let us focus on the Foundation Models Framework. You'll notice that there are several files here and to make it easy to follow, the key files we'll be editing are numbered.

Our job will be to take the output of our itinerary generator and wire it up to these views to create the rich and interactive UI that you'll see in the app.

As you go through these files, you'll notice there are special comments formatted this way. Mark, Code Along, Chapter, and a number. Each number here corresponds directly to the chapter and section with the same number in your Code Along guide. You can use the Xcode Find Navigator to search for the chapter number to see all the outstanding code changes.

Enter the chapter number here, and you'll see all the code changes. As we complete each step, we'll keep deleting these comments so we can track progress throughout the Code Along.

So in summary, we'll follow three simple steps. First, experiment in the playground. Second, implement the core logic in the view model, and finally, display the results in the view. Let's take a closer look at each of these views.

The first screen is a starting point, the main list of landmarks. This is powered by LandmarksView.swift. We won't be touching this file today. It's all set up for us to let us browse and select a destination. When you tap a landmark, you land on the details screen. This view is controlled by the landmark DetailView.swift file. Its main job is to check if the Foundation Models Framework is available on device and decide what UI to show based on that.

Next is the landmark trip view. Its role is to present the generate itinerary button. And this is also where we'll first display the raw unstructured text that we get back from the model.

And finally, the itinerary view is our destination. This view renders the rich structured itinerary data we'll have towards the end of the code along.

We're now ready to dive into the agenda. We've structured the code along into six chapters. We'll start with the absolute basics where you learn how to start prompting the model to generate text. Then we'll move beyond simple text and see how to get structured Swift types back from the model, making it easy to map model output to your custom views. We'll then dive into prompting techniques. That lets you improve models accuracy by providing high quality examples directly in your prompts. Next, we'll learn how to stream the model's response to update the UI in real time for a great user experience. We'll then explore tool calling. Tools are powerful ways to give the model access to your own custom functions and data to extend its capabilities.

And finally, we'll cover performance optimizations to make our generative features feel faster and more responsive. With that, let's dive into the basics of Foundation Models Framework. You can use the Foundation Models Framework to send a prompt to the on-device large language model, or LLM for short. The LLM can then reason about your prompt and generate text. For example, you could ask it to generate a three-day itinerary to Paris, and the model will respond with a detailed plan.

To start prompting the model, you'll need to create a session. The framework is built around this idea of stateful language model session, which maintains a history of all prompts and responses.

In this chapter, we'll get familiar with foundation models prompts and sessions. First, we'll start in the playground to get a feel for the API. We'll create a language model session and get our first response from the model. Then, we'll add concise instructions to shape the tone and content. Next, we look at availability API to handle different states gracefully. Once we are comfortable, we'll switch to the app, update the itinerary generator in a view model, and display the raw text output in our views. So let's head on over to our Code Along Guide.

Our goal in chapter one is to make our very first request on-device language model. We'll use the Xcode Playground to send a simple text prompt and see what happens. This will help us understand the model's basic behavior.

Feel free to copy and paste this code block into Xcode Playground.swift file and you can use this handy copy button on the top right corner. I'll be adding these lines of code step by step and explaining what is going on. Let's head over to our Xcode. open up our Playground.swift file. To prompt a model, you need three simple steps. The first is to import the Foundation Models Framework, which we've already done. The next step is to create a playground.

As soon as you use a playground macro to create a playground, you'll see a canvas show up on the right. If it doesn't, you can always click on editor options and ensure that there's a check mark next to canvas. you can click the refresh button and what that does is run all the code contained within the playground block. Right now you don't see an output because we haven't added anything. Step two in prompting the model is to create a session.

What we have here is let a variable session equal to language model session and you'll see the playground canvas automatically shows what is in the session variable. So you see that there are tools which we'll discuss in a later chapter and then transcript which includes all the conversations that you have with the model.

Step three is to prompt the model.

We say let response equal to try await session dot respond to and provide a prompt generate a three day itinerary to Paris. This is an async request so we await its response.

As soon as we do that, on the right side on the canvas, you'll see we have a response variable which includes a few properties. First is prompt. The prompt shows generate a 3-day itinerary to Paris and then there's a property called content which is of type String.

Let's click on this and you'll see that there's a detailed 3-day itinerary to Paris. Certainly here's a 3-day itinerary for exploring Paris, highlighting some of the city's most iconic sites and experience, and you see day by day plans for day one, morning, afternoon, and so on.

Great. Let's go back to our guide here and discuss a key topic. When you make the very first call to Session.response, you might notice that there's a slight delay. This is because the on-device language model needs to be loaded into memory before it can process your request. Our first request triggers a system to load the model, which causes the initial latency. We'll see how to address this in a later chapter. And we also saw that the output was unstructured natural language text, which is easy for us to read, but hard to use in a custom Swift UI. In the next chapter, we'll see how to generate structured output using Swift types instead of raw text. Finally, it's important to note that the entire itinerary without any data ever leaving your device. It's completely private and works offline. So congratulations, you've successfully prompted the on-device foundation model using the Foundation Models Framework.

Oh, and one last thing, let me head back to our playground.

We are always interested in improving the model, and if you want to provide feedback, you can always use these buttons right here in Canvas to share your feedback with us. Let's head on over to our Code Along Guide to Section 1.2, Guiding the model with instructions.

Our goal now is to get more consistent and higher quality results. We can do this by providing the model with instructions. Think of instructions as permanent rules or persona for the entire conversation within a single session. Feel free again to copy this piece of code into Playground and run it. and I'm going to go and add these instructions.

Back in our Playground.swift file, I add a new variable called instructions, and I say, your job is to create an itinerary for the user. Each day needs an activity, hotel, and restaurant. Always include a title, a short description, and a day-by-day plan. We can pass these instructions into the language model session using the instruction argument. When you pass this, the canvas will automatically detect code changes and update our results. We see now that we have our content property under response and this will include the request that we made, which is include activity, hotel and restaurant, and you can see this here, activity, hotel, and restaurants.

A question you may have is, what is the difference between these instructions and prompts? Let's take a look.

Instructions can be used to define a persona, set rules, and specify desired format for the response. This should come from the developer. Prompts, on the other hand, can come from someone using the app. The model is trained to obey instructions over prompts, and this can help protect against prompt injection attacks where the user may ask the model to ignore guidance provided in the prompt. As a rule, keep the instructions static and avoid inserting user input into them.

Also note that instructions are maintained throughout the session's life. Every interaction is recorded in the session's transcript, and The initial instructions are always the first entry.

Great, we're able to successfully prompt a model and get responses. But it's important to consider that our app might run on devices where Apple intelligence isn't available and showing a non-functional feature can be a bad user experience.

For example, the device may not even support Apple intelligence or The device may support Apple intelligence, but the user has not enabled it. Or the model assets are still downloading and they're not ready for use yet. Let's take a close look at how to handle these cases. We'll head on back to our Code Along Guide. We are now in section 1.3 in our Code Along Guide, Handling Model Availability.

The model provides APIs for availability.

Let's head on over to Xcode and take a closer look at each of these cases in this switch block and what they mean for your app.

Back in our Playground.swift file, a neat feature of playground is you can add multiple of these in the same Swift file.

I added a new # playground block here that includes the availability code. All right. Let's take a look at these APIs. You can also check the output of the multiple playgrounds. The second playground will show up as a second tab here on our canvas. And you'll see my Mac does support Apple intelligence. So it says foundation model is available and ready to go. Let's take a closer look at these cases now.

The first case is available. This means you have a green light. the model is loaded and you're ready to make generation requests.

If it says unavailable and device not eligible, this means the model doesn't support Apple Intelligence. You should gracefully hide the generative UI and show an alternate experience.

For unavailable and Apple Intelligence not enabled, this means the device is capable, but Apple Intelligence is turned off in settings. This is your chance to prompt the user to enable it.

Unavailable and model not ready, this is a temporary state, likely because the model assets are still downloading. The best practice is to tell the user to try again. We're now ready to add these features into our app. Let's head on over to our Code Along Guide.

We're now in the app section of chapter one. In this section, we'll update our landmark DetailView.swift to check the model availability and display a message if it is unavailable.

Feel free to copy these code blocks. You can search for these marked comments to know exactly where to insert these code changes and I'll be doing this live with you. Let's head on over to our Xcode project and click on landmark detail view.swift in the views folder. Again, as a reminder, you can always use the find navigator to look for all the code changes that you need to make in this chapter. All right, the first thing to do is to add our model instance.

So we say private let model equal to system language model.default. This is exactly the same line of code we used in our playground, so it should look familiar to you. And since I've added this, I'm gonna delete this comment. So it disappears from our find navigator. The next code change we need to do is to delete this placeholder availability code I have here. This was purely for convenience, so I'm gonna delete this. And as soon as I do that, Xcode will promptly remind me that availability has not been defined yet, but that's an easy fix because we have our model now, model.availability.

And I'm gonna get rid of this line of code too. Okay? With that code change, we've made all the changes to this specific file. Now, we've added these availability checks, which is familiar to you because we use the same in the playground, but how do you test them? You may not have access to multiple test devices. Thankfully, there's an easy way. Right here in the scheme settings in the project, there's an option to simulate unavailability. Let's take a look. Click on foundation models code along, click edit scheme, and if you scroll down, you'll see an option that says simulated foundation models availability.

If you click this, there are a few different options, and these options should be familiar to you because these are the cases we covered in the playground. So I'm gonna click Apple Intelligence Not Enabled, close, and I'm going to build and run our app.

We have our app here, I'm gonna select Sahara Desert, and aha, I see a message here that says, Trip Planner is unavailable because Apple Intelligence has not been turned on.

And this is the same message we have in our unavailability view.

Great. Let me switch this back so we can keep adding additional features throughout the code along.

All right. Let's head on over to section 1.5 in our Code Along Guide.

Now we are ready to update the app's itinerary generator to initialize a language model session and define a function called generate itinerary to invoke the model from our views. The code again should look familiar to you because we already implemented this in the code along. Now we'll be migrating this into our app. So let's head on over to Xcode and open our itinerary Generator.swift file, which you will find in the view models folder.

We'll again use our find navigator to look for all the code changes that we need to make and track progress. All right. In itinerary Generator.swift file, the first change we'll need to make is to add a session property.

And I'm going to do that first. So we define a variable called session for language model session.

Next, Xcode will remind us that we have not initialized a session. So we are going to initialize this session right here in the init functions.

Okay, so here's what we added. We added an instructions variable where we use the same instructions we had in the playground. Your job is to create an itinerary for the user. Each day needs an activity hotel and restaurant. Always include a title, a short description, and a day-by-day plan and I have a session with language model session and we pass in the instructions.

Okay, the third and final change we need to do is to update our generate itinerary function. This is the function that we'll invoke from our views in order to send in the prompt and get back a response. Let's go make this code change.

Okay, here's what we added. First, we said let prompt equal to generate a day count day itinerary to landmark.name. Day count here defaults to three and then landmark.name is the name of the landmark that the user clicks on when they open the app. So we gather this name and we pass it to the prompt So we can generate a response for that specific landmark. Next, we have letResponse equal to tryAwaitSession.Response and pass in a prompt. Finally, the response variable has a property.content, which you can recall from our playground canvas that we observed, which had all the natural unstructured text, which is a string, and we assign it to itinerary content.

That includes all the code changes for our view model, which is now ready to be called from our views. Let's head on back to our Code Along Guide to section 1.6. This is our final section in chapter one. We will now update the landmark trip view to take the output from the itinerary generator and display it in the app. Again, feel free to make these code changes by following along these comments. I'm gonna head over to Xcode.

Click on Views.

And landmark trip view. Okay.

All right, the first code change we'll need to make is to add a local variable for the itinerary generator class in our view model.

All right, so we have itinerary generator of the type itinerary and I'm gonna delete this comment.

The next code we need to do, a code change we need to do is to create an instance of this when the view is loaded.

So here's what we introduce under task modify. We said let generator equal to itinerary generator, which is the ViewModel class, and we pass in the landmark, so that it has information about which landmark the user clicked on, which if you recall, we pass it to the prompt. And then we hold on to the itinerary generator here. I'm going to delete the code change that we just made.

The next change we need to do is to update our view itself. Let's take a closer look at the view. By default, we have a Boolean variable here called requested itinerary. It is set to false.

Because it's set to false, we load up the first view on top here, which is a text field that has the landmark name. We access the landmark.name, and then we access the short description using Landmark.shortDescription. This is what shows up when the user has not generated, asked the model to generate an itinerary. But when the requested itinerary is set to false, we need to load up a new view where we can populate it with the model's output, and that is what we are going to implement right now. So I'm gonna remove the else case here and introduce a new else case where I say, if let content equal to itinerary generator dot itinerary content. If you recall, itinerary content is the string variable has our model's output. And then we simply take that content and we update it in our text view. Since we made this change, I'm gonna get rid of this comment too. We're almost there. We have one final change in this view. If you scroll back down here, we've defined a button that will show up at the bottom of the screen and currently this button is hidden. So we'll need to make two minor code changes here. One, we want to show the button so you can either comment this out or straight out delete it like what I'm doing. And then we need to insert code here to generate the itinerary when the user taps on the button. So let's add that here. Okay, so we said await itinerary generator and we invoke the generate itinerary function. If you recall, this is the function that takes the prompt and then passes it to the model and gets the output. That concludes all the code changes in this chapter.

Okay, we are now ready to build and run this app. Click on the run button here, which will build and run the app.

Here is our app. I'm gonna click on Sahara Desert here and we see that we have our generate itinerary button and when I click on this generate itinerary button, the prompt and instructions are being sent to the on device LLM which is going to generate a response asynchronously, token by token, right here on device.

If you see this itinerary like I do, congratulations, you built your very first fully functional on-device generative AI feature using Foundation Models Framework. With just a few lines of Swift, you've tapped into the power of Apple Intelligence.

This is great, but what we have here is a wall of text. What if I wanted to pull out a hotel name and show it on a map? This isn't the rich experience that we want. We'll address this in chapter two with guided generation. We'll discuss how you can get outputs using Swift structs directly from the model. For now, let's quickly recap chapter one.

In this chapter, we learned how to create a session and prompt the model for a basic text response. We saw how to provide instructions to guide the model's output and we covered how to handle different availability states using the availability API. Finally, we integrated these features into our app by updating the view model and view.

That wraps up chapter one.

Now that we can generate raw text, let's see how we can get structured data from the model to build a much richer UI.

Let's start with a fundamental challenge when working with LLMs. By default, they give us unstructured text, like the itinerary we just generated. While a human can read it, for an app developer, this can be challenging to work with. For example, how would you reliably extract the hotel for day one to plot it on a map? You'd have to write complex string parsing code that could break if the model's output changed. What we want instead is structured data that maps directly to our apps logic.

We'd need a more advanced nested structure that can be implemented using Swift structs. This itinerary object should contain an array of objects which in turn should contain an array of activity objects and so on. This is where guided generation comes in. The Foundation Models Framework provides APIs that allow you to specify exactly what your output should look like. If you have a Swift struct, you can simply apply @Generable to it. And this lets the model generate structured data using native Swift types.

We'll start this chapter in the playground where we'll define a simple struct with a generable macro applied to it. We'll then build on it to create more complex nested data structures for the model to generate. Finally, we'll go back to the app, we'll refactor our itinerary generator to output our new structured itinerary type and update our views to display it in a rich UI.

Let's head on back to our Code Along Guide.

We're now in chapter two, generating structured outputs. Our goal is to move beyond simple strings and get structured type-safe Swift data directly from the model. This will allow us to build rich custom UI without any fragile string parsing.

Feel free to copy this piece of code again into Playground and take a look at its output. And I'll be explaining what is going on when we introduce this new struct called simple itinerary. Let's go to the Xcode Playground file and make these code changes.

I'm gonna get rid of the second playground we just added and right here I'm going to introduce this new struct called simple itinerary. Let me walk through what this looks like and how we can incorporate it in our foundation model code to generate this output. First, this struct has a few different properties. It has a title, which is of type String. It has a description, which is of type String. And it has days, which is an array of String.

We want the model to generate these fields and we can provide it additional information by providing guides. The guide has a description argument which says, an exciting name for the trip. This tells the model that it has to generate a title for this variable and similarly we have description, a short and engaging description for the trip and similarly for day count. What we can do now is provide this to the model and we can do that using the generating argument. So previously we had Session.response and just the prompt. So I'm going to add a new argument called generating and provide simple Itinerary.self.

And we can go and refresh our canvas.

This will run through the code and we'll take a look at the output.

Okay. We have our response here. And let's take a closer look at the content property here.

Previously, this content was a string. If you look carefully here, it says this is a struct simple itinerary. Let's open this up. And what you'll notice is the output one to one matches with the struct that we just defined here. So we have our title, which is Parisian Bliss. And that is our title property here. We have our description that is right here. And then we have our array of String. So you'll see days, which is an array of String with day by day activity plan.

Awesome. So let's go back to our Code Along Guide and the section 2.2. An itinerary doesn't just have to be string or array of Strings. It can have nested structs too. Now, let's take a look at a full itinerary struct that we'll be building in our app. So we're going to make a small code change here. All you have to do is replace simple itinerary with itinerary.self, and we are going to make this code chain and I'll explain what this itinerary struct looks like. Back to Xcode.

I'm going to delete this simple itinerary and replace simple itinerary with just itinerary. Okay, so what does itinerary look like? You can command and click on this to open the definition or head on over to the models folder, click on itinerary.swift file, and here you'll see a new struct called itinerary which has similar fields to what we saw in simple itinerary but more. Let's take a closer look. It also has a title which is of type String. It has a description, it has rationale. And if you take a closer look at days, you'll see that it's no longer an array of String. It is actually an array of day plan, which in turn is its own struct. It has its own title, its own subtitle, its own destination and an activity, which is an array of another struct called activity, which has a type, title, description, and type here happens to be an enum, which is also generable. The enum is a great way to have the model generate specific cases that are predefined. For example, here, the type can only be sightseeing, food and dining, shopping, hotel and lodging. If you scroll all the way up to the top, there is another way to constrain what the model can generate. We can use enums or here for destination name. We have a guide that says any of and we provide model data dot landmark. What this tells the model is that it has to generate a destination name that is one of the landmarks that we see when we open up the app. This includes the Serengeti, the Grand Canyon, Sahara Desert and so on. So the output must be one of these. So this is what the itinerary struct looks like. And this is what we actually use in the app. Let's head on back to our Swift playground. And because if you recall, we said the destination name should be one of the names from the list. Paris is not part of the list. So I'm going to change it to something that is actually on the list. How about Grand Canyon? Canvas will detect this code change and let's take a look at the output.

There we have our response. It includes a content and again, if you take a closer look, it is of struct itinerary, not simple itinerary because we updated it. Let's open this up. You'll see it has a title, destination name, description, rationale and days, which is an array of date plan struct. You open it up, you have multiple days, and activities is a type of an activity struct, and so on.

The key thing to note here is that when you apply @Generable, it is completely composable. The framework understands how to build this entire complex object from the top down, all while guaranteeing structural correctness. Now let's integrate this into our app. Let's head on over to our Code Along Guide. We're now in the app section of chapter two. In this section, we'll update our itinerary generator to use the itinerary generable struct that we just tested in the playground. Feel free to copy this code again and I'm going to be making these code changes with you. So let's head on over to itinerary generator under view models folder and bring up my find navigator, set it to chapter 2 so I can take a look at all the code changes I'll be making in this chapter. The first code change we need to make is way up top here where we have to update our itinerary content to not be a string anymore but be a type of type itinerary. So let's first change the name of this variable to just itinerary and update string to itinerary. We can delete this comment because we've made this code change. The next code change we need to do is if you scroll down to the generate itinerary function you see that Xcode is promptly reminding us that itinerary content no longer exists. So we can update this to itinerary because we just added this. And it is complaining because the content currently that is coming out of Session.respond is a string. So similar to last time in the playground, we're going to add the generating argument and provide itinerary.self.

So the model can now output a value that is of the type itinerary and because we made this code change I'm going to get rid of this comment right here. Okay, the final change we'll need to make is to remove additional structural guidance that we are providing in our instructions. Notice how we say each day needs an activity, hotel and restaurant, always include a title, short description, day by day. But all of this information is already in our itinerary generable struct. We don't need to provide it again in our instructions. So the benefit, another benefit of using generables is you can make your prompts much simpler, which can help improve performance as well. So I'm gonna get rid of this comment.

And that concludes all the code changes in this section. So we've updated our itinerary generator view model to be able to generate our generable structure. Let's head on over to section 2.4, updating our view to display the structure data. In this section, we'll update our landmark trip view to generate itinerary view instead of the raw text that we saw in the previous section. This is a very quick code change, so let's head on over to our landmark trip view, which you'll find as the second number to find the views folder.

And the code change we'll need to make is right here.

If you recall previously, we loaded this view when the model output was generated, but we are no longer generating a string. So we can no longer use a text view. So first we have to update this, but then we also need to update this with another view instead of text so that we can actually extract the fields from our itinerary and populate it in a rich UI. So let me replace this with an updated view and I'll talk about what that looks like.

Okay, so here is what I did. And you can also copy and paste this from our guide. Let's take a closer look. So I said itinerary equal to itinerary generator.itinerary. And instead of the text view, we have this itinerary view, which takes in a landmark and takes in the generated itinerary. Now this itinerary view exists in our views folder but we haven't looked at it so let's take a closer look. This should be the file number three of course you can also command and click on this to open it up. Alright we won't be making any code changes to this file in this chapter but you see comments here which means we'll likely be making changes will surely be making changes in a later chapter. What this view does is it can take an itinerary that was generated by the model, extract the fields and create the rich UI we saw in the initial demo. If you take a closer look at our body here we see it can extract the itinerary title, its description, populated and then it's if you scroll down you'll see that when it extracts the day-by-day activity, there is a dedicated view called day view that can show that and we use for each to loop through these and extract all the properties and lay it out. Notice this is so much simpler than being able to parse strings and update it.

All right, so let's head on over to our slides. So the key benefit of guided generation is that it fundamentally guarantees structural correctness. It uses a technique called constraint decoding to do that. What it does is give you control over what the model should generate, whether that be strings or numbers or arrays or even a custom data structure that you define.

This also means that our prompts can be a lot simpler and more focused on the desired behavior instead of prompting the model for specific output formats. This also tends to improve model accuracy allow for optimizations that speed up inference. So to recap, in this chapter we explored how to get structured data from the model. We use the generable macro to define our own Swift types and saw how to create complex data structures by nesting them. We then updated our app to generate and render this structured data in a rich user interface. Let's go build this model to take a look at all the changes we did. Here's our app. Let's click on Sahara Desert and generate itinerary. Similar to before, it's going to take our prompts and instructions and send it to the model and now instead of generating the wall of text, it generates the itinerary type, we extract all the fields, and then populate it in our app using the new view, which is itinerary view. All right, this concludes this chapter. And then, now we are getting that we're getting structured data as model outputs. We can now switch gears and focus on improving the quality and consistency of the output with additional prompting techniques.

While a good prompt tells the model what to do, sometimes it's more effective to just show it. We can include a high quality example as an instance of our generable type directly in a prompt.

This is great because it gives the model a better idea towards the type of responses I'm looking for. So in this chapter, we'll be focusing on improving the quality of our generated content. We'll start again in the playground by using the Prompt Builder API to create more dynamic prompts. Then we'll explore one-shot prompting by providing a high quality example in the prompt to improve the model's accuracy. Finally, we'll integrate what we learned into our apps itinerary generator.

Let's head on over to our Code Along Guide.

We're now in chapter three, prompting techniques. Our goal now is to improve the quality and reliability of a model's output. First, we'll explore how to introduce dynamic prompts using the Prompt Builder API.

Let's head on over to Playground and take a look. Again, feel free to copy this code block into your Playground.swift file.

We are in Xcode, Heading over to Playground.swift file.

Okay. The key code change we're gonna make here is to introduce a prompt using the prompt builder API. Previously, if you recall, under Session.respond, we provided the two argument with generate a three-day itinerary to Grand Canyon in the format of a string. But instead, we can define the prompt not as a string, but using the prompt builder API and passing the values to a closure. The key benefit is that it can now include things like Swift conditionals. So right up top here, we have a variable called which is a Boolean, which is currently set to true. And then within the Prompt Builder API, I use this Boolean to conditionally update my prompt. So if the kit-friendly Boolean is true, then we inject this additional information into the prompt, which is the itinerary must be kit-friendly. We can update our Session.response call to include this new prompt and refresh our canvas.

Let's take a look at our output.

We have our response variable, content.

I'm gonna open up rationale here and take a look. So it says, this itinerary provides a safe, engaging and educational experience for children, ensuring they enjoy the natural beauty of Grand Canyon while being supported by age appropriate activities and accommodation. So you'll see that the model is honoring our request and this came in as a conditional and the benefit of this again is that you can have these prompt speed dynamic. This could be something that the user selects on the app or it could be something that you learn as a developer from the user's preference and update a prompt.

Awesome. Let's go back to our Code Along Guide to section 3.2. Our goal now is to use a more advanced prompting technique called one-shot prompting to show the model exactly what a high quality response looks like. So let's head on over to our Code Along.

So right in my prompt filter API here inside disclosure, I'm going to add another line of code here. Here I say, here is an example of the desired format, but don't copy its content. And I introduce an example. Let's take a closer look. It says Itinerary.exampleTripToJapan. Now what is this? So you can command click on this or head over to models folder, click on itinerary and scroll down and you'll see that example trip to Japan is defined right here. The first thing that you'll notice that this is not a big string that includes an example. This is actually an instance of the itinerary generable with all its properties populated. You'll see that we have a title, a destination name, description, rationale, days, and all the properties manually populated for you. We can head back to our playground and you'll see that we do have an output here and this output will include the additional information that we provided as a one-shot example in order to guide the tone and quality of the response.

The most important part is that we are embedding this itinerary.exampleTripToJapan directly into the prompt. This is our golden example. We're also telling the model explicitly, don't copy its content. We wanted to learn from the style and structure and not just repeat the data. Let's head back now to our guide.

We're now in the app section of chapter 3. We'll now integrate this one-shot prompting approach into the app. The code change we'll need to make is to update the prompt in our itinerary generator in the ViewModels folder and include our example. Let's go make this code change. We are back in our Xcode. I'm going to click on View Models and Itinerary Generator. I'm going to pull up my Find Navigator and click Section 3. You'll see the code change that we do need to make right here.

So within our Generate Itinerary function, we obviously define our prompt here and we're going to replace this prompt.

and I'm going to delete the previous prompt.

Again, just like what we used in the playground, we say let prompt equal to use a prompt builder API, pass this closure. This includes the same string that we previously had, but we also include this additional information whereby introduce Itinerary.exampleTripToJapan, which is of the type Itinerary. So not only does it include all the guidance, but also the schema that's part of this prompt now.

And because we made this change, we can get rid of this comment. And you'll notice that we made all the changes in chapter 3, which means we are ready to build and run this app. and take a look at the build app.

We can choose Serenity here, click on Generate Itinerary. We can ensure that the model will take the prompt, the instructions and the additional example, pass it to the model and generate our final output. There you go.

Okay, our app is working great. Let's close this and head on over to our slides. So in this chapter, we focused on prompting techniques. We learned how to use a prompt builder to construct prompts dynamically and saw how you can use one-shot prompting to improve the quality and consistency of the model's output. We then applied this by updating our app to include a detailed example in our prompt. While @Generable enforces the structure, the one-shot example teaches the model about relationship and the style within the structure.

The model also uses the provided example for the desired tone of voice, ensuring that the generated text aligns with the tone you want to set for the app.

While the difference in output may not always be dramatic, it's an important way to significantly improve the quality of your generated content. And that wraps up our section on prompting techniques.

This is a great place to pause. Let's take a quick 10 minute break. Feel free to use this time to catch up on the code, grab a coffee or stretch your legs. When we get back, we have some really exciting topics ahead. We'll make a UI update in real time with streaming, extend the model's capabilities with tool calling and wrap up with performance optimizations. We'll be back in 10 minutes. See you soon.

Welcome back everyone. I hope you had a great break. Let's keep going. With our high quality prompts in place, let's enhance the user experience by streaming the response in real time. In this chapter, we'll focus on refactoring our itinerary generator to use the streaming API to improve the user experience by streaming the model's response. We'll see how to handle partially generated content as the model is generating the response. We'll then update our view to render the itinerary as it's being generated, providing for a much responsive feel. So let's head on over to our guide. We're now in chapter four, streaming responses. Our goal in this chapter is to dramatically improve the user experience by streaming responses and showing the itinerary as it is being generated. We'll start by updating the itinerary generator file. This section doesn't include a playground component because it's easy to appreciate the streaming responses directly in the app. So let's head on over to our Xcode and open up itinerary generator.

We'll again use our find navigator, update to chapter four and take a look at all the code changes we need to make starting with itinerary. The first change we'll need to make is update our itinerary variable to be of the type Itinerary.partiallyGenerated.

So what is partiallyGenerated? Think of this as a mirror version of our struct where every single property is an optional. @Generable defines this automatically for us. It's a perfect way to represent data that arrives over time. So that is the first code change. I'm going to remove the comment here.

And the next code change we'll need to make is down here. So recall, our generate itinerary function included this async call to Session.respond, we passed our prompt, and then we passed our generable, and then we received our output. What we want instead is the model to generate responses and stream the responses to us. So what we are going to do is replace this code with a new API called Session.streamResponse. Let's take a look.

So we replaced Session.response with Session.streamResponse and kept the rest of the argument same. So you still pass in a prompt, you still provide the generating argument with the itinerary. But we don't have an await here. What we get instead is an async sequence called stream, which means we can then loop over it and assign all the outputs to our itinerary, which includes all these options. So we say try await partial response in stream, and we can extract it using partialresponse.content where you'll get a snapshot every time of whatever has been generated at that point in time. Because we made this code change, I'm going to remove this comment as well.

Okay, that includes all the code changes we need to make to our itinerary generator. So let's head on over to our Code Along Guide and move on to section 4.2. Now we are ready to update our views. Since partially generated fields are optionals, we can use if let statements to safely unwrap these options. And that is what we are going to do in this section. So we'll update our itinerary view, which we previously just got a preview in an earlier chapter, but now we are going to actually go make code changes to this. So let's head on over to Xcode, click on the views folder and click on itinerary view.

Okay, at the very top, you'll notice that we have itinerary, so we should also update this with the partially generated type that we also defined in our view model. And we need to make this code change to all the generables that we have here. So not only itinerary, but all the nested generables too. So if you scroll all the way down, if you recall, we have our day view, which includes a day plan, which should also be partially generated. And each time I make this code change, I'm going to remove these comments and further down, you'll also remember we have our activity array, and we are going to do the same to that.

Okay, so that is the primary code change to the generables. Let's go back all the way up to the top and you'll see Xcode is complaining about a few other things. So the other code change we'll need to make is, if you recall, I said these are optional, so we have to unwrap them. So let's go and do that.

So here is what I did. I said if let title equal to itinerary.title, If let is a great way to deal with these optionals. And because I have a title here, I don't need to extract it from itinerary. So I remove that. So that takes care of title. Now I need to repeat the same step for our description.

I use if let and update the text view to include description.

And then I need to repeat this again for rationale.

And I need to do this again for the other fields, which is days.

Okay, so you get the gist. So we have to keep doing this for all the itinerary fields that properties that we are accessing to safely unwrap them. Now I'm going to do something that I've been asking all of you to do all this while, which is go back to our Code Along Guide here and copy the completely updated file and paste it here because we have to do this for every single property. If you scroll here, you see in step three, we says repeat this for all these properties. So we changed title description rationale, but you have to do this for all the day plan and the activity views too. So instead, what I'm going to do right now in this code along is click on the show updated views, which includes all the code changes. So what I'm going to do is click on this copy button on the top right and go back to our Xcode itineraryview.swift file and just replace all the code with the updated code. And you can see in our find navigator that we don't have any more comments so we've made all the code changes. So I showed you a few different code changes that we need to make but you have to do the same for every single property. So that concludes all the code changes for chapter 4. So to quickly recap, we spoke about the changes we need to make to view model, which is used partially generated, and we updated our views to unwrap these options. So we're now ready to run this app. Click on run, it will build and run this app. And we have our app right here. I'm going to click on Sahara Desert here and click generate itinerary. Unlike previously where it was an async call, now we are able to stream responses as it is being generated. This has great user experience because someone using the app can start consuming this content even before all of the itinerary has been loaded.

Awesome. In this chapter, we made a big leap in user experience. We refactored our app to use a streaming API and learn how to work with partially generated content in our view model. And finally, we updated our view to display the itinerary as it is being generated in real time. That wraps up chapter four on streaming responses. Now our app is looking great but let's make it even smarter by giving the model new capabilities with tool calling. First let me introduce the concept of tool calling. In addition to what you provide to the prompt the model brings its own core knowledge from its training data but remember the model is built into the OS and its knowledge is frozen in time. So For example, if you ask it about weather in Cupertino right now, there's no way for it to know what that information is. To handle cases where you need real time or dynamic data, the framework supports tool calling. Here's how it works. We have a session transcript.

If you provided tools to the session, the session will present the tool definition to the model along with the instructions. In our example, the prompt tells the model which destination we want to visit.

Now, if the model decides that calling a tool can enhance the response, it will produce one or more tool calls. In this example, the model produces two tool calls, querying restaurants and hotels.

At this phase, the Foundation Models Framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the session transcript.

Finally, the model will incorporate the tool output and everything else in the transcript into the final response.

As we've seen so far, the model can be very creative, often giving a slightly different itinerary each time we make a request. While this randomness is great for creativity, it can be a challenge when we need predictable For an advanced feature like tool calling, especially when testing and debugging, we need to ensure that the model behaves consistently. We want to guarantee that it will call our tool when we expect it to. To achieve this, we are going to make another small change to our request using generation options API to use greedy sampling. Greedy sampling tells the model to stop being creative and to always pick the most obvious next token. This makes the models output deterministic. For our app, this ensures that the model will reliably call our tool every single time.

In this chapter, we'll take a look at a tool that can find points of interest. We'll then provide this tool to our language model session and instruct the model how to use it.

Back in the app, we'll integrate this tool into our itinerary generator to get real world data into our itineraries. Let's head on over to our Code Along Guide. We are now in chapter five, tool calling.

Our itinerary contains model generated hotel and restaurant names, and these may not be up to date. Our goal is to give the model a tool it can use to call a Swift code and fetch hotel and restaurant names that we've provided.

Let's go make these code changes to first build a tool and later use this tool in our app. I'll head on over to Xcode and click on our ViewModels folder and you'll see a new file here that says Find Points of Interest Tool.

Click on that. So here we have a class called Find Points of Interest Tool that conforms to the tool protocol, which means we have to define a few properties here that will go through step by step. So let's start making these code changes and I'll explain what is going on. The first change we'll need to make is to add a name and description for our tool. So I'm going to do that here.

So we provide our tool with a name which is find points of interest and a description which is find points of interest for a landmark. This is critical for the model to understand when to invoke this tool. So it will use the name and the description to determine when to invoke this tool. The next change we'll need to make is down here where let me pull up our find navigator so we can see all the code changes that we need to be making. Next code change we need to make here is to define the categories that the tool can search for points of interest for and we'll do that by introducing this generable enum. So The category is an enum that includes hotels and restaurants. This can of course include other cases like museums or campgrounds and others. We're going to use this in our next code chain which is to update our arguments.

Here we have an argument struct here. Let's update this and I'll talk about what this does.

the argument struct, I have a property here that says let point of interest and it is of type category which is something we just defined. So this point of interest could be a hotel or a restaurant and we also provide a guide. The guide has a description that says this is the type of destination to look for. So this argument is the contract between the tool and the model. When the model wants to invoke the tool, it will pass this argument to the tool so that the tool has access to whether it's a hotel or a restaurant that it wants the response from the tool, the category that it wants the response from the tool.

We've updated the argument. And now we're going to update our call function right here.

This function is the heart of our tool. It receives the arguments, performs an action, and returns an output that gets added back into the session's transcript for the model to see and use. So let's make this change.

Okay. And I'll go through step by step what's going on here.

First, I say let results equal to await get suggestions. We have not defined this. We'll define this in a moment. Essentially, think of this as a function that the call method can invoke in order to get these specific points of interest. And then the results will be part of the output here, which you can then, as you see in the return statement, we can insert this result as a string output back to be provided back to the model. The model then uses that information along with the prompts and instructions to generate the final response. So, the last code change we need to make, of course, is to define this function. I have a placeholder function here called getSuggestions. Let's update this.

All right. So within getSuggestions, I have a switch block here which takes in a category and then if it's a restaurant, it can return restaurant1, restaurant2 or restaurant3. Similarly, if it's a hotel, it can return hotel one, hotel two, or hotel three. Now, these are, for this demo, we are using hardcoded data. In a real app, this is where you would call APIs like MapKit or a server-side API to fetch real live data.

Okay, so we made all the code changes to our tool, which means we have fully defined our tool. Let's head on over back to our Code Along Guide and move on to section 5.2.

So what we're going to do now is test this tool. So we'll head on over to our playground and provide this tool to the model and take a look at the results. Again, like before, feel free to copy paste this and I'm going to step through each of these lines of code and explain what exactly is happening. So back in our Xcode, I'm going to switch over to Playground.swift file. And for this section, I'm going to just clean up the previous code and start from scratch.

Okay, so we have our empty playground here.

First, I'm going to add instructions.

Here, a neat feature of playground is that it has access to all the data structures in your Xcode project. without having to build the app. So what I'm doing here is create a landmark variable that has access to the model data defined here under the models folder under model data dot Swift and I say model data dot landmark zero which means I'm going to access one of those landmarks that you see specifically we are going to access the first landmark and if you recall that is Sahara Desert. So you have access to the same list of landmarks that you get when you run the app. So we take that and then we just defined this Find Points of Interest tool right here in the ViewModels folder. So we are going to create an instance of this tool and we can pass it the landmark because it uses that information. And finally, we have our instructions just like before. There are two minor code changes if you look carefully. One, it's no longer a string but instruction builder similar to prompt builder wherein we pass in a closure and provide our instructions. And the second key change you'll notice, very important for tool calling, is we say always use the find points of interest tool to find hotels and restaurants in this landmark. Now this instruction is telling the model that it must invoke this tool in order to get the points of interest response. Now we'll create a language model Session. Similar to previous code change, we said language model session and pasta instruction, but we do introduce a new argument called tools. Here tools can be an array of tools. We have only one tool here which is point of interest tool. Since it's an array, you can provide multiple tools so the model can reason about your prompts and instructions and decide which tool to call when and get back the response. So we've included our tool in our session. Next, we define a prompt. There are no changes to the prompt itself here. And finally, We will invoke the model.

No code changes here too, except we do introduce options that we briefly discussed in the slide. This generation option with sampling set to greedy will ensure that we always get consistent, repeatable and deterministic output given that the rest of the prompts and instructions are consistent. Okay, let's take a look at the canvas here and take a look at the output. Okay, we have our response generated and we have our content here.

So we have our title, description, rationale, days. Let me pick one of these days, day 0, arrival and let me take a look at the activities.

I'll open up activity 0, activity 1, and activity 2.

Now if you look closely, you'll see here under activity 1 description it says, "Enjoy a traditional Moroccan dinner at restaurant 1." You'll also see this in the title, "Dine-in at restaurant 1." And similarly, you see title for activity 2 here that says, "Stay in hotel 1 and unwind at hotel 1." This is the output of the tool that is being inserted into the output of the model. So the model took in a prompt instructions, the landmark name, invoke the tool, got back the hotel and restaurant names and inserted it back to the transcript and generated this response.

Let's take a look at the transcript itself.

So what I'm doing here is just creating a temporary variable for the session itself and capturing it into inspect session. The reason I'm doing this is to take a closer look at the session and transcript and we can see the tool calls being placed. Okay. So we have our inspect session, which we just created.

Now we are going to take a look at these properties. you see tools. It has one tool that we provided. And if you look at transcript, it has six elements in this entries. And here we have our instructions, which is always the very first entry in the transcript. And then we have a prompt, which is our initial request. And then we have tool calls. The model autonomously decided that it needs to call our tool. Then we have our tool outputs. The framework executed our tool and inserted these tool outputs back into the transcript. And then finally we have our response. The model synthesized the original prompt, the tool output data to generate this final response. There are two tool calls here because we are requesting for both restaurants as well as hotels. And you'll see this under the tool calls. So there's a request for a restaurant and a hotel.

Awesome. Let's head back to our Code Along Guide.

So now that we know how a tool works, we defined the tool, we tested the tool in our playground, We're now ready to update our itineraryGenerator.swift file to incorporate our tool into the app. That's what we'll do in section 5.3. We'll make our code changes to itineraryGenerator.swift. Feel free to copy and paste this into your files. The key changes as you see here we'll be making is to update our instructions, create an instance of tool, and also pass it to our language model session. Let's head on over to Xcode and open up our itineraryGenerator.swift. I'll also bring up my find navigator, set this to chapter 5 and we'll start making code changes.

So the first change we need to do is of course update our instructions.

I'm going to delete the previous instructions because I have this new instructions which includes point of interest tool that we defined and this additional text that is asking the model to call this tool in order to get the points of interest. And we also of course need to update the language model session using the tools argument. And since it can accept multiple tools it is an array and and we'll pass in the tool.

Okay, so that is the two code changes that we need to make in our initializer. And we did that, so I'm gonna get rid of these comments so we can track our changes. Okay, the final change we need to make is in the generate itinerary method here.

Recall, we mentioned that if we want get deterministic outputs, we can use greedy sampling. By default, it does random sampling. So right here, after in this session.stream response, after we pass the prompt, after we pass the generating argument, we can pass our options. Let me clean this up so it's easy for everyone to read.

All right, so we have our Session.streamResponse, we have a prompt, we have our generating argument, and finally we have our options which includes generation options and we use sampling and set it to greedy. Okay, so that concludes all the code changes that we need to make. Let's ensure we get rid of this comment.

There you go. If you don't see anything for chapter 5 in your Find Navigator, that means we've made all the code changes, we are ready to build and run this app.

Click on the run button, this will build and run this app.

And here is our app. Let's go through the standard user flow, which is click on Sahara Desert. I see a generate itinerary button. I click on that. Now this includes a streaming API along with our tools and it takes our instructions, our prompts, sends it to the model along with the tool definition. And as you see here, you can see stay at hotel one and dine in at restaurant one. These were responses from the tool that were inserted back into the session transcript and the model used all the information from the instructions, the prompts, the tool calls, The tool responses, package all of it, synthesized it, and is able to generate the output in the format of the generable, itinerary generable.

Fantastic. All right, let's go back to our slides and recap. So in this chapter, we gave the model powers with tool calling. We discussed a custom tool with its own arguments and call function. We learned how to provide the tool to the language model session, and importantly, how to instruct the model on when and how to use the tool. Finally, we integrated our tool into the app to fetch points of interest and include them in the generated itinerary. That wraps up chapter five on tool calling.

Before we wrap up this code along, let's look at a couple of key techniques to optimize performance and make our generative features feel more responsive.

Let's head on over to a Code Along Guide and move on to chapter six, Performance and Optimization.

Our app is now feature complete, but to make our app performant, we first need to understand where the bottlenecks are. We can't optimize what we can't measure. For this, we will use a powerful developer tool called Instruments.

Let's head on over to Xcode.

We'll do something slightly different now. If you long press on the run button here, you'll see a few different options. You see run, test, profile and analyze. So I'm going to click profile. What this does is it'll build the app and then launch up Xcode instruments.

Let's wait for it to finish building and there it is. So this is Xcode Instruments. We'll choose the blank template and then once you have your instruments open, I'm going click on this plus symbol here and search for foundation models. Okay, we are now ready to profile our app. I'm gonna click on the record and this will launch our app and we will use this app like we usually do. So as a user, Sahara Desert looks interesting. I read the title description, looks good. I click on generate itinerary and I see this nice itinerary come up. The results are being streamed to me. I can read through this, take a look at all the different activities that plan. Okay, I'm going to stop recording. Now let's take a closer look at what we have here in the instruments. Okay, there are a few different tracks here and I'll explain what is going on in each of these to identify any potential bottlenecks that we can address. First track here is response. The blue bar here represents entire session. So this is ever since the user clicks on generate itinerary, we create a session and the model takes in the instructions, prompts and generates output. All of this is represented by this blue bar. The second row here is asset loading. Here if you take a closer look, you'll see that once the session starts, there is a little bit of a delay and then the models are loaded here, the model assets, which means all this time from the start of the session all the way to end of loading the model, the model is not generating any responses and roughly looks like this is about 700 milliseconds, which is almost a full second, right? And then if you look at the third track, this is where you see that the first token is generated, which means it waits for all the the models to be loaded and then it starts the token generation process, starting with the first token and continues to generate all the responses. So there is an opportunity for performance improvement here. If we could load these assets ahead of time, maybe we could start this generation process as soon as the session starts.

So that is one bottleneck we can try and address. The second bottleneck, if you look at the bottom here, I'm going to choose the inference section here. If you take a closer look, you will see here that there is max token count. And we see here that this currently amounts to 1044. And this token count includes everything we've added into the session. This includes your instructions, your prompts, your tools. It includes the generables with the itinerary, all of it. So it includes all of this here and we can see if there's an opportunity to reduce this because the number of tokens has an implication on the model's performance. So that is a second bottleneck that we can see if we can try and address.

Okay. If you recall, when we call Session.respond, the OS will load the model if it's not already in memory.

pre-warming can give your session a head start by loading the model before you even make a request. In our app, when someone taps on the landmark, it's pretty likely that they are going to make a request soon. We can pre-warm before they press the generate itinerary button to proactively load the model. By the time they finish reading the description, our model will be ready to go.

Let's also look at another optimization that can reduce request latency. Recall that generable structs provided to the model can help generate structured outputs, but this comes at the cost of increased token count, which affects initial processing time. Also recall that in Chapter 3, we passed an example itinerary called example trip to Japan. Since our instructions includes this full example of the generable schema, we can often exclude the schema definition itself from the front, which saves space and can speed up the model.

Thanks to Xcode instruments, we've identified the bottlenecks in our app. Now, we'll implement some optimizations directly in the app. First, we'll pre-warm the session by calling the pre-warm method when the user taps on the landmark. This does the framework to start loading the model before the user even asks for the itinerary. Second, because our one-shot example is quite detailed, the full schema definition in the prompt is redundant. We can remove it by setting include schema and prompt to false. In our stream response call, we'll make this change. This will significantly reduce our input token count. Let's head on over to our Code Along Guide and take a look at the code changes we'll be making. We're now in chapter 6, the app section. The first part is to pre-warm the model and the code changes will be reflected in the itinerary generator where we'll add a function to pre-warm and then in the view as well so that we can call the pre-warm method when the view is loaded. So let's go make these changes in itinerary generator and landmark trip view. Let's head on over to Xcode. I'll keep the instrument open because I do want to check the effect of these optimizations. So I'm going to go to Xcode, click on itinerary generator. This is already open for us. And I'm going to use the find navigator to open chapter 6.

Okay.

The first change we'll make to PREWARM is to add the PREWARM code here. We've defined this placeholder function called PREWARM model. So all I'm going to do here is call the pre-womp method in the session.

It's as easy as that.

Now we have a function that we can invoke from our view that will pre-womp the model. If you ahead of time know what the prompt is going to be, you can also use a prompt prefix to update a pre-womp method.

So inside a Session.prewarm function, there is an optional argument called prompt prefix where you can provide a prompt so the model has knowledge of the prompt that the user might provide and it can prewarm using this. So here we pass a prompt with a closure that says generate a three-day itinerary to landmark.name. This can further improve performance.

Okay, the next code change we need to make is actually in landmark trip view. So in our views folder, we have our landmark trip view.

Here, we need to update our task in order to call the pre-warm method when the model is actually loaded. So let's do this here.

Again, this is as simple as calling the generator.prewarmModel function that we just defined. So that includes all the code changes for prewarming the model. Let's head back to our Code Along Guide and take a look at the second optimization that we discussed, which is to reduce the max token count. So we're now in section 6.2, where we'll optimize the prompt.

The code change we'll need to make here is again in itinerary generator. So we'll include this additional argument called include schema in prompt and set it to false. Let's make this change and I'll briefly again explain what is going on.

So back to our itinerary generator.

Here we have our Session.stream response where we pass in the prompt, the generable, and options. So we'll also include our new argument called include schema in prompt and set it to false. Now what this tells the model is that we can exclude the schema of the itinerary that we pass because we are already passing the example trip to Japan in instruments, which includes the golden example along with the structure. So we can skip including the schema which will help in reducing the maximum token count. Because we made this change, I'm going to get rid of this comment too.

Okay, that concludes all the changes in chapter 6, which means we are now ready to profile the app once again. Okay, let's do this again. So I'm going to click on the profile option again and again this will build the app and launch up our profiler in a moment.

You see Xcode is building, launched up our profiler again. Now when I record it will relaunch the app and we'll go through the same process again of using the app. Click record. I have my app here. I'm going to follow the same exact steps. I'm going to click on Sahara Desert. I'll read the title. The description looks good to me. I want to generate the itinerary and I see the itinerary being generated. Looks good. We have our day-by-day plan, what restaurants to eat in, what hotels to stay. We'll let it finish executing and I'm going to stop profiling. Let's do the same thing we did previously and take a look at the output and see what effect our optimizations have had on the app. The very first thing you should notice is that asset loading happened well before the session started thanks to our pre-warm function. So we loaded this asset at this point when the user clicked on the detail view we called the pre-warm method by adding the pre-warm function in the task which means by the time the user used to read the title and description, the model was already loaded and ready. And if you take a closer look at the start of the session here, you'll see that the output starts generating almost as soon as the session started. So the session started because the model has already been loaded. It starts to prepare the vocabulary, it starts generating the tokens and your responses are now much quicker. Let's also take a look at the second optimization that we did and what effect it has had. So down here under inference you'll see the maximum token count has dropped to 700. Previously it was 1000 so we have dropped the maximum token count to 700 by excluding the schema from the prompt. Now this also means that the model is able to much quickly process the initial token and start generating responses a lot quicker. Awesome. So in this last chapter we looked at performance. We learned how to pre-warm the model to make our app feel more responsive and how to optimize a prompt by excluding the schema when it's not needed. These are two simple but effective ways to improve the performance of your generative features. Now let's take one final look at that app we've built together. Let's go back to Xcode and build and run. Okay, so this should look familiar to you as it's the app running on your machine. We started with this simple Swift list of landmarks and when we select Serengeti here we see this detail view. Now let's tap on generate itinerary one last time.

The UI builds itself in real time. That's the streaming API from chapter four using Session.stream response and partially generated content. In chapter two, we used add generable to get this rich structured response. And in chapter five, we use tool calls to find these points of interest, which the model intelligently decided to call to get this data.

Okay, we've covered a lot today from basic text generation to guided generation, streaming, tool calling, and performance optimizations, but there's still more to explore. We didn't have time to cover some advanced topics such as training custom model adapters, dynamic runtime schemas, or diving into guardrails and error handling. To learn more about these topics, I highly recommend watching other WWDC25 videos on the Foundation Models Framework.

Looking at Slido, there are a lot of great questions here and if we didn't get to yours, please bring them to the developer forums at developer.apple.com/forums where we can continue this discussion. The completed sample project from today, including few additional features is available for download in the Foundation Models Framework documentation. Finally, you'll receive a survey later today. We hope you enjoyed the session and we'd appreciate your feedback. With that, thank you so much for coding along with me. We'll see you again soon. Bye.
--- END FILE ---

--- FILE: LanguageModelSession.md ---
# LanguageModelSession

**An object that represents a session that interacts with a language model.**

## Availability

- **iOS** 26.0+
- **iPadOS** 26.0+
- **Mac Catalyst** 26.0+
- **macOS** 26.0+
- **visionOS** 26.0+


## Overview

A session is a single context that you use to generate content with, and maintains state between requests. You can reuse the existing instance or create a new one each time you call the model. When you create a session you can provide instructions that tells the model what its role is and provides guidance on how to respond.

```swift
let session = LanguageModelSession(instructions: """
    You are a motivational workout coach that provides quotes to inspire \
    and motivate athletes.
    """
)
let prompt = "Generate a motivational quote for my next workout."
let response = try await session.respond(to: prompt)
```

The framework records each call to the model in a [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript](https://developer.apple.com/documentation/FoundationModels/Transcript) that includes all prompts and responses. If your session exceeds the available context size, it throws [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)).

## Topics

### Creating a session

- [init(model:tools:instructions:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/init(model:tools:instructions:)) ‚Äî Start a new session in blank slate state with instructions builder.
- [SystemLanguageModel](https://developer.apple.com/documentation/foundationmodels/systemlanguagemodel) ‚Äî An on-device large language model capable of text generation tasks.
- [Tool](https://developer.apple.com/documentation/foundationmodels/tool) ‚Äî A tool that a model can call to gather information at runtime or perform side effects.
- [Instructions](https://developer.apple.com/documentation/foundationmodels/instructions) ‚Äî Details you provide that define the model‚Äôs intended behavior on prompts.
### Creating a session from a transcript

- [init(model:tools:transcript:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/init(model:tools:transcript:)) ‚Äî Start a session by rehydrating from a transcript.
- [Transcript](https://developer.apple.com/documentation/foundationmodels/transcript) ‚Äî A transcript contains a linear history of [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript/Entry](https://developer.apple.com/documentation/FoundationModels/Transcript/Entry) entries.
### Preloading the model

- [prewarm(promptPrefix:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/prewarm(promptprefix:)) ‚Äî Requests that the system eagerly load the resources required for this session into memory and optionally caches a prefix of your prompt.
### Inspecting session properties

- [isResponding](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/isresponding) ‚Äî A Boolean value that indicates a response is being generated.
- [transcript](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/transcript) ‚Äî A full history of interactions, including user inputs and model responses.
### Generating a request

- [respond(options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(options:prompt:)) ‚Äî Produces a response to a prompt.
- [respond(generating:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(generating:includeschemainprompt:options:prompt:)) ‚Äî Produces a generable object as a response to a prompt.
- [respond(schema:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(schema:includeschemainprompt:options:prompt:)) ‚Äî Produces a generated content type as a response to a prompt and schema.
- [respond(to:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(to:options:)) ‚Äî Produces a response to a prompt.
- [respond(to:generating:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(to:generating:includeschemainprompt:options:)) ‚Äî Produces a generable object as a response to a prompt.
- [respond(to:schema:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(to:schema:includeschemainprompt:options:)) ‚Äî Produces a generated content type as a response to a prompt and schema.
- [Prompt](https://developer.apple.com/documentation/foundationmodels/prompt) ‚Äî A prompt from a person to the model.
- [LanguageModelSession.Response](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/response) ‚Äî A structure that stores the output of a response call.
- [GenerationOptions](https://developer.apple.com/documentation/foundationmodels/generationoptions) ‚Äî Options that control how the model generates its response to a prompt.
### Streaming a response

- [streamResponse(to:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(to:options:)) ‚Äî Produces a response stream to a prompt.
- [streamResponse(to:generating:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(to:generating:includeschemainprompt:options:)) ‚Äî Produces a response stream to a prompt and schema.
- [streamResponse(to:schema:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(to:schema:includeschemainprompt:options:)) ‚Äî Produces a response stream to a prompt and schema.
- [streamResponse(options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(options:prompt:)) ‚Äî Produces a response stream to a prompt.
- [streamResponse(generating:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(generating:includeschemainprompt:options:prompt:)) ‚Äî Produces a response stream for a type.
- [streamResponse(schema:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(schema:includeschemainprompt:options:prompt:)) ‚Äî Produces a response stream to a prompt and schema.
- [LanguageModelSession.ResponseStream](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/responsestream) ‚Äî An async sequence of snapshots of partially generated content.
- [GeneratedContent](https://developer.apple.com/documentation/foundationmodels/generatedcontent) ‚Äî A type that represents structured, generated content.
- [ConvertibleFromGeneratedContent](https://developer.apple.com/documentation/foundationmodels/convertiblefromgeneratedcontent) ‚Äî A type that can be initialized from generated content.
- [ConvertibleToGeneratedContent](https://developer.apple.com/documentation/foundationmodels/convertibletogeneratedcontent) ‚Äî A type that can be converted to generated content.
### Generating feedback

- [logFeedbackAttachment(sentiment:issues:desiredOutput:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/logfeedbackattachment(sentiment:issues:desiredoutput:)) ‚Äî Logs and serializes data that includes session information that you attach when reporting feedback to Apple.
### Getting the error types

- [LanguageModelSession.GenerationError](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/generationerror) ‚Äî An error that may occur while generating a response.
- [LanguageModelSession.ToolCallError](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/toolcallerror) ‚Äî An error that occurs while a system language model is calling a tool.
### Instance Methods

- [logFeedbackAttachment(sentiment:issues:desiredResponseContent:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/logfeedbackattachment(sentiment:issues:desiredresponsecontent:))
- [logFeedbackAttachment(sentiment:issues:desiredResponseText:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/logfeedbackattachment(sentiment:issues:desiredresponsetext:))

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession)*
--- END FILE ---

--- FILE: Tokenizing-natural-language-text.md ---
# Tokenizing natural language text

**Enumerate the words in a string.**


## Overview

When you work with natural language text, it‚Äôs often useful to tokenize the text into individual words. Using [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer) to enumerate words, rather than simply splitting components by whitespace, ensures correct behavior in multiple scripts and languages. For example, neither Chinese nor Japanese uses spaces to delimit words.

The example and accompanying steps below show how you use [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer) to enumerate over the words in natural language text.

```swift
let text = """
All human beings are born free and equal in dignity and rights.
They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.
"""

let tokenizer = NLTokenizer(unit: .word)
tokenizer.string = text

tokenizer.enumerateTokens(in: text.startIndex..<text.endIndex) { tokenRange, _ in
    print(text[tokenRange])
    return true
}
```

1. Create an instance of [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer), specifying [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenUnit/word](https://developer.apple.com/documentation/NaturalLanguage/NLTokenUnit/word) as the unit to tokenize.

2. Set the [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer/string](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer/string) property of the tokenizer to the natural language text.

3. Enumerate over the entire range of the string by calling the [doc://com.apple.naturallanguage/documentation/NaturalLanguage/NLTokenizer/enumerateTokensInRange:usingBlock:](https://developer.apple.com/documentation/NaturalLanguage/NLTokenizer/enumerateTokensInRange:usingBlock:) method, specifying the entire range of the string to process.

4. In the enumeration block, take a substring of the original text at `tokenRange` to obtain each word.

5. Run this code to print out each word in text on a new line.

---

*Source: [https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage/tokenizing-natural-language-text](https://developer.apple.com/documentation/com.apple.naturallanguage/documentation/NaturalLanguage/tokenizing-natural-language-text)*
--- END FILE ---

--- FILE: Discover_machine_learning_and_AI_frameworks_on_Apple_plaforms.md ---
Hi there, I‚Äôm Jaimin Upadhyay, an engineering manager on the On-Device Machine Learning team at Apple. Today, I would like to talk about how you can make use of Apple Intelligence and machine learning in your apps and personal projects. Whether you are an app developer ready to tap into Apple Intelligence through UI components or directly in code, an ML engineer converting and optimizing models for on-device deployment, or an AI enthusiast exploring the frontier of what is possible on your Mac, we have the tools for you. I‚Äôll walk you through a high level overview of these tools, highlight the latest additions, and point you to resources to learn more along the way. We will start with an overview of the intelligence built into the operating system and its relationship with your app. Next, we will explore how you can programmatically tap into this intelligence through our system frameworks. We will then talk about how Apple‚Äôs tools and APIs can help you optimize and deploy any machine learning model for on-device execution. And we will finish up by discussing how you can stay on top of the latest innovations in ML and AI on Apple hardware.

We‚Äôve got a long and exciting tour to cover, so let‚Äôs get started. We start with platform intelligence. Machine Learning and Artificial Intelligence are at the core of a lot of built-in apps and features in our operating system. Whether it‚Äôs Optic ID to authenticate you on Apple Vision Pro, or understanding your handwriting to help you with math on iPad, or removing background noise to improve your voice quality on FaceTime, machine learning is at the core. ML Models powering these features have been trained and optimized for efficiency on device and last year marked the start of a new chapter, to bring generative intelligence into the core of our operating systems, with large foundation models that power Apple Intelligence. This brought Writing Tools, Genmoji, and Image Playground across the system, making it easy to integrate them into your apps. If you‚Äôre using system text controls, you‚Äôll get Genmoji support automatically. You can even use the APIs to make them appear right in your text.

The Image Playground framework provides Swift UI extensions to bring up the imagePlaygroundSheet in your app. And, for most of you, using the standard UI frameworks to display textViews, your apps were already set up to support Writing Tools. It's that simple. You can either use standard views or add a few lines of code to your custom ones. This way, your users can easily access Apple Intelligence within your apps with a consistent and familiar UI. But, what if you want to go beyond the default UI or need more control? This brings us to the topic of ML-powered APIs that give you programmatic access to system models and capabilities.

We offer a wide variety of such APIs. While some provide access to prominent system models with essential utilities, others expose convenient APIs for specialized ML tasks. Let's dive into these by revisiting how you can integrate image generation into your app. iOS 18.4 introduced ImageCreator class to ImagePlayground framework.

This lets you create images programmatically. Just instantiate the ImageCreator. Request for images based on some ideas. Here, we use a text prompt and a selected style. Then, you can show or use them in your app as you prefer. Also in 18.4, we introduced the Smart Reply API. You can let your users choose generated smart replies for their messages and e-mails, by donating the context to a keyboard. Let‚Äôs take a quick look at how you can set it up. To donate your conversation, configure a UIMessage or UIMail ConversationContext with your data, then set it on your entry view before the keyboard is requested.

When a user selects a smart reply from the keyboard for an instant message, it will be directly inserted into the document. However, in a mail conversation, the selection will instead be delegated back to your view‚Äôs corresponding insertInputSuggestion delegate method. You can then generate and insert your own longer replies appropriate for an e-mail. To learn more, check out ‚ÄúAdopting Smart Reply in your messaging or email app‚Äù documentation page. Note that this is all running on device and using Apple‚Äôs foundation models. In iOS 26, we are going even further with the introduction of: the Foundation Models framework. It provide programmatic access to a highly optimized on-device language model that‚Äôs specialized for everyday tasks. Now it can power these features across all your apps. It‚Äôs great for things like summarization, extraction, classification, and more.

You can use it to enhance existing features in your apps, like providing personalized search suggestions. Or you can create entirely new features, like generating itinerary in a travel app.

You can even use it to create dialogue on-the-fly for characters in a game. That one is my personal favorite! Prompting the model is as easy as three lines of code. Import the framework, create a session, and send your prompt to the model. Since the framework is on device, your user's data stays private and doesn't need to be sent anywhere. The AI features are readily available and work offline, eliminating the need to set up an account or obtain API keys. And all of this, at no cost to you or your users for any requests.

The Foundation Models framework provides much more than simple prompting for text responses. Sometimes you need an LLM to generate structured responses that you can use directly in your app.

This is easy with the Foundation Models framework. You can take existing types in your app and mark them as generable. Also add some natural language guides to each property, along with optional controls over their generated values. This lets you use Guided Generation with a simple prompt. When you indicate the response to generate your type.

The framework will customize the language model decoding loop and stop the model from making structural mistakes. Your data structure is filled with the correct information, so you don‚Äôt have to deal with JSON schemas. Just focus on the prompt and let the framework do the rest! The synergy between Swift, the framework and your custom types makes it easy for you to rapidly iterate and explore new ideas within your app.

When developing your use case, it's important to consider the knowledge available to the foundation model. In addition to information provided via your prompt and generable type descriptions, The model has a core set of knowledge derived from the data it was trained on. This data was fixed in time and does not contain recent events. While the model is incredibly powerful for a device-scale model, it‚Äôs not as knowledgeable as larger server-scale models. To help with use cases requiring additional knowledge from your app or over the network, the foundation model‚Äôs framework supports tool calling. Tool calling lets you go beyond text generation and perform some actions. It provides the model access to live or personal data, like weather and calendar events, not just what was trained months ago. It can even let the model cite sources of truth, which allows the users to fact-check its output. Finally, tools can take real actions, whether it‚Äôs in your app, on the system, or in real world.

This was just a sneak peek of the framework's awesome capabilities, but there is so much more. For a more detailed introduction watch ‚ÄúMeet the Foundation Models framework‚Äù session. There you will also learn about streaming responses, stateful sessions, and the framework's tight integration with Xcode. And if you prefer learning by doing, we have a code along session for building your first intelligent app using the new APIs.

We also have a session dedicated to design considerations for your use cases. It focuses on best practices to help you write reflective prompts, AI safety considerations, understanding what is possible with a device-scale language model, and some solid strategies for evaluating and testing quality and safety. Be sure to check out ‚ÄúExplore prompt design and safety for on-device Foundation models‚Äù to learn more.

The new Foundation Models framework joins the suite of other Machine Learning powered APIs and tools you can use to tap into on-device intelligence for your app‚Äôs features. These frameworks each focus on a specific domain with highly optimized task-specific models.

There is Vision to understand the content of images and videos.

Natural language to identify language, parts of speech, and named entities in natural language text.

Translation to perform text translations between multiple languages.

Sound analysis to recognize many categories of sound. And speech to identify and transcribe spoken words in audio. All with just a few lines of code.

Let me highlight some interesting new additions to these frameworks this year.

Let's start with Vision.

Vision has over 30 APIs for different types of image analysis. And today, Vision is adding two new APIs.

Vision is bringing improvements to text recognition. Instead of just reading lines of text, Vision now provides document recognition.

It can group different document structures, making it easier to process and understand documents.

Vision also has a new lens smudge detection mode. It helps you identify smudges on camera lens that can potentially ruin images. For more details on Lens Smudge Detection and the other cool new additions to Vision, check out the session ‚ÄúReading documents using the Vision Framework‚Äù for more details.

Next, let‚Äôs talk about the Speech framework. The SFSpeechRecognizer class in Speech framework gave you access to the speech-to-text model powering Siri and worked well for short-form dictation.

Now in iOS 26, we‚Äôre introducing a new API, SpeechAnalyzer, that supports many more use cases and leverages the power of Swift. The new API lets you perform speech-to-text processing with very little code entirely on device.

Along with the API, we are providing a new speech-to-text model that is both faster and more flexible than the previous one.

You pass audio buffers to the analyzer instance, which then routes them through the new speech-to-text model. The model predicts the text that matches the spoken audio and returns it to your app.

The new model is especially good for long-form and distant audio, such as lectures, meetings, and conversations.

Watch the ‚ÄúBring advanced speech-to-text to your app with SpeechAnalyzer‚Äù session to dive deeper.

Apple‚Äôs ML powered APIs offer tons of capabilities that your app can readily take advantage of! And many of these APIs can be extended or customized to your specific use case.

The Create ML app and framework give you the ability to fine-tune the system models with your own data.

Create your own image classifier to use with the Vision framework, or a custom word tagger to use with Natural Language. You can even extend the capabilities of Vision Pro to recognize and track specific objects with 6 degrees of freedom for spatial experiences.

So far we have talked about how you can leverage or extend the ML and AI powered capabilities built into the system. Next, let‚Äôs talk about bringing any model to device.

When choosing and integrating a model into your app, there is a lot to consider. But it is made easy with Core ML. All you need is a model in the Core ML format.

These model assets contain a description of the model‚Äôs inputs, outputs, and architecture along with its learned parameters.

You can find a wide variety of open models in the Core ML format on developer.apple.com ready for use.

They are organized by category with a description of each model‚Äôs capabilities and a list of different variants along with some high-level performance information on different devices.

Similarly, you may want to check out the Apple space on Hugging Face. In addition to models already in Core ML format, you will also find links to the source model definition.

These model definitions are often expressed in PyTorch along with training and fine-tuning pipelines.

Core ML Tools provides utilities and workflows for transforming trained models to Core ML model format.

These workflows not only directly translate the model‚Äôs representation but also apply optimizations for on-device execution.

Some of these optimizations are automatic, such as fusing operations and eliminating redundant computation.

However, coremltools also provides a suite of fine-tuning and post-training based model compression techniques.

These will help you reduce the size of your model and improve its inference performance in terms of memory, power and latency.

These techniques are opt-in and allow you to explore different trade-offs between performance and model accuracy.

Check out the ‚ÄúBring your models to Apple Silicon‚Äù session from WWDC24 to learn more. Also, make sure to check out the latest release notes and examples in the user guide.

Once you have your model in the Core ML format, you can easily integrate it with Xcode. You can inspect your model‚Äôs key characteristics or explore its performance on any connected device.

You can get insights about the expected prediction latency, load times, and also, introspect where a particular operation is supported and executed, right in Xcode.

New this year, you can visualize the structure of the full model architecture and dive into details of any op.

This brand new view helps you build a deeper understanding of the model you are working with, making debugging and performance opportunities incredibly visible.

When it's time to get coding, Xcode generates a type safe interface in Swift specific to your model. And integration is just a few lines of code.

At runtime, Core ML makes use of all available compute, optimizing execution across the CPU, GPU, and Neural Engine.

While Core ML is the go-to framework for deploying models on-device, there may be scenarios where you need finer-grained control.

For instance, if you need to sequence or integrate ML with graphics workload, you can use Core ML models with both MPS Graph and Metal.

Alternatively, when running real-time signal processing on the CPU, Accelerate‚Äôs BNNS Graph API provides strict latency and memory management control for your ML task.

These frameworks form part of Core ML‚Äôs foundation and are also directly accessible to you.

This year, there are some new capabilities in BNNSGraph, including a new Graph Builder that lets developers create graphs of operations. This means you can write pre- and post-processing routines or even small machine-learning models to run in real time on CPU. Check out ‚ÄúWhat‚Äôs new in BNNS Graph‚Äù for all the details.

Finally, let‚Äôs talk about how you can keep up with the fast-paced development happening in machine learning and how can the Apple platform assist you. ML research is moving at a rapid pace, there‚Äôs advancements made every single day. New models and techniques are being explored and built at an unprecedented rate. There is a lot to try and keep up with. It can be challenging without the right tools and resources.

To keep up with the current frontier of exploration, one needs the ability to run large models, tinker with unique architectures, and learn from an open community. We have sophisticated tools and resources to help on your endeavors exploring the frontier. One such powerful tool is MLX.

It‚Äôs array framework for numerical computing and machine learning. It‚Äôs designed by Apple‚Äôs machine learning researchers and developed fully open source.

MLX provides access to state-of-the-art models and the ability to perform efficient fine-tuning, training, and distributed learning on Apple Silicon machines.

MLX can run state-of-the-art ML inference on large language models like Mistral with a single command line call.

For example, here it‚Äôs generating code for quick sort with a maximum token length of 1024.

This allows you to stay in-step with state-of-the-art research, thanks to the open source community working to make these models work with MLX.

The MLX community on Hugging Face has hundreds of frontier models readily available to you through one line of code. Check out ‚ÄúExplore large language models on Apple silicon with MLX‚Äù session to learn about how you can run DeepSeek-R1 on your Apple Silicon machine.

MLX is designed to take advantage of the best of Apple Silicon. This includes a new programming model specific to unified memory.

Most systems commonly used for machine learning have a discrete GPU with separate memory.

Data is often resident and tied to a specific device.

Operations run where the data is.

You cannot efficiently run operations that use data from multiple pools of memory. They would require a copy in memory.

Apple Silicon, on the other hand, has a unified memory architecture.

This means that the CPU and the GPU share the same physical memory.

Arrays in MLX aren‚Äôt tied to a device, but operations are, allowing you to even run different operations on CPU and GPU in parallel on the same buffer.

Check out ‚ÄúGet started with MLX for Apple silicon‚Äù session to learn about this unique programming model and other features of MLX. You can even fine-tune your model with a single line of code and scale up as needed for distributed training easily.

It‚Äôs available in Python, Swift, C++ or C, and other languages of your choice through the multiple bindings created by the open source community.

In addition to MLX, if you are using one of the popular training frameworks like PyTorch and Jax, we‚Äôve got you covered with Metal, so you can explore the frontier without deviating from the standard tools that have been embraced by the ML community over the years.

Lastly, developer.apple.com is a great resource for AI enthusiasts and researchers to get a peek at the latest machine learning resources from Apple.

With that, we've covered our agenda. Let‚Äôs step back a little and take a look at everything we talked about today.

Based on your needs and experience with models, you can choose the frameworks and tools that best support your project‚Äôs Machine Learning and AI capabilities.

Whether you want to fine-tune an LLM on your Mac, optimize a computer vision model to deploy on Apple Vision Pro, or use one of our ML-powered APIs to quickly add magical features to your apps, we have you covered. And all of this is optimized for Apple Silicon, providing efficient and powerful execution for your machine learning and AI workloads.

We are sure you will find the resources we went over here helpful and can‚Äôt wait to see the new experiences you create by tapping into Apple Intelligence. There has never been a better time to experiment and explore what you can do with machine learning and AI on Apple platforms.

Here we covered just the surface.

I highly encourage you to check out the machine learning and AI category in the Developer app and on our developer forums to learn more.

Ask questions and have discussions with the broader developer community there.

I hope this has been as fun for you as it has been for me. Thanks for watching!
--- END FILE ---

--- FILE: Getting-a-Core-ML-Model.md ---
# Getting a Core ML Model

**Obtain a Core ML model to use in your app.**


## Overview

Core ML supports a variety of machine learning models, including neural networks, tree ensembles, support vector machines, and generalized linear models. Core ML requires the Core ML model format (models with a `.mlmodel` file extension).

Using [doc://com.apple.documentation/documentation/CreateML](https://developer.apple.com/documentation/CreateML) and your own data, you can train custom models to perform tasks like recognizing images, extracting meaning from text, or finding relationships between numerical values. Models trained using Create ML are in the Core ML model format and are ready to use in your app.

Apple also provides several popular, open source [https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/) that are already in the Core ML model format. You can download these models and start using them in your app.

Additionally, various research groups and universities publish their models and training data, which may not be in the Core ML model format. Use [https://coremltools.readme.io/](https://coremltools.readme.io/) to convert these models to use in your app.

---

*Source: [https://developer.apple.com/documentation/com.apple.coreml/documentation/CoreML/getting-a-core-ml-model](https://developer.apple.com/documentation/com.apple.coreml/documentation/CoreML/getting-a-core-ml-model)*
--- END FILE ---

--- FILE: generating-content-and-performing-tasks.md ---
# Generating content and performing tasks with Foundation Models

**Enhance the experience in your app by prompting an on-device large language model.**


## Overview

The Foundation Models framework lets you tap into the on-device large models at the core of Apple Intelligence. You can enhance your app by using generative models to create content or perform tasks. The framework supports language understanding and generation based on model capabilities.

For design guidance, see Human Interface Guidelines > Technologies > [https://developer.apple.com/design/human-interface-guidelines/generative-ai](https://developer.apple.com/design/human-interface-guidelines/generative-ai).


## Understand model capabilities

When considering features for your app, it helps to know what the on-device language model can do. The on-device model supports text generation and understanding that you can use to:

The on-device language model may not be suitable for handling all requests, like:

The model can complete complex generative tasks when you use guided generation or tool calling. For more on handling complex tasks, or tasks that require extensive world-knowledge, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation) and [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling).


## Check for availability

Before you use the on-device model in your app, check that the model is available by creating an instance of [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel) with the [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/default](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel/default) property.

Model availability depends on device factors like:

- The device must support Apple Intelligence.

- The device must have Apple Intelligence turned on in Settings.


> **NOTE**: It can take some time for the model to download and become available when a person turns on Apple Intelligence.


Always verify model availability first, and plan for a fallback experience in case the model is unavailable.

```swift
struct GenerativeView: View {
    // Create a reference to the system language model.
    private var model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            // Show your intelligence UI.
        case .unavailable(.deviceNotEligible):
            // Show an alternative UI.
        case .unavailable(.appleIntelligenceNotEnabled):
            // Ask the person to turn on Apple Intelligence.
        case .unavailable(.modelNotReady):
            // The model isn't ready because it's downloading or because of other system reasons.
        case .unavailable(let other):
            // The model is unavailable for an unknown reason.
        }
    }
}
```


## Create a session

After confirming that the model is available, create a [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession) object to call the model. For a single-turn interaction, create a new session each time you call the model:

```swift
// Create a session with the system model.
let session = LanguageModelSession()
```

For a multiturn interaction ‚Äî where the model retains some knowledge of what it produced ‚Äî reuse the same session each time you call the model.


## Provide a prompt to the model

A [doc://com.apple.foundationmodels/documentation/FoundationModels/Prompt](https://developer.apple.com/documentation/FoundationModels/Prompt) is an input that the model responds to. Prompt engineering is the art of designing high-quality prompts so that the model generates a best possible response for the request you make. A prompt can be as short as ‚Äúhello‚Äù, or as long as multiple paragraphs. The process of designing a prompt involves a lot of exploration to discover the best prompt, and involves optimizing prompt length and writing style.

When thinking about the prompt you want to use in your app, consider using conversational language in the form of a question or command. For example, ‚ÄúWhat‚Äôs a good month to visit Paris?‚Äù or ‚ÄúGenerate a food truck menu.‚Äù

Write prompts that focus on a single and specific task, like ‚ÄúWrite a profile for the dog breed Siberian Husky‚Äù. When a prompt is long and complicated, the model takes longer to respond, and may respond in unpredictable ways. If you have a complex generation task in mind, break the task down into a series of specific prompts.

You can refine your prompt by telling the model exactly how much content it should generate. A prompt like, ‚ÄúWrite a profile for the dog breed Siberian Husky‚Äù often takes a long time to process as the model generates a full multi-paragraph essay. If you specify ‚Äúusing three sentences‚Äù, it speeds up processing and generates a concise summary. Use phrases like ‚Äúin a single sentence‚Äù or ‚Äúin a few words‚Äù to shorten the generation time and produce shorter text.

```swift
// Generate a longer response for a specific command.
let simple = "Write me a story about pears."

// Quickly generate a concise response.
let quick = "Write the profile for the dog breed Siberian Husky using three sentences."
```


## Provide instructions to the model

[doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions](https://developer.apple.com/documentation/FoundationModels/Instructions) help steer the model in a way that fits the use case of your app. The model obeys prompts at a lower priority than the instructions you provide. When you provide instructions to the model, consider specifying details like:

- What the model‚Äôs role is; for example, ‚ÄúYou are a mentor,‚Äù or ‚ÄúYou are a movie critic‚Äù.

- What the model should do, like ‚ÄúHelp the person extract calendar events,‚Äù or ‚ÄúHelp the person by recommending search suggestions‚Äù.

- What the style preferences are, like ‚ÄúRespond as briefly as possible‚Äù.

- What the possible safety measures are, like ‚ÄúRespond with ‚ÄòI can‚Äôt help with that‚Äô if you‚Äôre asked to do something dangerous‚Äù.

Use content you trust in instructions because the model follows them more closely than the prompt itself. When you initialize a session with instructions, it affects all prompts the model responds to in that session. Instructions can also include example responses to help steer the model. When you add examples to your prompt, you provide the model with a template that shows the model what a good response looks like.


## Generate a response

To call the model with a prompt, call [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re) on your session. The response call is asynchronous because it may take a few seconds for the on-device foundation model to generate the response.

```swift
let instructions = """
    Suggest five related topics. Keep them concise (three to seven words) and make sure they \
    build naturally from the person's topic.
    """

let session = LanguageModelSession(instructions: instructions)

let prompt = "Making homemade bread"
let response = try await session.respond(to: prompt)
```


> **NOTE**: A session can only handle a single request at a time, and causes a runtime error if you call it again before the previous request finishes. Check [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/isResponding](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/isResponding) to verify the session is done processing the previous request before sending a new one.


Instead of working with raw string output from the model, the framework offers guided generation to generate a custom Swift data structure you define. For more information about guided generation, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation).

When you make a request to the model, you can provide custom tools to help the model complete the request. If the model determines that a [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) can assist with the request, the framework calls your [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) to perform additional actions like retrieving content from your local database. For more information about tool calling, see [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling)


## Consider context size limits per session

The *context window size* is a limit on how much data the model can process for a session instance. A token is a chunk of text the model processes, and the system model supports up to 4,096 tokens. A single token corresponds to three or four characters in languages like English, Spanish, or German, and one token per character in languages like Japanese, Chinese, or Korean. In a single session, the sum of all tokens in the instructions, all prompts, and all outputs count toward the context window size.

If your session processes a large amount of tokens that exceed the context window, the framework throws the error [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)). When you encounter the error, start a new session and try shortening your prompts. If you need to process a large amount of data that won‚Äôt fit in a single context window limit, break your data into smaller chunks, process each chunk in a separate session, and then combine the results.


## Tune generation options and optimize performance

To get the best results for your prompt, experiment with different generation options. [doc://com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions](https://developer.apple.com/documentation/FoundationModels/GenerationOptions) affects the runtime parameters of the model, and you can customize them for every request you make.

```swift
// Customize the temperature to increase creativity.
let options = GenerationOptions(temperature: 2.0)

let session = LanguageModelSession()

let prompt = "Write me a story about coffee."
let response = try await session.respond(
    to: prompt,
    options: options
)
```

When you test apps that use the framework, use Xcode Instruments to understand more about the requests you make, like the time it takes to perform a request. When you make a request, you can access the [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript](https://developer.apple.com/documentation/FoundationModels/Transcript) entries that describe the actions the model takes during your [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession).

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models)*
--- END FILE ---

--- FILE: FoundationModels-Using-on-device-LLM-in-your-app.md ---
# Foundation Models: Using Apple's On-Device LLM in Your Apps

## Overview

Foundation Models is an Apple framework that provides access to on-device large language models (LLMs) that power Apple Intelligence. This framework enables developers to enhance their apps with generative AI capabilities without requiring cloud connectivity or compromising user privacy.

Key capabilities include:
- Text generation and understanding
- Content summarization and extraction
- Structured data generation
- Custom tool integration

## Getting Started

### Check Model Availability

Always check if the model is available before attempting to use it. Model availability depends on device factors such as Apple Intelligence support, system settings, and device state.

```swift
struct GenerativeView: View {
    // Create a reference to the system language model
    private var model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            // Show your intelligence UI
            Text("Model is available")
        case .unavailable(.deviceNotEligible):
            // Show an alternative UI
            Text("Device not eligible for Apple Intelligence")
        case .unavailable(.appleIntelligenceNotEnabled):
            // Ask the person to turn on Apple Intelligence
            Text("Please enable Apple Intelligence in Settings")
        case .unavailable(.modelNotReady):
            // The model isn't ready (downloading or other system reasons)
            Text("Model is downloading or not ready")
        case .unavailable(let other):
            // The model is unavailable for an unknown reason
            Text("Model unavailable: \(other)")
        }
    }
}
```

### Create a Session

After confirming model availability, create a `LanguageModelSession` to interact with the model:

```swift
// Create a basic session with the system model
let session = LanguageModelSession()

// Create a session with instructions
let instructions = """
    You are a helpful assistant that provides concise answers.
    Keep responses under 100 words and focus on clarity.
    """
let sessionWithInstructions = LanguageModelSession(instructions: instructions)
```

- For single-turn interactions, create a new session each time
- For multi-turn interactions, reuse the same session to maintain context

## Basic Usage

### Provide Instructions to the Model

Instructions help steer the model's behavior for your specific use case. The model prioritizes instructions over prompts.

Good instructions typically specify:
- The model's role (e.g., "You are a mentor")
- What the model should do (e.g., "Help extract calendar events")
- Style preferences (e.g., "Respond as briefly as possible")
- Safety measures (e.g., "Respond with 'I can't help with that' for dangerous requests")

```swift
let instructions = """
    You are a cooking assistant.
    Provide recipe suggestions based on ingredients.
    Keep suggestions brief and practical for home cooks.
    Include approximate cooking time.
    """

let session = LanguageModelSession(instructions: instructions)
```

### Provide a Prompt to the Model

A prompt is the input that the model responds to. Effective prompts are:
- Conversational (questions or commands)
- Focused on a single, specific task
- Clear about the desired output format and length

```swift
// Simple prompt
let prompt = "What's a good month to visit Paris?"

// Specific prompt with output constraints
let specificPrompt = "Write a profile for the dog breed Siberian Husky using three sentences."
```

### Generate a Response

Call the model asynchronously to get a response:

```swift
// Basic response generation
let response = try await session.respond(to: prompt)
print(response.content)

// With custom generation options
let options = GenerationOptions(temperature: 0.7)
let customResponse = try await session.respond(to: prompt, options: options)
```

Note: A session can only handle one request at a time. Check `isResponding` to verify the session is available before sending a new request.

## Advanced Features

### Guided Generation

Guided generation allows you to receive model responses as structured Swift data instead of raw strings. This provides stronger guarantees about the format of the response.

#### 1. Define a Generable Type

```swift
@Generable(description: "Basic profile information about a cat")
struct CatProfile {
    // A guide isn't necessary for basic fields
    var name: String

    @Guide(description: "The age of the cat", .range(0...20))
    var age: Int

    @Guide(description: "A one sentence profile about the cat's personality")
    var profile: String
}
```

#### 2. Request a Response in Your Custom Type

```swift
// Generate a response using the custom type
let catResponse = try await session.respond(
    to: "Generate a cute rescue cat",
    generating: CatProfile.self
)

// Use the structured data
print("Name: \(catResponse.content.name)")
print("Age: \(catResponse.content.age)")
print("Profile: \(catResponse.content.profile)")
```

#### 3. Printing a Response from your Custom Type

When printing values from a LanguageModelSession.Response always use the instance property content. Not output.

For example:

```swift
import FoundationModels
import Playgrounds

@Generable
struct CookbookSuggestions {
    @Guide(description: "Cookbook Suggestions", .count(3))
    var suggestions: [String]
}

#Playground {
    let session = LanguageModelSession()

    let prompt = "What's a good name for a cooking app?"

    let response = try await session.respond(
        to: prompt,
        generating: CookbookSuggestions.self
    )

    // Notice how print values come from content. Not output.
    print(response.content.suggestions)
}
```

### Tool Calling

Tool calling allows the model to use custom code you provide to perform specific tasks, access external data, or integrate with other frameworks.

#### 1. Create a Custom Tool

```swift
// Define a tool for searching recipes
struct RecipeSearchTool: Tool {
    struct Arguments: Codable {
        var searchTerm: String
        var numberOfResults: Int
    }
    
    func call(arguments: Arguments) async throws -> ToolOutput {
        // Search your recipe database
        let recipes = await searchRecipes(term: arguments.searchTerm, 
                                         limit: arguments.numberOfResults)
        
        // Return results as a string the model can use
        return .string(recipes.map { "- \($0.name): \($0.description)" }.joined(separator: "\n"))
    }
    
    private func searchRecipes(term: String, limit: Int) async -> [Recipe] {
        // Implementation to search your database
        // ...
    }
}
```

#### 2. Provide the Tool to a Session

```swift
// Create the tool
let recipeSearchTool = RecipeSearchTool()

// Create a session with the tool
let session = LanguageModelSession(tools: [recipeSearchTool])

// The model will automatically use the tool when appropriate
let response = try await session.respond(to: "Find me some pasta recipes")
```

#### 3. Handle Tool Errors

```swift
do {
    let answer = try await session.respond("Find a recipe for tomato soup.")
} catch let error as LanguageModelSession.ToolCallError {
    // Access the name of the tool
    print(error.tool.name) 
    
    // Access the underlying error
    if case .databaseIsEmpty = error.underlyingError as? RecipeSearchToolError {
        // Handle specific error
    }
} catch {
    print("Other error: \(error)")
}
```

## Snapshot streaming

- LLM generate text as short groups of characters called tokens.
- Typically, when streaming tokens, tokens are delivered in what's called a delta. But Foundation Models does this different.
- As deltas are produced, the responsibility for accumulating them usually falls on the developer
- You append each delta as they come in. And the response grows as you do. But it gets tricky when the result has structure.
- If you want to show the greeting string after each delta, you have to parse it out of the accumulation, and that's not trival, especially for complicated structures.
- Structured output is at the core of the Foundation Model framework. Which is why we stream snapshots.

## Snapshot streaming

- LLM generate text as short groups of characters called tokens.
- Typically, when streaming tokens, tokens are delivered in what's called a delta. But Foundation Models does this different.
- As deltas are produced, the responsibility for accumulating them usually falls on the developer
- You append each delta as they come in. And the response grows as you do. But it gets tricky when the result has structure.
- If you want to show the greeting string after each delta, you have to parse it out of the accumulation, and that's not trival, especially for complicated structures.
- Structured output is at the core of the Foundation Model framework. Which is why we stream snapshots.

### What are snapshots

- Snapshots represent partically generated response. Their properties are all optinoal. And they get filled in as the model produces more of the response.
- Snapshots are a robust and convenient representation for streaming structure output.
- You are already familar with the `@Generable` macro, and as it turns out, it's also where the definitions for partially generated types come from.
- If you expand the macro, you'll discover it produces a types named `PartiallyGenerated`. It is effectively a mirror of the outer structure except every property is optional.
- The partically generated type comes into play when you call the 'streamResponse` method on your session.

```swift
import FoundationModels
import Playgrounds

@Generable
struct TripIdeas {
    @Guide(description: "Ideas for upcoming trips")
    var ideas: [String]
}

#Playground {
    let session = LanguageModelSession()

    let prompt = "What are some exciting trip ideas for the upcoming year?"

    let stream = session.streamResponse(
        to: prompt,
        generating: TripIdeas.self
    )

    for try await partial in stream {
        print(partial)
    }
}
```

- Stream response returns an async sequence. And the elements of that sequence are instances of a partially generated type.
- Each element in the sequence will contain an updated snapshot.
- These snapshots work great with declarative frameworks like SwiftUI.
- First, create state holding a partially generated type.
- Then, just iterate over a response stream, stores its elements, and watch as your UI comes to life.

## Best Practices and Limitations

### Context Size Limits

- The system model supports up to 4,096 tokens per session
- A token is roughly 3-4 characters in languages like English
- All instructions, prompts, and outputs count toward this limit
- If you exceed the limit, you'll get a `LanguageModelSession.GenerationError.exceededContextWindowSize` error
- For large data processing, break it into smaller chunks across multiple sessions

### Optimizing Performance

- Use `GenerationOptions` to tune model behavior:
  ```swift
  let options = GenerationOptions(temperature: 2.0) // Higher temperature = more creative
  ```
- Use Xcode Instruments to monitor request performance
- Access `Transcript` entries to see model actions during a session:
  ```swift
  let transcript = session.transcript
  ```

### Prompt Engineering Tips

- Be specific about what you want
- Specify output constraints (e.g., "in three sentences")
- Break complex tasks into multiple simple prompts
- Use examples in instructions to guide the model's output format

## References

- [Generating content and performing tasks with Foundation Models](https://developer.apple.com/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models)
- [Generating Swift data structures with guided generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation)
- [Expanding generation with tool calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling)
- [Human Interface Guidelines: Generative AI](https://developer.apple.com/design/human-interface-guidelines/technologies/generative-ai)
--- END FILE ---

--- FILE: Improving-Your-Model-s-Accuracy.md ---
# Improving Your Model‚Äôs Accuracy

**Use metrics to tune the performance of your machine learning model.**


## Overview

Evaluating and improving your model starts with looking at its performance across different data sets. Metrics from each dataset inform which changes have the most impact on your model‚Äôs accuracy.

No single metric can tell you everything about your model‚Äôs performance. To improve your model, you compare the metrics ([doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics) or [doc://com.apple.createml/documentation/CreateML/MLRegressorMetrics](https://developer.apple.com/documentation/CreateML/MLRegressorMetrics) depending on your model type) among your training, validation, and testing data sets. For example, the accuracy discussed in the [doc://com.apple.createml/documentation/CreateML/creating-an-image-classifier-model](https://developer.apple.com/documentation/CreateML/creating-an-image-classifier-model) article is derived from the [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics/classificationError](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics/classificationError) metric for each data set.

You can also access these values programmatically after creating a model and loading your testing data:

```swift
print("Training Metrics\n", model.trainingMetrics)
print("Validation Metrics\n", model.validationMetrics)

let evaluationMetrics = model.evaluation(on: testData)
print("Evaluation Metrics\n", evaluationMetrics)
```

In this case, you see output for several different metrics, including [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics/classificationError](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics/classificationError), [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics/precisionRecall](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics/precisionRecall), and [doc://com.apple.createml/documentation/CreateML/MLClassifierMetrics/confusion](https://developer.apple.com/documentation/CreateML/MLClassifierMetrics/confusion) for classifiers and [doc://com.apple.createml/documentation/CreateML/MLRegressorMetrics/maximumError](https://developer.apple.com/documentation/CreateML/MLRegressorMetrics/maximumError) and [doc://com.apple.createml/documentation/CreateML/MLRegressorMetrics/rootMeanSquaredError](https://developer.apple.com/documentation/CreateML/MLRegressorMetrics/rootMeanSquaredError) for regressors. Use these values from each data set to determine where your model needs to improve.


### Improve Your Model‚Äôs Training Accuracy

If the training accuracy of your model is low, it‚Äôs an indication that your current model configuration can‚Äôt capture the complexity of your data.

Try adjusting the training parameters. When working with image data, double the maximum number of iterations in the `MLImageClassifierBuilder` playground UI (the default value is 10).

![Image](improving-your-model-s-accuracy-1)

For natural language data, try a different underlying algorithm (see [doc://com.apple.createml/documentation/CreateML/MLTextClassifier/ModelAlgorithmType](https://developer.apple.com/documentation/CreateML/MLTextClassifier/ModelAlgorithmType)). For more general tasks, use a different underlying model than the type determined by [doc://com.apple.createml/documentation/CreateML/MLClassifier](https://developer.apple.com/documentation/CreateML/MLClassifier) (see *Supporting Classifier Types*) or [doc://com.apple.createml/documentation/CreateML/MLRegressor](https://developer.apple.com/documentation/CreateML/MLRegressor) (see *Supporting Regressor Types*).


### Improve Your Model‚Äôs Validation Accuracy

If your model‚Äôs accuracy on the validation set is low or fluctuates between low and high each time you train the model, you need more data. You can generate more input data from the examples you already collected, a technique known as *data augmentation*. For image data, you can combine operations like cropping, rotation, blurring, and exposure adjustment to make one image into many examples.

![Image](improving-your-model-s-accuracy-2)

It‚Äôs also possible for you to have lots of data and validation accuracy that is still significantly lower than your training accuracy. In this case, your model is _overfitting, _meaning that it‚Äôs learning too many specific details about your training set that don‚Äôt generally apply to other examples. In this case, you need to reduce the number of training iterations to prevent the model from learning too much about your training data.


### Improve Your Model‚Äôs Evaluation Accuracy

If your model‚Äôs accuracy on your testing data is lower than your training or validation accuracy, it usually indicates that there are meaningful differences between the kind of data you trained the model on and the testing data you‚Äôre providing for evaluation.

For example, suppose you train your [doc://com.apple.createml/documentation/CreateML/MLImageClassifier](https://developer.apple.com/documentation/CreateML/MLImageClassifier) on many images of indoor cats, but then test only on images of outdoor cats. Because of the differences in lighting, exposure, and background, it‚Äôs unlikely that your testing data will yield good results. Differences between images that seem obvious to humans can be difficult for a model to resolve without sufficient training data.

To correct for this, provide more diverse data in your training set. In general, more examples lead to higher performance, but it‚Äôs also important to show your model examples that are as varied your testing data.

---

*Source: [https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/improving-your-model-s-accuracy](https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/improving-your-model-s-accuracy)*
--- END FILE ---

--- FILE: MLCheckpoint.md ---
# MLCheckpoint

**The state of a model‚Äôs asynchronous training session at a specific point in time during the feature extraction or training phase.**

## Availability

- **iOS** 15.0+
- **iPadOS** 15.0+
- **Mac Catalyst** 15.0+
- **macOS** 11.0+
- **tvOS** 16.0+
- **visionOS** 1.0+


## Topics

### Inspecting a checkpoint

- [phase](https://developer.apple.com/documentation/createml/mlcheckpoint/phase) ‚Äî The training session‚Äôs phase when it created the checkpoint.
- [iteration](https://developer.apple.com/documentation/createml/mlcheckpoint/iteration) ‚Äî The iteration number of a training session‚Äôs phase when it created the checkpoint.
- [date](https://developer.apple.com/documentation/createml/mlcheckpoint/date) ‚Äî The time when the training session created the checkpoint.
- [url](https://developer.apple.com/documentation/createml/mlcheckpoint/url) ‚Äî The location of the checkpoint in the file system.
### Assessing a checkpoint

- [metrics](https://developer.apple.com/documentation/createml/mlcheckpoint/metrics) ‚Äî Measurements of the model‚Äôs performance at the time the session saved the checkpoint.
- [MLProgress.Metric](https://developer.apple.com/documentation/createml/mlprogress/metric) ‚Äî Metrics you use to evaluate a model‚Äôs performance during a training session.
### Encoding and decoding a checkpoint

- [encode(to:)](https://developer.apple.com/documentation/createml/mlcheckpoint/encode(to:)) ‚Äî Encodes the checkpoint into the encoder.
- [init(from:)](https://developer.apple.com/documentation/createml/mlcheckpoint/init(from:)) ‚Äî Creates a new checkpoint by decoding from the decoder.

---

*Source: [https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLCheckpoint](https://developer.apple.com/documentation/com.apple.createml/documentation/CreateML/MLCheckpoint)*
--- END FILE ---
=== END SWIFT DOCUMENTATION ===
=== END CONTEXT ===


Please implement the requirements above. Write the code directly - do not explain, just write the implementation.