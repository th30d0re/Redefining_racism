You are a Swift/SwiftUI developer. Your task is to implement the following requirements.

IMPORTANT INSTRUCTIONS:
1. Write clean, production-ready Swift code
2. Follow Apple's Swift API Design Guidelines
3. Use SwiftUI for UI components where appropriate
4. Include proper error handling
5. The code must compile without errors
6. After writing code, the build will be verified automatically
7. If the build fails, you will receive error feedback and should try a DIFFERENT approach

TESTING & DEBUGGING REFERENCE:
For building, testing, and debugging iOS/macOS apps, reference this workflow guide:
/Users/emmanuel/Dev/Tools/Eocon-Foundation-V1/.Foundation/Docs/swiftDocs/Testing/XCODEBUILD_MCP_WORKFLOW.md

This guide covers:
- XcodeBuild MCP server tools for programmatic Xcode interaction
- Building for simulator, booting simulators, installing/launching apps
- UI automation: screenshots, accessibility hierarchy, tap simulation
- Debugging UI issues (button taps, gestures, navigation)

=== TASK/REQUIREMENTS ===
I have created the following plan after thorough exploration and analysis of the codebase. Follow the below plan verbatim. Trust the files and references. Do not re-verify what's written in the plan. Explore only when absolutely necessary. First implement all the proposed file changes and then I'll review all the changes together at the end.

## Observations

T8 builds three macOS-only research tools on top of the already-complete T1–T7 foundation. \`ContentView\` already has \`.training\`, \`.validation\`, \`.data\` sidebar cases wired to \`ContentUnavailableView\` placeholders. \`HistoricalPolicies\` and \`DefaultPolicyScorer\` are fully implemented. \`AppDependencies\` is the established DI container. All new code must be gated with \`#if os(macOS)\` and follow the MVVM-S pattern (\`@Observable @MainActor\` ViewModel + pure-value Style struct + SwiftUI View).

## Approach

Build bottom-up: data types → backend actors → ViewModels + Style structs → Views → wire into \`ContentView\` and \`AppDependencies\`. This mirrors how T1–T7 were layered and keeps each step independently compilable.

---

## Implementation Steps

### Step 1 — Shared macOS-only data types

Create \`file:app/decodingOppression/decodingOppression/Models/TrainingModels.swift\` gated with \`#if os(macOS)\`.

Define the following types, all \`Codable, Sendable\`:

| Type | Key fields |
|---|---|
| \`LoRAConfig\` | \`epochs: Int\`, \`learningRate: Double\`, \`loraRank: Int\`, \`alpha: Int\` — with static \`default\` constant (3, 0.0001, 8, 16) |
| \`TrainingClause\` | \`id: UUID\`, \`text: String\`, \`sourcePolicy: String\`, \`targetGroup: TargetGroup\`, \`effectDirection: EffectDirection\`, \`architectureScores: ArchitectureScores\`, \`proxyVariables: [String]\`, \`usesProxyVariables: Bool\` |
| \`ValidationResult\` | \`policyName: String\`, \`year: Int\`, \`expectedCOI: Double\`, \`actualCOI: Double\`, \`delta: Double\`, \`passed: Bool\` — \`passed\` is \`abs(delta) <= 0.10\` |
| \`LoRAAdapterMetadata\` | \`id: UUID\`, \`name: String\`, \`timestamp: Date\`, \`trainingConfig: LoRAConfig\`, \`validationResults: [ValidationResult]?\`, \`isActive: Bool\` |

Define \`TrainingProgress\` as a \`Sendable\` enum (not \`Codable\`):
- \`.epoch(current: Int, total: Int, trainLoss: Double, valLoss: Double)\`
- \`.complete(adapterPath: URL)\`
- \`.failed(Error)\`

\`ArchitectureScores\` is already \`Codable, Sendable\` in \`file:app/decodingOppression/decodingOppression/Models/PipelineContracts.swift\` — reuse it directly.

---

### Step 2 — \`TrainingDataStore\` actor

Create \`file:app/decodingOppression/decodingOppression/Data/TrainingDataStore.swift\` gated with \`#if os(macOS)\`.

\`actor TrainingDataStore\`:
- Holds \`private var clauses: [TrainingClause]\` and \`private(set) var hasUnsavedChanges: Bool = false\`.
- \`func load() async throws\` — reads \`historical_clauses.jsonl\` from Application Support (\`FileManager.default.urls(for: .applicationSupportDirectory, in: .userDomainMask)\`). Falls back to the bundled \`Data/historical_clauses.jsonl\` if the Application Support copy does not exist. Decodes each line as a \`TrainingClause\` via \`JSONDecoder\`.
- \`func save() async throws\` — encodes each clause as a JSON line via \`JSONEncoder\`, writes atomically to the Application Support path, sets \`hasUnsavedChanges = false\`.
- \`func allClauses() -> [TrainingClause]\` — returns current in-memory array.
- \`func filtered(policy: String?, targetGroup: TargetGroup?) -> [TrainingClause]\` — filters in-memory array.
- \`func add(_ clause: TrainingClause)\`, \`func update(_ clause: TrainingClause)\`, \`func delete(id: UUID)\` — mutate in-memory array and set \`hasUnsavedChanges = true\`.
- \`static let shared = TrainingDataStore()\` for use in \`AppDependencies\`.

---

### Step 3 — \`ValidationRunner\`

Create \`file:app/decodingOppression/decodingOppression/AI/ValidationRunner.swift\` gated with \`#if os(macOS)\`.

\`struct ValidationRunner: Sendable\` (stateless — no actor needed):
- \`func run(policy: HistoricalPolicy, scorer: PolicyScorer) -> ValidationResult\` — calls \`scorer.score(clauses: policy.clauses)\`, computes \`delta = actualCOI - expectedCOI\`, returns \`ValidationResult\`.
- \`func runAll(scorer: PolicyScorer) -> [ValidationResult]\` — maps over \`HistoricalPolicies.chain\`.

Both methods are synchronous because \`DefaultPolicyScorer.score\` is pure Swift math with no async work.

---

### Step 4 — \`TrainingManager\` actor

Create \`file:app/decodingOppression/decodingOppression/MLX/TrainingManager.swift\` gated with \`#if os(macOS)\`.

\`actor TrainingManager\`:
- \`private var trainingTask: Task<Void, Never>?\`
- \`private var latestCheckpointURL: URL?\`
- \`private let adapterDirectory: URL\` — computed from Application Support.

\`func train(config: LoRAConfig, dataStore: TrainingDataStore) -> AsyncStream<TrainingProgress>\`:
- Returns an \`AsyncStream<TrainingProgress>\` whose continuation drives the UI.
- Inside the stream's \`Task\`: load the base model via \`MLXLLM\`, load training clauses from \`dataStore\`, build prompt/completion pairs, run the LoRA training loop for \`config.epochs\` epochs.
- After each epoch: write a checkpoint to \`adapterDirectory/checkpoints/epoch_N/\`, yield \`.epoch(current:total:trainLoss:valLoss:)\`.
- On successful completion: write the final adapter atomically to \`adapterDirectory/adapters/<name>/\`, write \`LoRAAdapterMetadata\` as JSON alongside it, yield \`.complete(adapterPath:)\`.
- On \`Task.isCancelled\`: stop the loop, preserve \`latestCheckpointURL\`, yield nothing further (stream ends cleanly).
- On error: yield \`.failed(error)\`.
- Store the \`Task\` handle in \`trainingTask\`.

\`func cancel()\` — cancels \`trainingTask\`.

\`func resume(from checkpointURL: URL, config: LoRAConfig, dataStore: TrainingDataStore) -> AsyncStream<TrainingProgress>\` — same as \`train\` but loads weights from the checkpoint before starting the loop.

\`func setActiveAdapter(metadata: LoRAAdapterMetadata) throws\` — reads all \`LoRAAdapterMetadata\` files in \`adapterDirectory\`, sets \`isActive = false\` on all, sets \`isActive = true\` on the matching one, writes back.

\`func loadActiveAdapterMetadata() throws -> LoRAAdapterMetadata?\` — scans \`adapterDirectory\` for the metadata file where \`isActive == true\`.

---

### Step 5 — \`AppDependencies\` macOS additions

In \`file:app/decodingOppression/decodingOppression/decodingOppressionApp.swift\`, inside the \`AppDependencies\` class, add a \`#if os(macOS)\` block:

\`\`\`
#if os(macOS)
let trainingManager: TrainingManager
let validationRunner: ValidationRunner
let trainingDataStore: TrainingDataStore
#endif
\`\`\`

Initialize them in \`init(modelDownloadManager:)\` inside the same guard. \`ValidationRunner\` is a value type so it can be a \`let\` constant. \`TrainingManager\` and \`TrainingDataStore\` are actors initialized with \`TrainingManager()\` and \`TrainingDataStore.shared\`.

---

### Step 6 — \`TrainingViewModel\`

Create \`file:app/decodingOppression/decodingOppression/ViewModels/TrainingViewModel.swift\` gated with \`#if os(macOS)\`.

\`@Observable @MainActor final class TrainingViewModel\`:

| Property | Type | Purpose |
|---|---|---|
| \`config\` | \`LoRAConfig\` | Bound to config fields; initialized to \`LoRAConfig.default\` |
| \`isTraining\` | \`Bool\` | Drives button label and field editability |
| \`currentEpoch\` | \`Int\` | Epoch counter display |
| \`totalEpochs\` | \`Int\` | Mirrors \`config.epochs\` during training |
| \`trainLoss\` | \`Double\` | Latest train loss |
| \`valLoss\` | \`Double\` | Latest val loss |
| \`lossHistory\` | \`[(epoch: Int, trainLoss: Double, valLoss: Double)]\` | Data source for Swift Charts \`LineMark\` |
| \`estimatedTimeRemaining\` | \`TimeInterval?\` | Computed from epoch duration × remaining epochs |
| \`completedAdapterMetadata\` | \`LoRAAdapterMetadata?\` | Set on \`.complete\`; drives success banner |
| \`activeAdapterMetadata\` | \`LoRAAdapterMetadata?\` | Loaded on appear; updated after \"Set as Active\" |
| \`trainingClauseCount\` | \`Int\` | Loaded from \`TrainingDataStore\`; disables Start button when 0 |
| \`error\` | \`Error?\` | Set on \`.failed\` |
| \`isAnalysisRunning\` | \`Bool\` | Injected from outside; drives warning banner |

Methods:
- \`func onAppear(manager: TrainingManager, dataStore: TrainingDataStore) async\` — loads \`activeAdapterMetadata\` and \`trainingClauseCount\`.
- \`func startTraining(manager: TrainingManager, dataStore: TrainingDataStore)\` — sets \`isTraining = true\`, iterates the \`AsyncStream<TrainingProgress>\`, updates properties on each event.
- \`func cancelTraining(manager: TrainingManager)\` — calls \`manager.cancel()\`, resets \`isTraining = false\`.
- \`func setActiveAdapter(manager: TrainingManager)\` — calls \`manager.setActiveAdapter(metadata:)\` with \`completedAdapterMetadata\`, refreshes \`activeAdapterMetadata\`.

---

### Step 7 — \`ValidationViewModel\`

Create \`file:app/decodingOppression/decodingOppression/ViewModels/ValidationViewModel.swift\` gated with \`#if os(macOS)\`.

\`@Observable @MainActor final class ValidationViewModel\`:

| Property | Type | Purpose |
|---|---|---|
| \`results\` | \`[ValidationResult]\` | One per historical policy; empty until first run |
| \`runningPolicyNames\` | \`Set<String>\` | Which cards show a spinner |
| \`summaryText\` | \`String?\` | e.g. \"4 of 4 tests passed\" — set after \`runAll\` |

Methods:
- \`func runAll(runner: ValidationRunner, scorer: PolicyScorer) async\` — runs all 4 policies sequentially (not concurrently, per spec \"informative non-blocking\"), updates \`results\` and \`summaryText\`.
- \`func run(policy: HistoricalPolicy, runner: ValidationRunner, scorer: PolicyScorer) async\` — runs a single policy, inserts/replaces its \`ValidationResult\` in \`results\`.

Both methods add/remove the policy name from \`runningPolicyNames\` around the call.

---

### Step 8 — \`TrainingDataViewModel\`

Create \`file:app/decodingOppression/decodingOppression/ViewModels/TrainingDataViewModel.swift\` gated with \`#if os(macOS)\`.

\`@Observable @MainActor final class TrainingDataViewModel\`:

| Property | Type | Purpose |
|---|---|---|
| \`clauses\` | \`[TrainingClause]\` | Filtered view of store |
| \`allClauses\` | \`[TrainingClause]\` | Full unfiltered list |
| \`selectedClause\` | \`TrainingClause?\` | Drives right-panel editor |
| \`editingClause\` | \`TrainingClause?\` | Mutable copy being edited |
| \`filterPolicy\` | \`String?\` | Filter picker binding |
| \`filterTargetGroup\` | \`TargetGroup?\` | Filter picker binding |
| \`hasUnsavedChanges\` | \`Bool\` | Mirrored from store |
| \`showUnsavedChangesAlert\` | \`Bool\` | Triggers confirmation dialog |
| \`pendingAction\` | \`(() -> Void)?\` | Action to execute after confirmation |

Methods:
- \`func load(store: TrainingDataStore) async throws\` — calls \`store.load()\`, refreshes \`allClauses\` and \`clauses\`.
- \`func applyFilters()\` — recomputes \`clauses\` from \`allClauses\` using \`filterPolicy\` and \`filterTargetGroup\`.
- \`func saveEditing(store: TrainingDataStore) async throws\` — calls \`store.update(editingClause)\` then \`store.save()\`.
- \`func addNew(store: TrainingDataStore)\` — creates a blank \`TrainingClause\`, calls \`store.add(...)\`, sets \`selectedClause\` and \`editingClause\`.
- \`func delete(_ clause: TrainingClause, store: TrainingDataStore) async throws\` — calls \`store.delete(id:)\` then \`store.save()\`.
- \`func exportJSONL(store: TrainingDataStore) async throws\` — calls \`store.save()\`, then presents a save panel via \`NSSavePanel\`.
- \`func requestNavigation(action: @escaping () -> Void)\` — if \`hasUnsavedChanges\`, sets \`pendingAction = action\` and \`showUnsavedChangesAlert = true\`; otherwise calls \`action()\` immediately.

---

### Step 9 — Style structs

Create three style files, all gated with \`#if os(macOS)\`, following the pattern of \`file:app/decodingOppression/decodingOppression/Views/Style/AnalysisViewStyle.swift\` (no SwiftUI import, pure functions mapping state to visual properties):

**\`file:app/decodingOppression/decodingOppression/Views/Style/TrainingViewStyle.swift\`**
- \`static func trainButtonLabel(isTraining: Bool) -> String\`
- \`static func progressAnimation(reduceMotion: Bool) -> Animation?\`
- \`static func lossLineColor(isTrain: Bool) -> Color\` — two distinct colors for train vs val loss lines

**\`file:app/decodingOppression/decodingOppression/Views/Style/ValidationViewStyle.swift\`**
- \`static func resultColor(passed: Bool, differentiateWithoutColor: Bool) -> (Color, String)\` — color + SF Symbol name
- \`static func cardBorderColor(passed: Bool) -> Color\`
- \`static func bannerBackground(allPassed: Bool) -> Color\`

**\`file:app/decodingOppression/decodingOppression/Views/Style/TrainingDataViewStyle.swift\`**
- \`static func unsavedDotOpacity(hasChanges: Bool) -> Double\`
- \`static func sliderLabel(_ value: Double) -> String\` — formats 0–1 to 2 decimal places

---

### Step 10 — \`TrainingView\`

Create \`file:app/decodingOppression/decodingOppression/Views/TrainingView.swift\` gated with \`#if os(macOS)\`.

\`\`\`
struct TrainingView: View {
    @State var viewModel: TrainingViewModel
    @EnvironmentObject private var deps: AppDependencies
    @Environment(\\.accessibilityReduceMotion) private var reduceMotion
\`\`\`

Layout (ScrollView → VStack):

1. **Model status card** — shows \`viewModel.activeAdapterMetadata?.name ?? \"No adapter — using base model\"\` and \`viewModel.trainingClauseCount\` labeled clauses. Use \`.background(.thinMaterial, in: RoundedRectangle(cornerRadius: 10))\`.

2. **Config section** — \`Form\` or \`Grid\` with four fields bound to \`viewModel.config\`: Epochs (\`Stepper\`), Learning Rate (\`TextField\`), LoRA Rank (\`Stepper\`), Alpha (\`Stepper\`). Disabled when \`viewModel.isTraining\`.

3. **Start/Cancel button** — label from \`TrainingViewStyle.trainButtonLabel(isTraining:)\`. Disabled when \`!viewModel.isTraining && viewModel.trainingClauseCount == 0\`. On tap: calls \`viewModel.startTraining\` or \`viewModel.cancelTraining\`.

4. **Warning banner** — shown when \`viewModel.isAnalysisRunning\`, using \`Label(\"Analysis is running — training may be slower\", systemImage: \"exclamationmark.triangle\")\`.

5. **Progress section** (shown when \`viewModel.isTraining || viewModel.completedAdapterMetadata != nil\`):
   - \`Text(\"Epoch \\(viewModel.currentEpoch) of \\(viewModel.totalEpochs)\")\`
   - Estimated time remaining
   - Swift Charts \`Chart\` with two \`LineMark\` series (train loss, val loss) over \`viewModel.lossHistory\`. Use \`foregroundStyle(by: .value(\"Series\", \"Train\"))\` for automatic legend. Apply \`.accessibilityChartDescriptor(...)\` for VoiceOver audio graph.

6. **Success banner** (shown when \`viewModel.completedAdapterMetadata != nil\`) — \"Training complete. Adapter saved as [name · timestamp].\" with a \"Set as Active Adapter\" \`Button\`.

\`.task { await viewModel.onAppear(manager: deps.trainingManager, dataStore: deps.trainingDataStore) }\`

---

### Step 11 — \`ValidationView\`

Create \`file:app/decodingOppression/decodingOppression/Views/ValidationView.swift\` gated with \`#if os(macOS)\`.

\`\`\`
struct ValidationView: View {
    @State var viewModel: ValidationViewModel
    @EnvironmentObject private var deps: AppDependencies
    @Environment(\\.accessibilityDifferentiateWithoutColor) private var differentiateWithoutColor
\`\`\`

Layout:

1. **Header row** — \`HStack\` with title/subtitle on the left and \"Run All Tests\" \`Button\` on the right. Button calls \`Task { await viewModel.runAll(runner: deps.validationRunner, scorer: DefaultPolicyScorer()) }\`.

2. **Summary banner** — shown when \`viewModel.summaryText != nil\`. Background from \`ValidationViewStyle.bannerBackground(allPassed:)\`.

3. **Four test cards** — \`ForEach(HistoricalPolicies.chain)\`. Each card:
   - Left: policy name (\`Text\` bold), year + expected COI (\`Text\` secondary).
   - Right: if \`viewModel.runningPolicyNames.contains(policy.name)\` → \`ProgressView()\`. Else if result exists → pass/fail badge (color + icon from \`ValidationViewStyle.resultColor\`), actual COI, delta. Plus individual \"Run\" \`Button\`.
   - Card border: \`ValidationViewStyle.cardBorderColor(passed:)\` via \`.overlay(RoundedRectangle.stroke(...))\`.
   - Accessibility: \`.accessibilityLabel(\"\\(policy.name), \\(policy.year)\")\`, \`.accessibilityValue(resultDescription)\`.

---

### Step 12 — \`TrainingDataView\`

Create \`file:app/decodingOppression/decodingOppression/Views/TrainingDataView.swift\` gated with \`#if os(macOS)\`.

\`\`\`
struct TrainingDataView: View {
    @State var viewModel: TrainingDataViewModel
    @EnvironmentObject private var deps: AppDependencies
\`\`\`

Layout (\`HSplitView\`):

**Left panel** (clause table):
- Toolbar with filter pickers (\`Picker\` for policy, \`Picker\` for target group) and \"Add Clause\" + \"Export JSONL\" buttons. The Export button shows a dot overlay when \`viewModel.hasUnsavedChanges\`.
- \`Table(viewModel.clauses, selection: \$viewModel.selectedClause)\` with columns: Clause Text (truncated), Source Policy, Target Group, Effect Direction.
- On selection change: set \`viewModel.editingClause\` to a mutable copy of the selected clause.

**Right panel** (editor):
- Shown when \`viewModel.editingClause != nil\`, otherwise \`ContentUnavailableView(\"Select a Clause\", ...)\`.
- \`Form\`:
  - \`TextEditor\` for full clause text.
  - \`Picker(\"Target Group\", ...)\` bound to \`editingClause.targetGroup\`.
  - \`Picker(\"Effect Direction\", ...)\` bound to \`editingClause.effectDirection\`.
  - Four \`Slider(value:in:0...1)\` for AAR, SE, IJ, RSC with \`TrainingViewStyle.sliderLabel\` labels.
  - \`Toggle(\"Uses Proxy Variables\", ...)\` + \`TextField(\"Proxy terms (comma-separated)\", ...)\`.
  - \"Save\" and \"Delete\" buttons.

**Unsaved changes alert** — \`.alert(\"Unsaved Changes\", isPresented: \$viewModel.showUnsavedChangesAlert)\` with \"Discard\" and \"Cancel\" actions. On \"Discard\": calls \`viewModel.pendingAction?()\`.

\`.task { try? await viewModel.load(store: deps.trainingDataStore) }\`

---

### Step 13 — Wire into \`ContentView\`

In \`file:app/decodingOppression/decodingOppression/ContentView.swift\`, inside the \`#if os(macOS)\` block:

1. Add three \`@State\` properties:
   \`\`\`
   @State private var trainingViewModel = TrainingViewModel()
   @State private var validationViewModel = ValidationViewModel()
   @State private var trainingDataViewModel = TrainingDataViewModel()
   \`\`\`

2. Replace the three \`ContentUnavailableView\` placeholders:
   - \`.training\` → \`TrainingView(viewModel: trainingViewModel)\`
   - \`.validation\` → \`ValidationView(viewModel: validationViewModel)\`
   - \`.data\` → \`TrainingDataView(viewModel: trainingDataViewModel)\`

3. Pass \`deps\` via \`.environmentObject(deps)\` — already injected at the app root, so no change needed there.

---

### Step 14 — Xcode project registration

Add all new \`.swift\` files to the Xcode target in \`file:app/decodingOppression/decodingOppression.xcodeproj/project.pbxproj\`. Each file must be added to the \`decodingOppression\` target's compile sources build phase. The \`#if os(macOS)\` guards ensure iOS builds ignore the macOS-only code at compile time without needing separate file membership.

---

### Component dependency diagram

\`\`\`mermaid
graph TD
    CV[ContentView] --> TV[TrainingView]
    CV --> VV[ValidationView]
    CV --> TDV[TrainingDataView]

    TV --> TVM[TrainingViewModel]
    VV --> VVM[ValidationViewModel]
    TDV --> TDVM[TrainingDataViewModel]

    TVM --> TM[TrainingManager actor]
    TVM --> TDS[TrainingDataStore actor]
    VVM --> VR[ValidationRunner]
    VVM --> PS[DefaultPolicyScorer]
    TDVM --> TDS

    TM --> MLXLLM[MLXLLM - mlx-swift-lm]
    VR --> HP[HistoricalPolicies]
    VR --> PS

    TV --> TVS[TrainingViewStyle]
    VV --> VVS[ValidationViewStyle]
    TDV --> TDVS[TrainingDataViewStyle]

    AD[AppDependencies] --> TM
    AD --> VR
    AD --> TDS
\`\`\`
=== END TASK ===


=== REFERENCE CONTEXT ===
Use the following documentation and context as reference:

=== SWIFT DOCUMENTATION ===

--- FILE: Streaming-and-AirPlay.md ---
# Streaming and AirPlay

**Stream content wirelessly to other devices using AirPlay, and handle requests involving FairPlay-protected assets.**


## Topics

### Essentials

- [Supporting AirPlay in your app](https://developer.apple.com/documentation/avfoundation/supporting-airplay-in-your-app) — Set up your app to use AirPlay to send content wirelessly.
### Route selection

- [AVRouteDetector](https://developer.apple.com/documentation/avfoundation/avroutedetector) — An object that detects available media playback routes.
### Buffered playback

- [Implementing simple enhanced buffering for your content](https://developer.apple.com/documentation/avfoundation/implementing-simple-enhanced-buffering-for-your-content) — Configure your app for simple enhanced buffering to stream content faster to AirPlay-enabled devices and supported CarPlay vehicles.
- [Implementing flexible enhanced buffering for your content](https://developer.apple.com/documentation/avfoundation/implementing-flexible-enhanced-buffering-for-your-content) — Configure your app for flexible enhanced buffering to stream content faster to AirPlay-enabled devices and supported CarPlay vehicles.
- [Integrating AirPlay for Long-Form Video Apps](https://developer.apple.com/documentation/avfoundation/integrating-airplay-for-long-form-video-apps) — Integrate AirPlay features and implement a dedicated external playback experience by preparing the routing system for long-form video playback.
### Resource loading

- [AVAssetResourceLoader](https://developer.apple.com/documentation/avfoundation/avassetresourceloader) — An object that mediates resource requests from a URL asset.
- [AVAssetResourceLoaderDelegate](https://developer.apple.com/documentation/avfoundation/avassetresourceloaderdelegate) — Methods you can implement to handle resource-loading requests coming from a URL asset.
- [AVAssetResourceLoadingRequest](https://developer.apple.com/documentation/avfoundation/avassetresourceloadingrequest) — An object that encapsulates information about a resource request from a resource loader object.
- [AVAssetResourceRenewalRequest](https://developer.apple.com/documentation/avfoundation/avassetresourcerenewalrequest) — An object that encapsulates information about a resource request from a resource loader to renew a previously issued request.
- [AVAssetResourceLoadingRequestor](https://developer.apple.com/documentation/avfoundation/avassetresourceloadingrequestor) — An object that contains information about the originator of a resource-loading request.
- [AVAssetResourceLoadingDataRequest](https://developer.apple.com/documentation/avfoundation/avassetresourceloadingdatarequest) — An object for requesting data from a resource that an asset resource-loading request references.
- [AVAssetResourceLoadingContentInformationRequest](https://developer.apple.com/documentation/avfoundation/avassetresourceloadingcontentinformationrequest) — A query for retrieving essential information about a resource that an asset resource-loading request references.
### FairPlay Streaming

- [AVContentKeySession](https://developer.apple.com/documentation/avfoundation/avcontentkeysession) — An object that creates and tracks decryption keys for media data.
- [AVContentKeySessionDelegate](https://developer.apple.com/documentation/avfoundation/avcontentkeysessiondelegate) — A protocol that handles content key requests.
- [AVContentKey](https://developer.apple.com/documentation/avfoundation/avcontentkey) — An object that represents the content key decryptor.
- [AVContentKeySpecifier](https://developer.apple.com/documentation/avfoundation/avcontentkeyspecifier) — An object that uniquely identifies a content key.
- [AVContentKeyRequest](https://developer.apple.com/documentation/avfoundation/avcontentkeyrequest) — An object that encapsulates information about a content decryption key request issued from a content key session object.
- [AVPersistableContentKeyRequest](https://developer.apple.com/documentation/avfoundation/avpersistablecontentkeyrequest) — An object that encapsulates information about a persistable content decryption key request issued from a content key session.
- [AVContentKeyResponse](https://developer.apple.com/documentation/avfoundation/avcontentkeyresponse) — An object that encapsulates information about a response to a content decryption key request.
- [AVExternalContentProtectionStatus](https://developer.apple.com/documentation/avfoundation/avexternalcontentprotectionstatus) — Constants that specify whether sufficient protection exists to display the content.
- [AVSampleBufferAttachContentKey(_:_:_:)](https://developer.apple.com/documentation/avfoundation/avsamplebufferattachcontentkey(_:_:_:)) — Attaches a content key to a sample buffer for the purpose of content decryption.

---

*Source: [https://developer.apple.com/documentation/com.apple.avfoundation/documentation/AVFoundation/streaming-and-airplay](https://developer.apple.com/documentation/com.apple.avfoundation/documentation/AVFoundation/streaming-and-airplay)*
--- END FILE ---

--- FILE: README.md ---
# Apple Docs Avfoundation Documentation

This directory contains Apple Developer Documentation for the **AVFOUNDATION** framework.

## Summary

- **Total pages**: 31
- **Successfully downloaded**: 31
- **Failed**: 0

## Files

- [AVFoundation](AVFoundation.md)
- [Additional-data-capture](Additional-data-capture.md)
- [Audio-and-video-capture](Audio-and-video-capture.md)
- [Audio-mixing](Audio-mixing.md)
- [Audio-playback--recording--and-processing](Audio-playback--recording--and-processing.md)
- [Capture-setup](Capture-setup.md)
- [Composite-assets](Composite-assets.md)
- [Media-assets](Media-assets.md)
- [Media-playback](Media-playback.md)
- [Media-reading-and-writing](Media-reading-and-writing.md)
- [Offline-playback-and-storage](Offline-playback-and-storage.md)
- [Photo-capture](Photo-capture.md)
- [QuickTime-movies](QuickTime-movies.md)
- [Sample-buffer-playback](Sample-buffer-playback.md)
- [Speech-synthesis](Speech-synthesis.md)
- [Streaming-and-AirPlay](Streaming-and-AirPlay.md)
- [Video-effects](Video-effects.md)
--- END FILE ---

--- FILE: Media-reading-and-writing.md ---
# Media reading and writing

**Read images from video, export to alternative formats, and perform sample-level reading and writing of media data.**


## Topics

### Media export

- [Exporting video to alternative formats](https://developer.apple.com/documentation/avfoundation/exporting-video-to-alternative-formats) — Convert an existing movie file to a different format.
- [AVAssetExportSession](https://developer.apple.com/documentation/avfoundation/avassetexportsession) — An object that exports assets in a format that you specify using an export preset.
### Image generation

- [Creating images from a video asset](https://developer.apple.com/documentation/avfoundation/creating-images-from-a-video-asset) — Display images for specific times within the media timeline by generating images from a video’s frames.
- [AVAssetImageGenerator](https://developer.apple.com/documentation/avfoundation/avassetimagegenerator) — An object that generates images from a video asset.
### Media reading

- [Reading multiview 3D video files](https://developer.apple.com/documentation/avfoundation/reading-multiview-3d-video-files) — Render single images for the left eye and right eye from a multiview High Efficiency Video Coding format file by reading individual video frames.
- [AVAssetReader](https://developer.apple.com/documentation/avfoundation/avassetreader) — An object that reads media data from an asset.
- [AVAssetReaderOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput) — An abstract class that defines the interface to read media samples from an asset reader.
- [AVAssetReaderTrackOutput](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput) — An object that reads media data from a single track of an asset.
- [AVAssetReaderAudioMixOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderaudiomixoutput) — An object that reads audio samples that result from mixing audio from one or more tracks.
- [AVAssetReaderVideoCompositionOutput](https://developer.apple.com/documentation/avfoundation/avassetreadervideocompositionoutput) — An object that reads composited video frames from one or more tracks of an asset.
- [AVAssetReaderSampleReferenceOutput](https://developer.apple.com/documentation/avfoundation/avassetreadersamplereferenceoutput) — An object that reads sample references from an asset track.
- [AVAssetReaderOutputMetadataAdaptor](https://developer.apple.com/documentation/avfoundation/avassetreaderoutputmetadataadaptor) — An object that creates timed metadata group objects for an asset track.
### Media writing

- [Converting projected video to Apple Projected Media Profile](https://developer.apple.com/documentation/avfoundation/converting-projected-video-to-apple-projected-media-profile) — Convert content with equirectangular or half-equirectangular projection to APMP.
- [Converting side-by-side 3D video to multiview HEVC and spatial video](https://developer.apple.com/documentation/avfoundation/converting-side-by-side-3d-video-to-multiview-hevc-and-spatial-video) — Create video content for visionOS by converting an existing 3D HEVC file to a multiview HEVC format, optionally adding spatial metadata to create a spatial video.
- [Writing Fragmented MPEG-4 Files for HTTP Live Streaming](https://developer.apple.com/documentation/avfoundation/writing-fragmented-mpeg-4-files-for-http-live-streaming) — Create an HTTP Live Streaming presentation by turning a movie file into a sequence of fragmented MPEG-4 files.
- [Creating spatial photos and videos with spatial metadata](https://developer.apple.com/documentation/ImageIO/Creating-spatial-photos-and-videos-with-spatial-metadata) — Add spatial metadata to stereo photos and videos to create spatial media for viewing on Apple Vision Pro.
- [Tagging Media with Video Color Information](https://developer.apple.com/documentation/avfoundation/tagging-media-with-video-color-information) — Inspect and set video color space information when writing and transcoding media.
- [Evaluating an App’s Video Color](https://developer.apple.com/documentation/avfoundation/evaluating-an-app-s-video-color) — Check color reproduction for a video in your app by using test patterns, video test equipment, and light-measurement instruments.
- [AVOutputSettingsAssistant](https://developer.apple.com/documentation/avfoundation/avoutputsettingsassistant) — An object that builds audio and video output settings dictionaries.
- [AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter) — An object that writes media data to a container file.
- [AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput) — An object that appends media samples to a track in an asset writer’s output file.
- [AVAssetWriterInputPixelBufferAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputpixelbufferadaptor) — An object that appends video samples to an asset writer input.
- [AVAssetWriterInputTaggedPixelBufferGroupAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputtaggedpixelbuffergroupadaptor) — An object that appends tagged buffer groups to an asset writer input.
- [AVAssetWriterInputMetadataAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputmetadataadaptor) — An object that appends timed metadata groups to an asset writer input.
- [AVAssetWriterInputGroup](https://developer.apple.com/documentation/avfoundation/avassetwriterinputgroup) — A group of inputs with tracks that are mutually exclusive to each other for playback or processing.
### Captions

- [Caption Authoring](https://developer.apple.com/documentation/avfoundation/caption-authoring) — Create captions and subtitles in industry-standard formats.

---

*Source: [https://developer.apple.com/documentation/com.apple.avfoundation/documentation/AVFoundation/media-reading-and-writing](https://developer.apple.com/documentation/com.apple.avfoundation/documentation/AVFoundation/media-reading-and-writing)*
--- END FILE ---

--- FILE: Additional-data-capture.md ---
# Additional data capture

**Capture additional data including depth and metadata, and synchronize capture from multiple outputs.**


## Topics

### Depth data capture

- [Capturing Photos with Depth](https://developer.apple.com/documentation/avfoundation/capturing-photos-with-depth) — Get a depth map with a photo to create effects like the system camera’s Portrait mode (on compatible devices).
- [Creating Auxiliary Depth Data Manually](https://developer.apple.com/documentation/avfoundation/creating-auxiliary-depth-data-manually) — Generate a depth image and attach it to your own image.
- [Capturing depth using the LiDAR camera](https://developer.apple.com/documentation/avfoundation/capturing-depth-using-the-lidar-camera) — Access the LiDAR camera on supporting devices to capture precise depth data.
- [AVCamFilter: Applying Filters to a Capture Stream](https://developer.apple.com/documentation/avfoundation/avcamfilter-applying-filters-to-a-capture-stream) — Render a capture stream with rose-colored filtering and depth effects.
- [Streaming Depth Data from the TrueDepth Camera](https://developer.apple.com/documentation/avfoundation/streaming-depth-data-from-the-truedepth-camera) — Visualize depth data in 2D and 3D from the TrueDepth camera.
- [Enhancing Live Video by Leveraging TrueDepth Camera Data](https://developer.apple.com/documentation/avfoundation/enhancing-live-video-by-leveraging-truedepth-camera-data) — Apply your own background to a live capture feed streamed from the front-facing TrueDepth camera.
- [AVCaptureDepthDataOutput](https://developer.apple.com/documentation/avfoundation/avcapturedepthdataoutput) — A capture output that records scene depth information on compatible camera devices.
- [AVDepthData](https://developer.apple.com/documentation/avfoundation/avdepthdata) — A container for per-pixel distance or disparity information captured by compatible camera devices.
- [AVCameraCalibrationData](https://developer.apple.com/documentation/avfoundation/avcameracalibrationdata) — Information about the camera characteristics used to capture images and depth data.
### Metadata capture

- [AVCaptureMetadataInput](https://developer.apple.com/documentation/avfoundation/avcapturemetadatainput) — A capture input for providing timed metadata to a capture session.
- [AVCaptureMetadataOutput](https://developer.apple.com/documentation/avfoundation/avcapturemetadataoutput) — A capture output for processing timed metadata produced by a capture session.
- [AVMetadataObject](https://developer.apple.com/documentation/avfoundation/avmetadataobject) — The abstract superclass for objects provided by a metadata capture output.
- [Metadata Types](https://developer.apple.com/documentation/avfoundation/metadata-types) — Inspect the supported metadata object types that the framework supports.
### Synchronized capture

- [AVCaptureDataOutputSynchronizer](https://developer.apple.com/documentation/avfoundation/avcapturedataoutputsynchronizer) — An object that coordinates time-matched delivery of data from multiple capture outputs.
- [AVCaptureSynchronizedDataCollection](https://developer.apple.com/documentation/avfoundation/avcapturesynchronizeddatacollection) — A set of data samples collected simultaneously from multiple capture outputs.
- [AVCaptureSynchronizedSampleBufferData](https://developer.apple.com/documentation/avfoundation/avcapturesynchronizedsamplebufferdata) — A container for video or audio samples collected using synchronized capture.
- [AVCaptureSynchronizedMetadataObjectData](https://developer.apple.com/documentation/avfoundation/avcapturesynchronizedmetadataobjectdata) — A container for metadata objects collected using synchronized capture.
- [AVCaptureSynchronizedDepthData](https://developer.apple.com/documentation/avfoundation/avcapturesynchronizeddepthdata) — A container for scene depth information collected using synchronized capture.
- [AVCaptureSynchronizedData](https://developer.apple.com/documentation/avfoundation/avcapturesynchronizeddata) — The abstract superclass for media samples collected using synchronized capture.

---

*Source: [https://developer.apple.com/documentation/com.apple.avfoundation/documentation/AVFoundation/additional-data-capture](https://developer.apple.com/documentation/com.apple.avfoundation/documentation/AVFoundation/additional-data-capture)*
--- END FILE ---

--- FILE: README.md ---
# Apple Docs Naturallanguage Documentation

This directory contains Apple Developer Documentation for the **NATURALLANGUAGE** framework.

## Summary

- **Total pages**: 31
- **Successfully downloaded**: 31
- **Failed**: 0

## Files

- [Creating-a-text-classifier-model](Creating-a-text-classifier-model.md)
- [Creating-a-word-tagger-model](Creating-a-word-tagger-model.md)
- [Finding-similarities-between-pieces-of-text](Finding-similarities-between-pieces-of-text.md)
- [Identifying-parts-of-speech](Identifying-parts-of-speech.md)
- [Identifying-people--places--and-organizations](Identifying-people--places--and-organizations.md)
- [Identifying-the-language-in-text](Identifying-the-language-in-text.md)
- [NLContextualEmbedding](NLContextualEmbedding.md)
- [NLContextualEmbeddingKey](NLContextualEmbeddingKey.md)
- [NLEmbedding](NLEmbedding.md)
- [NLLanguage](NLLanguage.md)
- [NLLanguageRecognizer](NLLanguageRecognizer.md)
- [NLModel](NLModel.md)
- [NLScript](NLScript.md)
- [NLTagger](NLTagger.md)
- [NLTokenizer](NLTokenizer.md)
- [Natural-Language](Natural-Language.md)
- [Technologies](Technologies.md)
- [Tokenizing-natural-language-text](Tokenizing-natural-language-text.md)
--- END FILE ---

--- FILE: generative-ai.md ---
---
title: Generative AI | Apple Developer Documentation
source_url: https://developer.apple.com/design/human-interface-guidelines/generative-ai
scraped_date: '2025-10-25T18:04:03.310799Z'
extraction_method: Chrome DevTools MCP Server
content_length: 15601
filename: generative-ai.md
---

# Generative AI | Apple Developer Documentation
--- END FILE ---

--- FILE: accessibility.md ---
---
title: Accessibility | Apple Developer Documentation
source_url: https://developer.apple.com/design/human-interface-guidelines/accessibility
scraped_date: '2025-10-25T18:04:03.284807Z'
extraction_method: Chrome DevTools MCP Server
content_length: 19147
filename: accessibility.md
---

# Accessibility | Apple Developer Documentation

Accessibility
Accessible user interfaces empower everyone to have a great experience with your app or game.

When you design for accessibility, you reach a larger audience and create a more inclusive experience. An accessible interface allows people to experience your app or game regardless of their capabilities or how they use their devices. Accessibility makes information and interactions available to everyone. An accessible interface is:

Intuitive. Your interface uses familiar and consistent interactions that make tasks straightforward to perform.

Perceivable. Your interface doesn't rely on any single method to convey information. People can access and interact with your content, whether they use sight, hearing, speech, or touch.

Adaptable. Your interface adapts to how people want to use their device, whether by supporting system accessibility features or letting people personalize settings.

As you design your app, audit the accessibility of your interface. Use Accessibility Inspector to highlight accessibility issues with your interface and to understand how your app represents itself to people using system accessibility features. You can also communicate how accessible your app is on the App Store using Accessibility Nutrition Labels. To learn more about how to evaluate and indicate accessibility feature support, see Accessibility Nutrition Labels in App Store Connect help.

Vision

The people who use your interface may be blind, color blind, or have low vision or light sensitivity. They may also be in situations where lighting conditions and screen brightness affect their ability to interact with your interface.

Support larger text sizes. Make sure people can adjust the size of your text or icons to make them more legible, visible, and comfortable to read. Ideally, give people the option to enlarge text by at least 200 percent (or 140 percent in watchOS apps). Your interface can support font size enlargement either through custom UI, or by adopting Dynamic Type. Dynamic Type is a systemwide setting that lets people adjust the size of text for comfort and legibility. For more guidance, see Supporting Dynamic Type.

Use recommended defaults for custom type sizes. Each platform has different default and minimum sizes for system-defined type styles to promote readability. If you're using custom type styles, follow the recommended defaults.

Platform

	

Default size

	

Minimum size




iOS, iPadOS

	

17 pt

	

11 pt




macOS

	

13 pt

	

10 pt




tvOS

	

29 pt

	

23 pt




visionOS

	

17 pt

	

12 pt




watchOS

	

16 pt

	

12 pt

Bear in mind that font weight can also impact how easy text is to read. If you're using a custom font with a thin weight, aim for larger than the recommended sizes to increase legibility. For more guidance, see Typography.

Thicker weights are easier to read for smaller font sizes.

Consider increasing the font size when using a thin weight.

Strive to meet color contrast minimum standards. To ensure all information in your app is legible, it's important that there's enough contrast between foreground text and icons and background colors. Two popular standards of measure for color contrast are the Web Content Accessibility Guidelines (WCAG) and the Accessible Perceptual Contrast Algorithm (APCA). Use standard contrast calculators to ensure your UI meets acceptable levels. Accessibility Inspector uses the following values from WCAG Level AA as guidance in determining whether your app's colors have an acceptable contrast.

Text size

	

Text weight

	

Minimum contrast ratio




Up to 17 pts

	

All

	

4.5:1




18 pts

	

All

	

3:1




All

	

Bold

	

3:1

If your app doesn't provide this minimum contrast by default, ensure it at least provides a higher contrast color scheme when the system setting Increase Contrast is turned on. If your app supports Dark Mode, make sure to check the minimum contrast in both light and dark appearances.

A button with insufficient color contrast

A button with sufficient color contrast

Prefer system-defined colors. These colors have their own accessible variants that automatically adapt when people adjust their color preferences, such as enabling Increase Contrast or toggling between the light and dark appearances. For guidance, see Color.

The systemRed default color in iOS

The systemRed accessible color in iOS

Convey information with more than color alone. Some people have trouble differentiating between certain colors and shades. For example, people who are color blind may have particular difficulty with pairings such as red-green and blue-orange. Offer visual indicators, like distinct shapes or icons, in addition to color to help people perceive differences in function and changes in state. Consider allowing people to customize color schemes such as chart colors or game characters so they can personalize your interface in a way that's comfortable for them.

For someone with red-green color blindness, these indicators might appear the same.

Both visual indicators and color help differentiate between indicators.

Describe your app's interface and content for VoiceOver. VoiceOver is a screen reader that lets people experience your app's interface without needing to see the screen. For more guidance, see VoiceOver.

Hearing

The people who use your interface may be deaf or hard of hearing. They may also be in noisy or public environments.

Support text-based ways to enjoy audio and video. It's important that dialogue and crucial information about your app or game isn't communicated through audio alone. Depending on the context, give people different text-based ways to experience their media, and allow people to customize the visual presentation of that text:

Captions give people the textual equivalent of audible information in video or audio-only content. Captions are great for scenarios like game cutscenes and video clips where text synchronizes live with the media.

Subtitles allow people to read live onscreen dialogue in their preferred language. Subtitles are great for TV shows and movies.

Audio descriptions are interspersed between natural pauses in the main audio of a video and supply spoken narration of important information that's presented only visually.

Transcripts provide a complete textual description of a video, covering both audible and visual information. Transcripts are great for longer-form media like podcasts and audiobooks where people may want to review content as a whole or highlight the transcript as media is playing.

For developer guidance, see Selecting Subtitles and Alternative Audio Tracks.

Use haptics in addition to audio cues. If your interface conveys information through audio cues — such as a success chime, error sound, or game feedback — consider pairing that sound with matching haptics for people who can't perceive the audio or have their audio turned off. In iOS and iPadOS, you can also use Music Haptics and Audio graphs to let people experience music and infographics through vibration and texture. For guidance, see Playing haptics.

Augment audio cues with visual cues. This is especially important for games and spatial apps where important content might be taking place off screen. When using audio to guide people towards a specific action, also add in visual indicators that point to where you want people to interact.

Mobility

Ensure your interface offers a comfortable experience for people with limited dexterity or mobility.

Offer sufficiently sized controls. Controls that are too small are hard for many people to interact with and select. Strive to meet the recommended minimum control size for each platform to ensure controls and menus are comfortable for all when tapping and clicking.

Platform

	

Default control size

	

Minimum control size




iOS, iPadOS

	

44x44 pt

	

28x28 pt




macOS

	

28x28 pt

	

20x20 pt




tvOS

	

66x66 pt

	

56x56 pt




visionOS

	

60x60 pt

	

28x28 pt




watchOS

	

44x44 pt

	

28x28 pt

Consider spacing between controls as important as size. Include enough padding between elements to reduce the chance that someone taps the wrong control. In general, it works well to add about 12 points of padding around elements that include a bezel. For elements without a bezel, about 24 points of padding works well around the element's visible edges.

Elements with insufficient padding

Elements with sufficient padding

Support simple gestures for common interactions. For many people, with or without disabilities, complex gestures can be challenging. For interactions people do frequently in your app or game, use the simplest gesture possible — avoid custom multifinger and multihand gestures — so repetitive actions are both comfortable and easy to remember.

Offer alternatives to gestures. Make sure your UI's core functionality is accessible through more than one type of physical interaction. Gestures can be less comfortable for people who have limited dexterity, so offer onscreen ways to achieve the same outcome. For example, if you use a swipe gesture to dismiss a view, also make a button available so people can tap or use an assistive device.

Edit and tap to delete

Swipe to delete

Let people use Voice Control to give guidance and enter information verbally. With Voice Control, people can interact with their devices entirely by speaking commands. They can perform gestures, interact with screen elements, dictate and edit text, and more. To ensure a smooth experience, label interface elements appropriately. For developer guidance, see Voice Control.

Integrate with Siri and Shortcuts to let people perform tasks using voice alone. When your app supports Siri and Shortcuts, people can automate the important and repetitive tasks they perform regularly. They can initiate these tasks from Siri, the Action button on their iPhone or Apple Watch, and shortcuts on their Home Screen or in Control Center. For guidance, see Siri.

Support mobility-related assistive technologies. Features like VoiceOver, AssistiveTouch, Full Keyboard Access, Pointer Control, and Switch Control offer alternative ways for people with low mobility to interact with their devices. Conduct testing and verify that your app or game supports these technologies, and that your interface elements are appropriately labeled to ensure a great experience. For more information, see Performing accessibility testing for your app.

Speech

Apple's accessibility features help people with speech disabilities and people who prefer text-based interactions to communicate effectively using their devices.

Let people use the keyboard alone to navigate and interact with your app. People can turn on Full Keyboard Access to navigate apps using their physical keyboard. The system also defines accessibility keyboard shortcuts and a wide range of other keyboard shortcuts that many people use all the time. Avoid overriding system-defined keyboard shortcuts and evaluate your app to ensure it works well with Full Keyboard Access. For additional guidance, see Keyboards. For developer guidance, see Support Full Keyboard Access in your iOS app.

Support Switch Control. Switch Control is an assistive technology that lets people control their devices through separate hardware, game controllers, or sounds such as a click or a pop. People can perform actions like selecting, tapping, typing, and drawing when your app or game supports the ability to navigate using Switch Control. For developer guidance, see Switch Control.

Cognitive

When you minimize complexity in your app or game, all people benefit.

Keep actions simple and intuitive. Ensure that people can navigate your interface using easy-to-remember and consistent interactions. Prefer system gestures and behaviors people are already familiar with over creating custom gestures people must learn and retain.

Minimize use of time-boxed interface elements. Views and controls that auto-dismiss on a timer can be problematic for people who need longer to process information, and for people who use assistive technologies that require more time to traverse the interface. Prefer dismissing views with an explicit action.

Consider offering difficulty accommodations in games. Everyone has their own way of playing and enjoying games. To support a variety of cognitive abilities, consider adding the ability to customize the difficulty level of your game, such as offering options for people to reduce the criteria for successfully completing a level, adjust reaction time, or enable control assistance.

Let people control audio and video playback. Avoid autoplaying audio and video content without also providing controls to start and stop it. Make sure these controls are discoverable and easy to act upon, and consider global settings that let people opt out of auto-playing all audio and video. For developer guidance, see Animated images and isVideoAutoplayEnabled.

Allow people to opt out of flashing lights in video playback. People might want to avoid bright, frequent flashes of light in the media they consume. A Dim Flashing Lights setting allows the system to calculate, mitigate, and inform people about flashing lights in a piece of media. If your app supports video playback, ensure that it responds appropriately to the Dim Flashing Lights setting. For developer guidance, see Flashing lights.

Be cautious with fast-moving and blinking animations. When you use these effects in excess, it can be distracting, cause dizziness, and in some cases even result in epileptic episodes. People who are prone to these effects can turn on the Reduce Motion accessibility setting. When this setting is active, ensure your app or game responds by reducing automatic and repetitive animations, including zooming, scaling, and peripheral motion. Other best practices for reducing motion include:

Tightening animation springs to reduce bounce effects

Tracking animations directly with people's gestures

Avoiding animating depth changes in z-axis layers

Replacing transitions in x-, y-, and z-axes with fades to avoid motion

Avoiding animating into and out of blurs

Optimize your app's UI for Assistive Access. Assistive Access is an accessibility feature in iOS and iPadOS that allows people with cognitive disabilities to use a streamlined version of your app. Assistive Access sets a default layout and control presentation for apps that reduces cognitive load, such as the following layout of the Camera app.

To optimize your app for this mode, use the following guidelines when Assistive Access is turned on:

Identify the core functionality of your app and consider removing noncritical workflows and UI elements.

Break up multistep workflows so people can focus on a single interaction per screen.

Always ask for confirmation twice whenever people perform an action that's difficult to recover from, such a deleting a file.

For developer guidance, see Assistive Access.

Platform considerations

No additional considerations for iOS, iPadOS, macOS, tvOS, or watchOS.

visionOS

visionOS offers a variety of accessibility features people can use to interact with their surroundings in ways that are comfortable and work best for them, including head and hand Pointer Control, and a Zoom feature.

Pointer Control (hand)

Pointer Control (head)

Zoom

Prioritize comfort. The immersive nature of visionOS means that interfaces, animations, and interactions have a greater chance of causing motion sickness, and visual and ergonomic discomfort for people. To ensure the most comfortable experience, consider these tips:

Keep interface elements within a person's field of view. Prefer horizontal layouts to vertical ones that might cause neck strain, and avoid demanding the viewer's attention in different locations in quick succession.

Reduce the speed and intensity of animated objects, particularly in someone's peripheral vision.

Be gentle with camera and video motion, and avoid situations where someone may feel like the world around them is moving without their control.

Avoid anchoring content to the wearer's head, which may make them feel stuck and confined, and also prevent them from using assistive technologies like Pointer Control.

Minimize the need for large and repetitive gestures, as these can become tiresome and may be difficult depending on a person's surroundings.

For additional guidance, see Create accessible spatial experiences and Design considerations for vision and motion.

Resources

Related

Inclusion

Typography

VoiceOver

Developer documentation

Building accessible apps

Accessibility framework

Overview of Accessibility Nutrition Labels

Videos

Principles of inclusive app design

Evaluate your app for Accessibility Nutrition Labels

Catch up on accessibility in SwiftUI

Change log

Date

	

Changes




June 9, 2025

	

Added guidance and links for Assistive Access, Switch Control, and Accessibility Nutrition Labels.




March 7, 2025

	

Expanded and refined all guidance. Moved Dynamic Type guidance to the Typography page, and moved VoiceOver guidance to a new VoiceOver page.




June 10, 2024

	

Added a link to Apple's Unity plug-ins for supporting Dynamic Type.




December 5, 2023

	

Updated visionOS Zoom lens artwork.




June 21, 2023

	

Updated to include guidance for visionOS.

Current page is Accessibility

## Detailed Sections

### Quick Links

DownloadsDocumentationSample CodeVideosForums
5 Quick Links


## Vision

The people who use your interface may be blind, color blind, or have low vision or light sensitivity. They may also be in situations where lighting conditions and screen brightness affect their ability to interact with your interface.
Support larger text sizes. Make sure people can adjust the size of your text or icons to make them more legible, visible, and comfortable to read. Ideally, give people the option to enlarge text by at least 200 percent (or 140 percent in watchOS apps). Your interface can support font size enlargement either through custom UI, or by adopting Dynamic Type. Dynamic Type is a systemwide setting that lets people adjust the size of text for comfort and legibility. For more guidance, see Supporting Dynamic Type.
Use recommended defaults for custom type sizes. Each platform has different default and minimum sizes for system-defined type styles to promote readability. If you're using custom type styles, follow the recommended defaults.
PlatformDefault sizeMinimum sizeiOS, iPadOS17 pt11 ptmacOS13 pt10 pttvOS29 pt23 ptvisionOS17 pt12 ptwatchOS16 pt12 pt
Bear in mind that font weight can also impact how easy text is to read. If you're using a custom font with a thin weight, aim for larger than the recommended sizes to increase legibility. For more guidance, see Typography.
Thicker weights are easier to read for smaller font sizes.Consider increasing the font size when using a thin weight.
Strive to meet color contrast minimum standards. To ensure all information in your app is legible, it's important that there's enough contrast between foreground text and icons and background colors. Two popular standards of measure for color contrast are the Web Content Accessibility Guidelines (WCAG) and the Accessible Perceptual Contrast Algorithm (APCA). Use standard contrast calculators to ensure your UI meets acceptable levels. Accessibility Inspector uses the following values from WCAG Level AA as guidance in determining whether your app's colors have an acceptable contrast.
Text sizeText weightMinimum contrast ratioUp to 17 ptsAll4.5:118 ptsAll3:1AllBold3:1
If your app doesn't provide this minimum contrast by default, ensure it at least provides a higher contrast color scheme when the system setting Increase Contrast is turned on. If your app supports Dark Mode, make sure to check the minimum contrast in both light and dark appearances.
A button with insufficient color contrastA button with sufficient color contrast
Prefer system-defined colors. These colors have their own accessible variants that automatically adapt when people adjust their color preferences, such as enabling Increase Contrast or toggling between the light and dark appearances. For guidance, see Color.
The systemRed default color in iOSThe systemRed accessible color in iOS
Convey information with more than color alone. Some people have trouble differentiating between certain colors and shades. For example, people who are color blind may have particular difficulty with pairings such as red-green and blue-orange. Offer visual indicators, like distinct shapes or icons, in addition to color to help people perceive differences in function and changes in state. Consider allowing people to customize color schemes such as chart colors or game characters so they can personalize your interface in a way that's comfortable for them.
For someone with red-green color blindness, these indicators might appear the same.Both visual indicators and color help differentiate between indicators.
Describe your app's interface and content for VoiceOver. VoiceOver is a screen reader that lets people experience your app's interface without needing to see the screen. For more guidance, see VoiceOver.


## Hearing

The people who use your interface may be deaf or hard of hearing. They may also be in noisy or public environments.
Support text-based ways to enjoy audio and video. It's important that dialogue and crucial information about your app or game isn't communicated through audio alone. Depending on the context, give people different text-based ways to experience their media, and allow people to customize the visual presentation of that text:
Captions give people the textual equivalent of audible information in video or audio-only content. Captions are great for scenarios like game cutscenes and video clips where text synchronizes live with the media.Subtitles allow people to read live onscreen dialogue in their preferred language. Subtitles are great for TV shows and movies.Audio descriptions are interspersed between natural pauses in the main audio of a video and supply spoken narration of important information that's presented only visually.Transcripts provide a complete textual description of a video, covering both audible and visual information. Transcripts are great for longer-form media like podcasts and audiobooks where people may want to review content as a whole or highlight the transcript as media is playing.
For developer guidance, see Selecting Subtitles and Alternative Audio Tracks.
Use haptics in addition to audio cues. If your interface conveys information through audio cues — such as a success chime, error sound, or game feedback — consider pairing that sound with matching haptics for people who can't perceive the audio or have their audio turned off. In iOS and iPadOS, you can also use Music Haptics and Audio graphs to let people experience music and infographics through vibration and texture. For guidance, see Playing haptics.
Augment audio cues with visual cues. This is especially important for games and spatial apps where important content might be taking place off screen. When using audio to guide people towards a specific action, also add in visual indicators that point to where you want people to interact.


## Mobility

Ensure your interface offers a comfortable experience for people with limited dexterity or mobility.
Offer sufficiently sized controls. Controls that are too small are hard for many people to interact with and select. Strive to meet the recommended minimum control size for each platform to ensure controls and menus are comfortable for all when tapping and clicking.
PlatformDefault control sizeMinimum control sizeiOS, iPadOS44x44 pt28x28 ptmacOS28x28 pt20x20 pttvOS66x66 pt56x56 ptvisionOS60x60 pt28x28 ptwatchOS44x44 pt28x28 pt
Consider spacing between controls as important as size. Include enough padding between elements to reduce the chance that someone taps the wrong control. In general, it works well to add about 12 points of padding around elements that include a bezel. For elements without a bezel, about 24 points of padding works well around the element's visible edges.
Elements with insufficient paddingElements with sufficient padding
Support simple gestures for common interactions. For many people, with or without disabilities, complex gestures can be challenging. For interactions people do frequently in your app or game, use the simplest gesture possible — avoid custom multifinger and multihand gestures — so repetitive actions are both comfortable and easy to remember.
Offer alternatives to gestures. Make sure your UI's core functionality is accessible through more than one type of physical interaction. Gestures can be less comfortable for people who have limited dexterity, so offer onscreen ways to achieve the same outcome. For example, if you use a swipe gesture to dismiss a view, also make a button available so people can tap or use an assistive device.
Edit and tap to deleteSwipe to delete
Let people use Voice Control to give guidance and enter information verbally. With Voice Control, people can interact with their devices entirely by speaking commands. They can perform gestures, interact with screen elements, dictate and edit text, and more. To ensure a smooth experience, label interface elements appropriately. For developer guidance, see Voice Control.
Integrate with Siri and Shortcuts to let people perform tasks using voice alone. When your app supports Siri and Shortcuts, people can automate the important and repetitive tasks they perform regularly. They can initiate these tasks from Siri, the Action button on their iPhone or Apple Watch, and shortcuts on their Home Screen or in Control Center. For guidance, see Siri.
Support mobility-related assistive technologies. Features like VoiceOver, AssistiveTouch, Full Keyboard Access, Pointer Control, and Switch Control offer alternative ways for people with low mobility to interact with their devices. Conduct testing and verify that your app or game supports these technologies, and that your interface elements are appropriately labeled to ensure a great experience. For more information, see Performing accessibility testing for your app.


## Speech

Apple's accessibility features help people with speech disabilities and people who prefer text-based interactions to communicate effectively using their devices.
Let people use the keyboard alone to navigate and interact with your app. People can turn on Full Keyboard Access to navigate apps using their physical keyboard. The system also defines accessibility keyboard shortcuts and a wide range of other keyboard shortcuts that many people use all the time. Avoid overriding system-defined keyboard shortcuts and evaluate your app to ensure it works well with Full Keyboard Access. For additional guidance, see Keyboards. For developer guidance, see Support Full Keyboard Access in your iOS app.
Support Switch Control. Switch Control is an assistive technology that lets people control their devices through separate hardware, game controllers, or sounds such as a click or a pop. People can perform actions like selecting, tapping, typing, and drawing when your app or game supports the ability to navigate using Switch Control. For developer guidance, see Switch Control.


## Cognitive

When you minimize complexity in your app or game, all people benefit.
Keep actions simple and intuitive. Ensure that people can navigate your interface using easy-to-remember and consistent interactions. Prefer system gestures and behaviors people are already familiar with over creating custom gestures people must learn and retain.
Minimize use of time-boxed interface elements. Views and controls that auto-dismiss on a timer can be problematic for people who need longer to process information, and for people who use assistive technologies that require more time to traverse the interface. Prefer dismissing views with an explicit action.
Consider offering difficulty accommodations in games. Everyone has their own way of playing and enjoying games. To support a variety of cognitive abilities, consider adding the ability to customize the difficulty level of your game, such as offering options for people to reduce the criteria for successfully completing a level, adjust reaction time, or enable control assistance.
Let people control audio and video playback. Avoid autoplaying audio and video content without also providing controls to start and stop it. Make sure these controls are discoverable and easy to act upon, and consider global settings that let people opt out of auto-playing all audio and video. For developer guidance, see Animated images and isVideoAutoplayEnabled.
Allow people to opt out of flashing lights in video playback. People might want to avoid bright, frequent flashes of light in the media they consume. A Dim Flashing Lights setting allows the system to calculate, mitigate, and inform people about flashing lights in a piece of media. If your app supports video playback, ensure that it responds appropriately to the Dim Flashing Lights setting. For developer guidance, see Flashing lights.
Be cautious with fast-moving and blinking animations. When you use these effects in excess, it can be distracting, cause dizziness, and in some cases even result in epileptic episodes. People who are prone to these effects can turn on the Reduce Motion accessibility setting. When this setting is active, ensure your app or game responds by reducing automatic and repetitive animations, including zooming, scaling, and peripheral motion. Other best practices for reducing motion include:
Tightening animation springs to reduce bounce effectsTracking animations directly with people's gesturesAvoiding animating depth changes in z-axis layersReplacing transitions in x-, y-, and z-axes with fades to avoid motionAvoiding animating into and out of blurs
Optimize your app's UI for Assistive Access. Assistive Access is an accessibility feature in iOS and iPadOS that allows people with cognitive disabilities to use a streamlined version of your app. Assistive Access sets a default layout and control presentation for apps that reduces cognitive load, such as the following layout of the Camera app.
To optimize your app for this mode, use the following guidelines when Assistive Access is turned on:
Identify the core functionality of your app and consider removing noncritical workflows and UI elements.Break up multistep workflows so people can focus on a single interaction per screen.Always ask for confirmation twice whenever people perform an action that's difficult to recover from, such a deleting a file.
For developer guidance, see Assistive Access.


## Platform considerations

No additional considerations for iOS, iPadOS, macOS, tvOS, or watchOS.


### visionOS

visionOS offers a variety of accessibility features people can use to interact with their surroundings in ways that are comfortable and work best for them, including head and hand Pointer Control, and a Zoom feature.
Pointer Control (hand)  Pointer Control (head)  Zoom  Video with custom controls.  Content description: A recording of a person's hand using Pointer Control to interact with content in an app's visionOS window. A line with a pointer at the end extends from the person's hand. It changes position within the field of view as the person moves their hand.  Play  Video with custom controls.  Content description: A recording of someone using Pointer Control to interact with content in an app's visionOS window. The person isn't visible in the recording. Only the pointer is visible. It's centered in the field of view, and the person uses their head movement to position content beneath the pointer.  Play
Prioritize comfort. The immersive nature of visionOS means that interfaces, animations, and interactions have a greater chance of causing motion sickness, and visual and ergonomic discomfort for people. To ensure the most comfortable experience, consider these tips:
Keep interface elements within a person's field of view. Prefer horizontal layouts to vertical ones that might cause neck strain, and avoid demanding the viewer's attention in different locations in quick succession.Reduce the speed and intensity of animated objects, particularly in someone's peripheral vision.Be gentle with camera and video motion, and avoid situations where someone may feel like the world around them is moving without their control.Avoid anchoring content to the wearer's head, which may make them feel stuck and confined, and also prevent them from using assistive technologies like Pointer Control.Minimize the need for large and repetitive gestures, as these can become tiresome and may be difficult depending on a person's surroundings.
For additional guidance, see Create accessible spatial experiences and Design considerations for vision and motion.


#### Related

Inclusion
Typography
VoiceOver


#### Developer documentation

Building accessible apps
Accessibility framework
Overview of Accessibility Nutrition Labels


#### Videos

Principles of inclusive app design  Evaluate your app for Accessibility Nutrition Labels  Catch up on accessibility in SwiftUI


## Change log

DateChangesJune 9, 2025Added guidance and links for Assistive Access, Switch Control, and Accessibility Nutrition Labels.March 7, 2025Expanded and refined all guidance. Moved Dynamic Type guidance to the Typography page, and moved VoiceOver guidance to a new VoiceOver page.June 10, 2024Added a link to Apple's Unity plug-ins for supporting Dynamic Type.December 5, 2023Updated visionOS Zoom lens artwork.June 21, 2023Updated to include guidance for visionOS.


### Platforms

Toggle Menu
iOSiPadOSmacOStvOSvisionOSwatchOS


### Tools

Toggle Menu
SwiftSwiftUISwift PlaygroundTestFlightXcodeXcode CloudSF Symbols


### Topics & Technologies

Toggle Menu
AccessibilityAccessoriesApp ExtensionApp StoreAudio & VideoAugmented RealityDesignDistributionEducationFontsGamesHealth & FitnessIn-App PurchaseLocalizationMaps & LocationMachine Learning & AIOpen SourceSecuritySafari & Web


### Resources

Toggle Menu
DocumentationTutorialsDownloadsForumsVideos


### Support

Toggle Menu
Support ArticlesContact UsBug ReportingSystem Status


### Account

Toggle Menu
Apple DeveloperApp Store ConnectCertificates, IDs, & ProfilesFeedback Assistant


### Programs

Toggle Menu
Apple Developer ProgramApple Developer Enterprise ProgramApp Store Small Business ProgramMFi ProgramNews Partner ProgramVideo Partner ProgramSecurity Bounty ProgramSecurity Research Device Program


### Events

Toggle Menu
Meet with AppleApple Developer CentersApp Store AwardsApple Design AwardsApple Developer AcademiesWWDC
--- END FILE ---

--- FILE: traits-apple-developer-documentation.md ---
---
title: Traits | Apple Developer Documentation
source_url: https://developer.apple.com/documentation/testing/traits
scraped_date: '2025-10-25T18:04:03.301564Z'
extraction_method: Chrome DevTools MCP Server
content_length: 3943
filename: traits-apple-developer-documentation.md
---

# Traits | Apple Developer Documentation

Swift Testing
Traits
API Collection
Traits
Annotate test functions and suites, and customize their behavior.
Overview

Pass built-in traits to test functions or suite types to comment, categorize, classify, and modify the runtime behavior of test suites and test functions. Implement the TestTrait, and SuiteTrait protocols to create your own types that customize the behavior of your tests.

Topics
Customizing runtime behaviors
Enabling and disabling tests
Conditionally enable or disable individual tests before they run.
Limiting the running time of tests
Set limits on how long a test can run for until it fails.
static func enabled(if: @autoclosure () throws -> Bool, Comment?, sourceLocation: SourceLocation) -> Self
Constructs a condition trait that disables a test if it returns false.
static func enabled(Comment?, sourceLocation: SourceLocation, () async throws -> Bool) -> Self
Constructs a condition trait that disables a test if it returns false.
static func disabled(Comment?, sourceLocation: SourceLocation) -> Self
Constructs a condition trait that disables a test unconditionally.
static func disabled(if: @autoclosure () throws -> Bool, Comment?, sourceLocation: SourceLocation) -> Self
Constructs a condition trait that disables a test if its value is true.
static func disabled(Comment?, sourceLocation: SourceLocation, () async throws -> Bool) -> Self
Constructs a condition trait that disables a test if its value is true.
static func timeLimit(TimeLimitTrait.Duration) -> Self
Construct a time limit trait that causes a test to time out if it runs for too long.
Running tests serially or in parallel
Running tests serially or in parallel
Control whether tests run serially or in parallel.
static var serialized: ParallelizationTrait
A trait that serializes the test to which it is applied.
Annotating tests
Adding tags to tests
Use tags to provide semantic information for organization, filtering, and customizing appearances.
Adding comments to tests
Add comments to provide useful information about tests.
Associating bugs with tests
Associate bugs uncovered or verified by tests.
Interpreting bug identifiers
Examine how the testing library interprets bug identifiers provided by developers.
macro Tag()
Declare a tag that can be applied to a test function or test suite.
static func bug(String, Comment?) -> Self
Constructs a bug to track with a test.
static func bug(String?, id: String, Comment?) -> Self
Constructs a bug to track with a test.
static func bug(String?, id: some Numeric, Comment?) -> Self
Constructs a bug to track with a test.
Handling issues
static func compactMapIssues((Issue) -> Issue?) -> Self
Constructs an trait that transforms issues recorded by a test.
static func filterIssues((Issue) -> Bool) -> Self
Constructs a trait that filters issues recorded by a test.
Creating custom traits
protocol Trait
A protocol describing traits that can be added to a test function or to a test suite.
protocol TestTrait
A protocol describing a trait that you can add to a test function.
protocol SuiteTrait
A protocol describing a trait that you can add to a test suite.
protocol TestScoping
A protocol that tells the test runner to run custom code before or after it runs a test suite or test function.
Supporting types
struct Bug
A type that represents a bug report tracked by a test.
struct Comment
A type that represents a comment related to a test.
struct ConditionTrait
A type that defines a condition which must be satisfied for the testing library to enable a test.
struct IssueHandlingTrait
A type that allows transforming or filtering the issues recorded by a test.
struct ParallelizationTrait
A type that defines whether the testing library runs this test serially or in parallel.
struct Tag
A type representing a tag that can be applied to a test.
struct List
A type representing one or more tags applied to a test.
struct TimeLimitTrait
A type that defines a time limit to apply to a test.
--- END FILE ---

--- FILE: loading.md ---
---
title: Loading | Apple Developer Documentation
source_url: https://developer.apple.com/design/human-interface-guidelines/loading
scraped_date: '2025-10-25T18:04:03.305794Z'
extraction_method: Chrome DevTools MCP Server
content_length: 3251
filename: loading.md
---

# Loading | Apple Developer Documentation

Loading
The best content-loading experience finishes before people become aware of it.

If your app or game loads assets, levels, or other content, design the behavior so it doesn't disrupt or negatively impact the user experience.

Best practices

Show something as soon as possible. If you make people wait for loading to complete before displaying anything, they can interpret the lack of content as a problem with your app or game. Instead, consider showing placeholder text, graphics, or animations as content loads, replacing these elements as content becomes available.

Let people do other things in your app or game while they wait for content to load. Loading content in the background helps give people access to other actions. For example, a game could load content in the background while players learn about the next level or view an in-game menu. For developer guidance, see Improving the player experience for games with large downloads.

If loading takes an unavoidably long time, give people something interesting to view while they wait. For example, you might provide gameplay hints, display tips, or introduce people to new features. Gauge the remaining loading time as accurately as possible to help you avoid giving people too little time to enjoy your placeholder content or having so much time that you need to repeat it.

Improve installation and launch time by downloading large assets in the background. Consider using the Background Assets framework to schedule asset downloads — like game level packs, 3D character models, and textures — to occur immediately after installation, during updates, or at other nondisruptive times.

Showing progress

Clearly communicate that content is loading and how long it might take to complete. Ideally, content displays instantly, but for situations where loading takes more than a moment or two, you can use system-provided components — called progress indicators — to show that loading is ongoing. In general, you use a determinate progress indicator when you know how long loading will take, and you use an indeterminate progress indicator when you don't. For guidance, see Progress indicators.

For games, consider creating a custom loading view. Standard progress indicators work well in most apps, but can sometimes feel out of place in a game. Consider designing a more engaging experience by using custom animations and elements that match the style of your game.

Platform considerations

No additional considerations for iOS, iPadOS, macOS, tvOS, or visionOS.

watchOS

As much as possible, avoid showing a loading indicator in your watchOS experience. People expect quick interactions with their Apple Watch, so aim to display content immediately. In situations where content needs a second or two to load, it's better to display a loading indicator than a blank screen.

Resources
Related

Launching

Progress indicators

Developer documentation

Background Assets

Videos
Discover Apple-Hosted Background Assets
Change log

Date

	

Changes




June 9, 2025

	

Revised guidance for storing downloads to reflect downloading large assets in the background.




June 10, 2024

	

Added guidelines for showing progress and storing downloads, and enhanced guidance for games.
--- END FILE ---

--- FILE: CB-CM-API-AVAILABLE.md ---
# CB_CM_API_AVAILABLE

## Availability

- **iOS** 5.0+
- **iPadOS** 5.0+
- **Mac Catalyst** 13.0+
- **macOS** 10.10+
- **tvOS** 9.0+
- **visionOS** 1.0+
- **watchOS** 4.0+


---

*Source: [https://developer.apple.com/documentation/com.apple.corebluetooth/documentation/CoreBluetooth/CB_CM_API_AVAILABLE](https://developer.apple.com/documentation/com.apple.corebluetooth/documentation/CoreBluetooth/CB_CM_API_AVAILABLE)*
--- END FILE ---

--- FILE: README.md ---
# Apple Docs Corebluetooth Documentation

This directory contains Apple Developer Documentation for the **COREBLUETOOTH** framework.

## Summary

- **Total pages**: 26
- **Successfully downloaded**: 26
- **Failed**: 0

## Files

- [CB-CM-API-AVAILABLE](CB-CM-API-AVAILABLE.md)
- [CBATTRequest](CBATTRequest.md)
- [CBAttribute](CBAttribute.md)
- [CBCentral](CBCentral.md)
- [CBCentralManager](CBCentralManager.md)
- [CBCentralManagerDelegate](CBCentralManagerDelegate.md)
- [CBCentralManagerState](CBCentralManagerState.md)
- [CBCharacteristic](CBCharacteristic.md)
- [CBDescriptor](CBDescriptor.md)
- [CBManager](CBManager.md)
- [CBMutableDescriptor](CBMutableDescriptor.md)
- [CBMutableService](CBMutableService.md)
- [CBPeripheral](CBPeripheral.md)
- [CBPeripheralManager](CBPeripheralManager.md)
- [CBPeripheralManagerState](CBPeripheralManagerState.md)
- [CBService](CBService.md)
- [CBUUID](CBUUID.md)
- [CBUUIDCharacteristicObservationScheduleString](CBUUIDCharacteristicObservationScheduleString.md)
- [Core-Bluetooth](Core-Bluetooth.md)
- [Deprecated-Constants](Deprecated-Constants.md)
- [NSBluetoothAlwaysUsageDescription](NSBluetoothAlwaysUsageDescription.md)
- [Using-Core-Bluetooth-Classic](Using-Core-Bluetooth-Classic.md)
--- END FILE ---

--- FILE: CMHeadphoneActivityManager.md ---
# CMHeadphoneActivityManager

**An object that starts and manages headphone activity services.**

## Availability

- **iOS** 18.0+
- **iPadOS** 18.0+
- **Mac Catalyst** 18.0+
- **macOS** 15.0+
- **watchOS** 11.0+


## Overview

This class delivers headphone activity updates to your app. Use an instance of the manager to determine if the device supports headphone activity updates, and to start and stop updates. Before using this class, check [doc://com.apple.coremotion/documentation/CoreMotion/CMHeadphoneActivityManager/isActivityAvailable](https://developer.apple.com/documentation/CoreMotion/CMHeadphoneActivityManager/isActivityAvailable) and [doc://com.apple.coremotion/documentation/CoreMotion/CMHeadphoneActivityManager/isStatusAvailable](https://developer.apple.com/documentation/CoreMotion/CMHeadphoneActivityManager/isStatusAvailable) to make sure the features are available.

This class provides similar information to [doc://com.apple.coremotion/documentation/CoreMotion/CMMotionActivityManager](https://developer.apple.com/documentation/CoreMotion/CMMotionActivityManager), except the activity information comes from headphone motion, rather than from device motion.


> **IMPORTANT**:  In iOS and macOS, include the [doc://com.apple.documentation/documentation/BundleResources/Information-Property-List/NSMotionUsageDescription](https://developer.apple.com/documentation/BundleResources/Information-Property-List/NSMotionUsageDescription) key in your app’s `Info.plist` file. If this key is absent, trying to start headphone activity updates terminates your app.


## Topics

### Checking Availability

- [isActivityAvailable](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/isactivityavailable) — A Boolean value that indicates whether the current device supports headphone activity.
- [isActivityActive](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/isactivityactive) — A Boolean value that indicates whether headphone motion activity is active.
- [isStatusAvailable](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/isstatusavailable) — A Boolean value that indicates whether the current device supports headphone status.
- [isStatusActive](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/isstatusactive) — A Boolean value that indicates whether headphone status is active.
- [authorizationStatus()](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/authorizationstatus()) — Returns the authorization status for monitoring headphone activity.
### Starting and Stopping Updates

- [startActivityUpdates(to:withHandler:)](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/startactivityupdates(to:withhandler:)) — Starts headphone activity updates, providing data to the given handler through the given queue.
- [stopActivityUpdates()](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/stopactivityupdates()) — Stops headphone activity updates.
- [startStatusUpdates(to:withHandler:)](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/startstatusupdates(to:withhandler:)) — Starts headphone status updates, providing data to the given handler through the given queue.
- [stopStatusUpdates()](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/stopstatusupdates()) — Stops headphone status updates.
### Supporting Types

- [CMHeadphoneActivityManager.Status](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/status) — Headphone connection status updates.
- [CMHeadphoneActivityManager.ActivityHandler](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/activityhandler) — The type for a handler to be invoked when headphone motion activity data is available.
- [CMHeadphoneActivityManager.StatusHandler](https://developer.apple.com/documentation/coremotion/cmheadphoneactivitymanager/statushandler) — The type for a handler to be invoked with status updates.

---

*Source: [https://developer.apple.com/documentation/com.apple.coremotion/documentation/CoreMotion/CMHeadphoneActivityManager](https://developer.apple.com/documentation/com.apple.coremotion/documentation/CoreMotion/CMHeadphoneActivityManager)*
--- END FILE ---

--- FILE: README.md ---
# Apple Docs Coremotion Documentation

This directory contains Apple Developer Documentation for the **COREMOTION** framework.

## Summary

- **Total pages**: 26
- **Successfully downloaded**: 26
- **Failed**: 0

## Files

- [CMAttitudeReferenceFrame](CMAttitudeReferenceFrame.md)
- [CMDeviceMotion](CMDeviceMotion.md)
- [CMDyskineticSymptomResult](CMDyskineticSymptomResult.md)
- [CMFallDetectionManager](CMFallDetectionManager.md)
- [CMHeadphoneActivityManager](CMHeadphoneActivityManager.md)
- [CMHighFrequencyHeartRateData](CMHighFrequencyHeartRateData.md)
- [CMMotionActivity](CMMotionActivity.md)
- [CMMotionActivityManager](CMMotionActivityManager.md)
- [CMMovementDisorderManager](CMMovementDisorderManager.md)
- [CMOdometerData](CMOdometerData.md)
- [CMPedometerData](CMPedometerData.md)
- [CMWaterSubmersionEvent](CMWaterSubmersionEvent.md)
- [CMWaterSubmersionManager](CMWaterSubmersionManager.md)
- [CMWaterSubmersionManagerDelegate](CMWaterSubmersionManagerDelegate.md)
- [CMWaterSubmersionMeasurement](CMWaterSubmersionMeasurement.md)
- [CMWaterTemperature](CMWaterTemperature.md)
- [Core-Motion](Core-Motion.md)
- [Getting-movement-disorder-symptom-data](Getting-movement-disorder-symptom-data.md)
- [Getting-processed-device-motion-data](Getting-processed-device-motion-data.md)
- [Getting-raw-accelerometer-events](Getting-raw-accelerometer-events.md)
- [Movement-disorder-algorithm-changelog](Movement-disorder-algorithm-changelog.md)
- [NSMotionUsageDescription](NSMotionUsageDescription.md)
- [Technologies](Technologies.md)
--- END FILE ---

--- FILE: CMFallDetectionManager.md ---
# CMFallDetectionManager

**An object for managing fall detection events.**

## Availability

- **watchOS** 7.2+


## Overview

In Series 4 and later, Apple Watch can detect when a wearer falls, and contact emergency services if necessary. Using the `CMFallDetectionManager`, your app can request the user’s authorization, and set up a delegate to receive notifications about these *fall detection events*. For more information, see [https://support.apple.com/en-us/HT208944](https://support.apple.com/en-us/HT208944).


> **IMPORTANT**:  To use this API, you must include the [doc://com.apple.documentation/documentation/BundleResources/Information-Property-List/NSFallDetectionUsageDescription](https://developer.apple.com/documentation/BundleResources/Information-Property-List/NSFallDetectionUsageDescription) key in your app’s `Info.plist` file and provide a usage description string for the key. The description appears in the prompt that the user receives when the system asks to access fall detection data for your app. If you don’t include a usage description string, your app crashes when you call this API.


`CMFallDetectionManager` requires an entitlement from Apple. To apply for the entitlement, see [https://developer.apple.com/contact/request/fall-detection-api](https://developer.apple.com/contact/request/fall-detection-api). This entitlement allows the app to run in the background without requiring any additional capabilities. However, you can add capabilities for other background modes, as needed by your app.

There are two approaches to detecting falls in your app. You can either query for [doc://com.apple.documentation/documentation/HealthKit/HKQuantityTypeIdentifier/numberOfTimesFallen](https://developer.apple.com/documentation/HealthKit/HKQuantityTypeIdentifier/numberOfTimesFallen) samples in HealthKit, or you can use Core Motion’s `CMFallDetectionManager`.


### Detect and Respond to Falls

The Core Motion fall detection manager is particularly useful for apps that need to respond to falls in a timely manner so that the app can provide help to the person who fell.

The fall detection manager:

- Notifies the app in real time

- Notifies the app of all fall events

- Provides background runtime so that your app can respond to the fall


### Detect and Monitor Falls Over Time

The HealthKit sample is particularly useful for apps that monitor falls over longer time periods, because there can be a delay between the fall event and HealthKit updating its samples.

HealthKit provides:

- Samples that are available on all devices that can access the person’s HealthKit data—not just the device that detected the fall

- Samples for falls where the person who fell confirmed the fall, or the system escalated the fall to emergency services. If the person who fell dismisses the fall alert, HealthKit doesn’t record the fall.


### Create the Manager

To receive fall detection notifications, ensure that the feature is available on the current device. If the feature is available, create the manager and set its delegate.

```swift
if CMFallDetectionManager.isAvailable  {
    
    // Create the manager.
    let manager = CMFallDetectionManager()
    
    // Assign a delegate that adopts the CMFallDetectionDelegte protocol.
    manager.delegate = myDelegate
    
    // Keep a reference to the manager.
    myManager = manager
}
```

Set the delegate as early as possible, ideally in your extension delegate’s [doc://com.apple.documentation/documentation/WatchKit/WKExtensionDelegate/applicationDidFinishLaunching()](https://developer.apple.com/documentation/WatchKit/WKExtensionDelegate/applicationDidFinishLaunching()) method. The system may not instantiate your app’s user interface when launching your app in the background, so you can’t set the delegate from your user interface code, such as when an interface controller activates, or in response to a change in SwiftUI’s [doc://com.apple.documentation/documentation/SwiftUI/ScenePhase](https://developer.apple.com/documentation/SwiftUI/ScenePhase) state.


> **IMPORTANT**:  Create only a single instance of the detection manager. Store the manager in a variable so you can access it later. Be sure to use a variable that remains in memory throughout your app’s lifetime—for example, add the variable to your extension delegate.



### Request User Authorization

After your user interface loads, check if you previously requested approval. If you haven’t, call [doc://com.apple.coremotion/documentation/CoreMotion/CMFallDetectionManager/requestAuthorization(handler:)](https://developer.apple.com/documentation/CoreMotion/CMFallDetectionManager/requestAuthorization(handler:)) to initiate a request.

```swift
// Check to see if you have already asked the user to
// authorize fall detection event notifications.
if myManager?.authorizationStatus == .notDetermined {
    
    // Request Authorization.
    myManager?.requestAuthorization { (authorizationStatus) in
        
        // Respond to the authorization status.
    }
}
```

Your delegate only receives fall detection event notifications if the user authorizes your app. The user can change your app’s authorization state at any time from Settings > Privacy > Fall Detection.

After you set the delegate and the user grants authorization, your app checks for recent fall events. If the system finds an event, it calls your delegate’s [doc://com.apple.coremotion/documentation/CoreMotion/CMFallDetectionDelegate/fallDetectionManager(_:didDetect:completionHandler:)](https://developer.apple.com/documentation/CoreMotion/CMFallDetectionDelegate/fallDetectionManager(_:didDetect:completionHandler:)) method and passes the most recent event. On subsequent launches, as long as your app remains authorized and the system has detected a recent fall event, the system calls `fallDetectionManager(_:didDetect:completionHandler:)` as soon as you set the delegate.

The system also wakes your app in the background to respond to any new fall detection events. The system only gives your app a short amount of time to respond to the event and call the completion handler. If the app exceeds the allotted time, the system may suspend it.

## Topics

### Checking Availability

- [isAvailable](https://developer.apple.com/documentation/coremotion/cmfalldetectionmanager/isavailable) — A Boolean value that indicates whether the current device supports fall detection.
### Requesting Authorization

- [requestAuthorization(handler:)](https://developer.apple.com/documentation/coremotion/cmfalldetectionmanager/requestauthorization(handler:)) — Requests authorization to receive notifications about fall detection events.
- [authorizationStatus](https://developer.apple.com/documentation/coremotion/cmfalldetectionmanager/authorizationstatus) — The authorization status for receiving fall detection event notifications.
- [CMAuthorizationStatus](https://developer.apple.com/documentation/coremotion/cmauthorizationstatus) — The authorization status for motion-related features.
### Handling Events

- [delegate](https://developer.apple.com/documentation/coremotion/cmfalldetectionmanager/delegate) — A delegate that can receive notifications about fall detection events.
- [CMFallDetectionDelegate](https://developer.apple.com/documentation/coremotion/cmfalldetectiondelegate) — A delegate that receives information about fall detection events and authorization status changes.

---

*Source: [https://developer.apple.com/documentation/com.apple.coremotion/documentation/CoreMotion/CMFallDetectionManager](https://developer.apple.com/documentation/com.apple.coremotion/documentation/CoreMotion/CMFallDetectionManager)*
--- END FILE ---

--- FILE: README.md ---
# Apple Docs Corelocation Documentation

This directory contains Apple Developer Documentation for the **CORELOCATION** framework.

## Summary

- **Total pages**: 26
- **Successfully downloaded**: 26
- **Failed**: 0

## Files

- [CL-EXTERN](CL-EXTERN.md)
- [CLAuthorizationStatus](CLAuthorizationStatus.md)
- [CLBeaconIdentityCondition](CLBeaconIdentityCondition.md)
- [CLCondition](CLCondition.md)
- [CLGeocoder](CLGeocoder.md)
- [CLHeading](CLHeading.md)
- [CLLocationCoordinate2D](CLLocationCoordinate2D.md)
- [CLLocationManager](CLLocationManager.md)
- [CLLocationSourceInformation](CLLocationSourceInformation.md)
- [CLPlacemark](CLPlacemark.md)
- [CLRegion](CLRegion.md)
- [CLServiceSession](CLServiceSession.md)
- [CLUpdate](CLUpdate.md)
- [CLVisit](CLVisit.md)
- [Converting-a-user-s-location-to-a-descriptive-placemark](Converting-a-user-s-location-to-a-descriptive-placemark.md)
- [Converting-between-coordinates-and-user-friendly-place-names](Converting-between-coordinates-and-user-friendly-place-names.md)
- [Core-Location-Constants](Core-Location-Constants.md)
- [Core-Location](Core-Location.md)
- [Deprecated](Deprecated.md)
- [Getting-the-current-location-of-a-device](Getting-the-current-location-of-a-device.md)
- [Location-Push-Service-Extension](Location-Push-Service-Extension.md)
- [Monitoring-location-changes-with-Core-Location](Monitoring-location-changes-with-Core-Location.md)
- [NSLocationAlwaysUsageDescription](NSLocationAlwaysUsageDescription.md)
- [Technologies](Technologies.md)
--- END FILE ---

--- FILE: CLHeading.md ---
# CLHeading

**The orientation of the user’s device, relative to true or magnetic north.**

## Availability

- **iOS** 3.0+
- **iPadOS** 3.0+
- **Mac Catalyst** 13.1+
- **macOS** 10.7+
- **watchOS** 2.0+


## Overview

A [doc://com.apple.corelocation/documentation/CoreLocation/CLHeading](https://developer.apple.com/documentation/CoreLocation/CLHeading) object contains computed values for the device’s azimuth (orientation) relative to true or magnetic north. It also includes the raw data for the three-dimensional vector used to compute those values. A navigation app might use the information to rotate a map so that it reflects the direction that the user is facing.

Typically, you don’t create instances of this class yourself, nor do you subclass it. Instead, you receive instances of this class through the delegate assigned to the [doc://com.apple.corelocation/documentation/CoreLocation/CLLocationManager](https://developer.apple.com/documentation/CoreLocation/CLLocationManager) object whose [doc://com.apple.corelocation/documentation/CoreLocation/CLLocationManager/startUpdatingHeading()](https://developer.apple.com/documentation/CoreLocation/CLLocationManager/startUpdatingHeading()) method you called.


> **NOTE**:  If you want heading objects to contain valid data for the [doc://com.apple.corelocation/documentation/CoreLocation/CLHeading/trueHeading](https://developer.apple.com/documentation/CoreLocation/CLHeading/trueHeading) property, configure your location manager object to deliver location updates. You can start the delivery of these updates by calling the location manager object’s [doc://com.apple.corelocation/documentation/CoreLocation/CLLocationManager/startUpdatingLocation()](https://developer.apple.com/documentation/CoreLocation/CLLocationManager/startUpdatingLocation()) method.


## Topics

### Getting the heading values

- [magneticHeading](https://developer.apple.com/documentation/corelocation/clheading/magneticheading) — The heading (measured in degrees) relative to magnetic north.
- [trueHeading](https://developer.apple.com/documentation/corelocation/clheading/trueheading) — The heading (measured in degrees) relative to true north.
- [headingAccuracy](https://developer.apple.com/documentation/corelocation/clheading/headingaccuracy) — The maximum deviation (measured in degrees) between the reported heading and the true geomagnetic heading.
### Getting the raw heading data

- [x](https://developer.apple.com/documentation/corelocation/clheading/x) — The geomagnetic data (measured in microteslas) for the x-axis.
- [y](https://developer.apple.com/documentation/corelocation/clheading/y) — The geomagnetic data (measured in microteslas) for the y-axis.
- [z](https://developer.apple.com/documentation/corelocation/clheading/z) — The geomagnetic data (measured in microteslas) for the z-axis.
- [CLHeadingComponentValue](https://developer.apple.com/documentation/corelocation/clheadingcomponentvalue) — A type used to report magnetic differences reported by the onboard hardware.
### Getting the event timestamp

- [timestamp](https://developer.apple.com/documentation/corelocation/clheading/timestamp) — The time at which this heading was determined.

---

*Source: [https://developer.apple.com/documentation/com.apple.corelocation/documentation/CoreLocation/CLHeading](https://developer.apple.com/documentation/com.apple.corelocation/documentation/CoreLocation/CLHeading)*
--- END FILE ---

--- FILE: DisplayMessageAction.md ---
# DisplayMessageAction

**An instance that asks StoreKit to display an App Store message, if appropriate.**

## Availability

- **iOS** 16.0+
- **iPadOS** 16.0+
- **Mac Catalyst** 16.0+
- **visionOS** 1.0+


## Overview

A StoreKit message represents a sheet that appears over your app to display important information from the App Store to the customer. Messages have a reason, indicated by the [doc://com.apple.storekit/documentation/StoreKit/Message/reason-swift.property](https://developer.apple.com/documentation/StoreKit/Message/reason-swift.property) value. StoreKit retrieves any messages from the App Store each time your app launches, and presents them by default. Your app can optionally delay or suppress App Store messages by listening for the messages and determining the appropriate time to ask the system to display them.

To use this API, read the [doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/displayStoreKitMessage](https://developer.apple.com/documentation/SwiftUI/EnvironmentValues/displayStoreKitMessage) environment value to get an instance of the structure for a given [doc://com.apple.documentation/documentation/SwiftUI/Environment](https://developer.apple.com/documentation/SwiftUI/Environment). Call the instance to ask StoreKit to display the App Store message. StoreKit displays a message only if it’s still pending. It doesn’t display expired messages. You call the instance directly because it defines a [doc://com.apple.storekit/documentation/StoreKit/DisplayMessageAction/callAsFunction(_:)](https://developer.apple.com/documentation/StoreKit/DisplayMessageAction/callAsFunction(_:)) method that Swift calls when you call the instance.


> **NOTE**:  If your app uses [doc://com.apple.documentation/documentation/UIKit/UIWindowScene](https://developer.apple.com/documentation/UIKit/UIWindowScene) and not SwiftUI views, use [doc://com.apple.storekit/documentation/StoreKit/Message/display(in:)](https://developer.apple.com/documentation/StoreKit/Message/display(in:)) instead.


The following code example listens for App Store messages and decides whether to defer them by saving them to an array, display them immediately, or suppress them. A private function that the app calls according to its logic asks the system to display all the deferred messages.

```swift
import SwiftUI
import StoreKit

struct MessageExampleView: View {
    @Environment(\.displayStoreKitMessage) private var displayStoreMessage
    @State private var deferredMessages = [Message]()
    
    enum MessageBehavior {
        // Display the message at a later time.
        case displayLater
        // Display the message immediately.
        case displayNow
        // Do not display the message.
        case ignore
    }

    var body: some View {
        Text("Hello World")
        .task {
            for await message in StoreKit.Message.messages {
                let behavior = processMessage(message)
                switch behavior {
                    case .displayNow:
                        try? displayStoreMessage(message)
                    case .displayLater:
                        // Save the message to display it later.
                        deferredMessages.append(message)
                    case .ignore:
                        // Suppresses the message.
                        break
                }
            }
        }
    }
        
    private func processMessage(_ message: Message) -> MessageBehavior { 
        var messageBehavior: MessageBehavior

        // Add your logic here to determine how your app should handle the message.

        return messageBehavior
    }
    
    // Call this function when the app is ready to display deferred messages.
    @MainActor private func displayDeferredMessages() {
        for message in deferredMessages {
            try? displayStoreMessage(message)
        }
        deferredMessages.removeAll()
    }
}
```

## Topics

### Displaying the message

- [callAsFunction(_:)](https://developer.apple.com/documentation/storekit/displaymessageaction/callasfunction(_:)) — Tells StoreKit to display the App Store message, if appropriate.

---

*Source: [https://developer.apple.com/documentation/com.apple.storekit/documentation/StoreKit/DisplayMessageAction](https://developer.apple.com/documentation/com.apple.storekit/documentation/StoreKit/DisplayMessageAction)*
--- END FILE ---

--- FILE: Advanced-Commerce-API.md ---
# Advanced Commerce API

**Support In-App Purchases through the App Store for exceptionally large catalogs of custom one-time purchases, subscriptions, and subscriptions with optional add-ons.**

## Availability

- **Advanced Commerce API** 1.0+


## Overview

Use this framework to offer an exceptionally large catalog of one-time purchases, subscriptions, and subscriptions with optional add-ons while using the App Store commerce system. Apps that use this API host and manage their own catalog of In-App Purchases, or SKUs. The App Store commerce system handles the end-to-end payment processing, global distribution, tax support, and customer service.

You can use the Advanced Commerce API and the StoreKit [doc://com.apple.documentation/documentation/StoreKit/in-app-purchase](https://developer.apple.com/documentation/StoreKit/in-app-purchase) API in the same app. Both APIs use the App Store commerce system, including the same signed JWS transactions and JWS renewal info. For products that you offer using the In-App Purchase API, you set up product identifiers in App Store Connect. For products that you offer using the Advanced Commerce API, you host and manage your own catalog of SKUs and add product details dynamically at runtime. For complete setup information, see [doc://com.apple.advancedcommerceapi/documentation/AdvancedCommerceAPI/setting-up-your-project-for-advanced-commerce](https://developer.apple.com/documentation/AdvancedCommerceAPI/setting-up-your-project-for-advanced-commerce).

Advanced Commerce API features are available through requests you make using StoreKit in your app and endpoint requests from your server. To authorize these requests, you generate JSON Web Tokens (JWTs). The App Store Server Library provides a client that makes it easier to create JWTs to authorize calls. For more information about the library, see [doc://com.apple.documentation/documentation/AppStoreServerAPI/simplifying-your-implementation-by-using-the-app-store-server-library](https://developer.apple.com/documentation/AppStoreServerAPI/simplifying-your-implementation-by-using-the-app-store-server-library). For more information about authorizing calls, see  [doc://com.apple.advancedcommerceapi/documentation/AdvancedCommerceAPI/authorizing-server-calls](https://developer.apple.com/documentation/AdvancedCommerceAPI/authorizing-server-calls).

Your server must support the Transport Layer Security (TLS) protocol 1.2 or later to call the Advanced Commerce API.


> **IMPORTANT**: To learn more about eligiblity and apply for access to the Advanced Commerce API, see [https://developer.apple.com/in-app-purchase/advanced-commerce-api/](https://developer.apple.com/in-app-purchase/advanced-commerce-api/).


## Topics

### Essentials

- [Setting up your project for Advanced Commerce API](https://developer.apple.com/documentation/advancedcommerceapi/setting-up-your-project-for-advanced-commerce) — Configure your app in App Store Connect, set up your server, and prepare your SKUs.
- [Creating SKUs for your In-App Purchases](https://developer.apple.com/documentation/advancedcommerceapi/creating-your-purchases) — Define and manage one-time charges, subscriptions, and bundled subscriptions within your app.
- [Setting up a link to manage subscriptions](https://developer.apple.com/documentation/advancedcommerceapi/setupmanagesubscriptions) — Create a deep link to a subscription-management page for your app.
- [Advanced Commerce API changelog](https://developer.apple.com/documentation/advancedcommerceapi/changelog) — Learn about new features and updates in the Advanced Commerce API.
### Tax codes and pricing

- [Specifying prices for Advanced Commerce SKUs](https://developer.apple.com/documentation/advancedcommerceapi/prices) — Provide prices for SKUs with the supported number of decimal places, in milliunits of currency.
- [Choosing tax codes for your SKUs](https://developer.apple.com/documentation/advancedcommerceapi/taxcodes) — Select a tax code for each SKU that represents a product your app offers as an in-app purchase.
- [Handling subscription price changes](https://developer.apple.com/documentation/advancedcommerceapi/handling-subscription-price-changes) — Provide necessary customer communications to notify and gather applicable consent before you initiate a price change.
### API authorization and rate limits

- [Authorizing API requests from your server](https://developer.apple.com/documentation/advancedcommerceapi/authorizing-server-calls) — Create JSON Web Tokens (JWTs) to authorize Advanced Commerce requests from your server.
- [Identifying rate limits for Advanced Commerce APIs](https://developer.apple.com/documentation/advancedcommerceapi/ratelimits) — Recognize and handle the rate limits that apply to Advanced Commerce API endpoints.
### In-app API requests

- [Sending Advanced Commerce API requests from your app](https://developer.apple.com/documentation/StoreKit/sending-advanced-commerce-api-requests-from-your-app) — Send Advanced Commerce API requests from your app that you authorize with a JSON Web Signature (JWS) you generate on your server.
- [Generating JWS to sign App Store requests](https://developer.apple.com/documentation/StoreKit/generating-jws-to-sign-app-store-requests) — Create signed JSON Web Signature (JWS) strings on your server to authorize your API requests in your app.
### One-time charge creation in the app

- [OneTimeChargeCreateRequest](https://developer.apple.com/documentation/advancedcommerceapi/onetimechargecreaterequest) — The request data your app provides when a customer purchases a one-time-charge product.
- [OneTimeChargeItem](https://developer.apple.com/documentation/advancedcommerceapi/onetimechargeitem) — The details of a one-time charge product, including its display name, price, SKU, and metadata.
### Subscription creation in the app

- [SubscriptionCreateRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptioncreaterequest) — The request data your app provides when a customer purchases an auto-renewable subscription.
- [SubscriptionCreateItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptioncreateitem) — The data that describes a subscription item.
### Subscription modification in the app

- [SubscriptionModifyInAppRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmodifyinapprequest) — The request data your app provides to make changes to an auto-renewable subscription.
- [SubscriptionModifyAddItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmodifyadditem) — The data your app provides to add items when it makes changes to an auto-renewable subscription.
- [SubscriptionModifyChangeItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmodifychangeitem) — The data your app provides to change an item of an auto-renewable subscription.
- [SubscriptionModifyRemoveItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmodifyremoveitem) — The data your app provides to remove an item from an auto-renewable subscription.
- [SubscriptionModifyPeriodChange](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmodifyperiodchange) — The data your app provides to change the period of an auto-renewable subscription.
### Subscription reactivation in the app

- [SubscriptionReactivateInAppRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionreactivateinapprequest) — The request your app provides to reactivate a subscription that has automatic renewal turned off.
- [SubscriptionReactivateItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionreactivateitem) — An item in a subscription to reactive.
### Subscription price change from the server

- [Change Subscription Price](https://developer.apple.com/documentation/advancedcommerceapi/change-subscription-price) — Increase or decrease the price of an auto-renewable subscription, a bundle, or individual items within a subscription at the next renewal.
- [SubscriptionPriceChangeRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionpricechangerequest) — The request body you use to change the price of an auto-renewable subscription.
- [SubscriptionPriceChangeResponse](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionpricechangeresponse) — A response that contains signed JWS renewal and JWS transaction information after a subscription price change request.
### Subscription cancellation from the server

- [Cancel a Subscription](https://developer.apple.com/documentation/advancedcommerceapi/cancel-a-subscription) — Turn off automatic renewal to cancel a customer’s auto-renewable subscription.
- [SubscriptionCancelRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptioncancelrequest) — The request body for turning off automatic renewal of a subscription.
- [SubscriptionCancelResponse](https://developer.apple.com/documentation/advancedcommerceapi/subscriptioncancelresponse) — The response body for a successful subscription cancellation.
### Subscription revocation from the server

- [Revoke Subscription](https://developer.apple.com/documentation/advancedcommerceapi/revoke-subscription) — Immediately cancel a customer’s subscription and all the items that are included in the subscription, and request a full or prorated refund.
- [SubscriptionRevokeRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionrevokerequest) — The request body you provide to terminate a subscription and all its items immediately.
- [SubscriptionRevokeResponse](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionrevokeresponse) — The response body for a successful revoke-subscription request.
### Refund request from the server

- [Request Transaction Refund](https://developer.apple.com/documentation/advancedcommerceapi/request-transaction-refund) — Request a refund for a one-time charge or subscription transaction.
- [RequestRefundRequest](https://developer.apple.com/documentation/advancedcommerceapi/requestrefundrequest) — The request body for requesting a refund for a transaction.
- [RequestRefundResponse](https://developer.apple.com/documentation/advancedcommerceapi/requestrefundresponse) — The response body for a transaction refund request.
- [RequestRefundItem](https://developer.apple.com/documentation/advancedcommerceapi/requestrefunditem) — Information about the refund request for an item, such as its SKU, the refund amount, reason, and type.
### Subscription metadata changes from the server

- [Change Subscription Metadata](https://developer.apple.com/documentation/advancedcommerceapi/change-subscription-metadata) — Update the SKU, display name, and description associated with a subscription, without affecting the subscription’s billing or its service.
- [SubscriptionChangeMetadataRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionchangemetadatarequest) — The request body you provide to change the metadata of a subscription.
- [SubscriptionChangeMetadataResponse](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionchangemetadataresponse) — The response body for a successful subscription metadata change.
- [SubscriptionChangeMetadataDescriptors](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionchangemetadatadescriptors) — The subscription metadata to change, specifically the description and display name.
- [SubscriptionChangeMetadataItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionchangemetadataitem) — The metadata to change for an item, specifically its SKU, description, and display name.
### Migration from the server

- [Migrate a Subscription to Advanced Commerce API](https://developer.apple.com/documentation/advancedcommerceapi/migrate-subscription-to-advanced-commerce-api) — Migrate a subscription that a customer purchased through In-App Purchase to a subscription you manage using the Advanced Commerce API.
- [SubscriptionMigrateRequest](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmigraterequest) — The subscription details you provide to migrate a subscription from In-App Purchase to the Advanced Commerce API, such as descriptors, items, storefront, and more.
- [SubscriptionMigrateResponse](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmigrateresponse) — A response that contains signed renewal and transaction information after a subscription successfully migrates to the Advanced Commerce API.
- [SubscriptionMigrateItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmigrateitem) — The SKU, description, and display name to use for a migrated subscription item.
- [SubscriptionMigrateRenewalItem](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmigraterenewalitem) — The item information that replaces a migrated subscription item when the subscription renews.
- [SubscriptionMigrateDescriptors](https://developer.apple.com/documentation/advancedcommerceapi/subscriptionmigratedescriptors) — The description and display name of the subscription to migrate to that you manage.
### Objects and types

- [Data types](https://developer.apple.com/documentation/advancedcommerceapi/datatypes) — Objects and data types for the Advanced Commerce API.
### Signed transaction information

- [JWSRenewalInfo](https://developer.apple.com/documentation/advancedcommerceapi/jwsrenewalinfo) — Subscription renewal information signed by the App Store, in JSON Web Signature (JWS) format.
- [JWSTransaction](https://developer.apple.com/documentation/advancedcommerceapi/jwstransaction) — Transaction information signed by the App Store, in JSON Web Signature (JWS) Compact Serialization format.
### Error handling

- [Error messages and codes](https://developer.apple.com/documentation/advancedcommerceapi/errorcodes) — Error messages and codes for the Advanced Commerce API.

---

*Source: [https://developer.apple.com/documentation/com.apple.advancedcommerceapi/documentation/AdvancedCommerceAPI](https://developer.apple.com/documentation/com.apple.advancedcommerceapi/documentation/AdvancedCommerceAPI)*
--- END FILE ---

--- FILE: RequestReviewAction.md ---
# RequestReviewAction

**An instance that tells StoreKit to request an App Store rating or review, if appropriate.**

## Availability

- **iOS** 16.0+
- **iPadOS** 16.0+
- **Mac Catalyst** 16.0+
- **macOS** 13.0+
- **visionOS** 1.0+


## Overview

Read the [doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/requestReview](https://developer.apple.com/documentation/SwiftUI/EnvironmentValues/requestReview) environment value to get an instance of this structure for a given [doc://com.apple.documentation/documentation/SwiftUI/Environment](https://developer.apple.com/documentation/SwiftUI/Environment). Call the instance to tell StoreKit to ask the user to rate or review your app, if appropriate. You call the instance directly because it defines a [doc://com.apple.storekit/documentation/StoreKit/RequestReviewAction/callAsFunction()](https://developer.apple.com/documentation/StoreKit/RequestReviewAction/callAsFunction()) method that Swift calls when you call the instance.

When you call this API in your shipping app and the system displays a rating and review request view, the system handles the entire process for you. Although you normally call this method when it makes sense in the user experience flow of your app, App Store policy governs the actual display of a rating and review request view. When your app calls this API, StoreKit uses the following criteria:

- If the person hasn’t rated or reviewed your app on this device, StoreKit displays the ratings and review request a maximum of three times within a 365-day period.

- If the person has rated or reviewed your app on this device, StoreKit displays the ratings and review request if the app version is new, and if more than 365 days have passed since the person’s previous review.


> **NOTE**:  Because this API may not present an alert, don’t call it in response to a button tap or other user action.


It’s up to your app to decide on the best timing for requesting reviews. For design guidance, see Human Interface Guidelines > [https://developer.apple.com/design/human-interface-guidelines/ratings-and-reviews](https://developer.apple.com/design/human-interface-guidelines/ratings-and-reviews).


### Test review requests

When your app calls this method while it’s in development mode, StoreKit always displays the rating and review request view, so you can test the user interface and experience. However, this method has no effect in apps that you distribute for beta testing using TestFlight.


### Provide a persistent link to your product page (optional)

People can review your app at any time on the App Store. To make it easier for people to leave reviews, you may include a persistent link to your App Store product page in your app’s settings or configuration screens. Append the query parameter `action=write-review` to your product page URL to automatically open the App Store page where users can write a review.

## Topics

### Call as function

- [callAsFunction()](https://developer.apple.com/documentation/storekit/requestreviewaction/callasfunction()) — Tells StoreKit to ask the user to rate or review your app, if appropriate.
### Environment value

- [requestReview](https://developer.apple.com/documentation/SwiftUI/EnvironmentValues/requestReview)

---

*Source: [https://developer.apple.com/documentation/com.apple.storekit/documentation/StoreKit/RequestReviewAction](https://developer.apple.com/documentation/com.apple.storekit/documentation/StoreKit/RequestReviewAction)*
--- END FILE ---

--- FILE: SKDownloaderExtension.md ---
# SKDownloaderExtension

**An application extension that uses the system implementation to schedule Apple-hosted asset-pack downloads automatically.**

## Availability

- **iOS** 26.0+
- **iPadOS** 26.0+
- **Mac Catalyst** 26.0+
- **macOS** 26.0+
- **tvOS** 26.0+
- **visionOS** 26.0+


## Overview

You can optionally implement the inherited `BAManagedDownloaderExtension` requirements, but don’t implement any of the inherited `BADownloaderExtension` requirements. For more information, see [doc://com.apple.documentation/documentation/BackgroundAssets](https://developer.apple.com/documentation/BackgroundAssets).

---

*Source: [https://developer.apple.com/documentation/com.apple.storekit/documentation/StoreKit/SKDownloaderExtension](https://developer.apple.com/documentation/com.apple.storekit/documentation/StoreKit/SKDownloaderExtension)*
--- END FILE ---
=== END SWIFT DOCUMENTATION ===
=== END CONTEXT ===


Please implement the requirements above. Write the code directly - do not explain, just write the implementation.