You are a Swift/SwiftUI developer. A previous attempt to complete a task failed.

TESTING & DEBUGGING REFERENCE:
For building, testing, and debugging iOS/macOS apps, reference this workflow guide:
/Users/emmanuel/Dev/Tools/Eocon-Foundation-V1/.Foundation/Docs/swiftDocs/Testing/XCODEBUILD_MCP_WORKFLOW.md

This guide covers:
- XcodeBuild MCP server tools for programmatic Xcode interaction
- Building for simulator, booting simulators, installing/launching apps
- UI automation: screenshots, accessibility hierarchy, tap simulation
- Debugging UI issues (button taps, gestures, navigation)

=== ORIGINAL TASK ===
I have created the following plan after thorough exploration and analysis of the codebase. Follow the below plan verbatim. Trust the files and references. Do not re-verify what's written in the plan. Explore only when absolutely necessary. First implement all the proposed file changes and then I'll review all the changes together at the end.

## Observations

T9 is the final ticket. All infrastructure is in place: \`TrainingClause\` / \`TrainingDataStore\` / \`TrainingManager\` / \`ValidationRunner\` / \`HistoricalPolicies\` / \`MLXClauseClassifier\` are fully implemented. The bundled adapter at \`Resources/Adapters/adapters.safetensors\` is a T3 stub (zero-weight placeholder). No \`historical_clauses.jsonl\`, \`validation_clauses.jsonl\`, or \`HistoricalPolicyValidationTests.swift\` exist yet. \`PolicyAnalysisSession\` does not currently accept \`GenerationOptions\`, which is required for greedy-sampling reproducibility in the validation tests.

## Approach

The work splits into three sequential phases: (1) author the labeled JSONL training data derived from the paper, (2) run the LoRA training on macOS and bundle the resulting adapter, (3) write the \`HistoricalPolicyValidationTests.swift\` end-to-end suite. The data file is the critical dependency for both training and the test suite.

---

## Implementation Steps

### Phase 1 — Author Training Data

#### 1. Create \`historical_clauses.jsonl\`

Create \`file:app/decodingOppression/decodingOppression/Data/historical_clauses.jsonl\`.

Each line is a JSON object matching \`TrainingClause\` (defined in \`file:app/decodingOppression/decodingOppression/Models/TrainingModels.swift\`):

| Field | Type | Notes |
|---|---|---|
| \`id\` | UUID string | unique per clause |
| \`text\` | String | verbatim or paraphrased clause text from the paper |
| \`sourcePolicy\` | String | one of the four canonical names below |
| \`targetGroup\` | String | \`outgroup\` / \`ingroupNonElite\` / \`elite\` / \`multiple\` |
| \`effectDirection\` | String | \`burden\` / \`benefit\` / \`neutral\` / \`mixed\` |
| \`architectureScores\` | Object | \`{ \"aar\": 0–1, \"se\": 0–1, \"ij\": 0–1, \"rsc\": 0–1 }\` |
| \`proxyVariables\` | Array\\<String\\> | proxy/dog-whistle terms found in the clause |
| \`usesProxyVariables\` | Bool | |

**Canonical policy names** (must match \`HistoricalPolicies.swift\` exactly):
- \`\"Virginia Slave Codes (1705)\"\`
- \`\"13th Amendment (1865)\"\`
- \`\"HOLC Redlining (1934)\"\`
- \`\"War on Drugs (1971)\"\`

**Distribution target** (≥150 clauses total, stratified 80/20 per policy):

| Policy | Train | Val | Total |
|---|---|---|---|
| Virginia Slave Codes | ~40 | ~10 | ~50 |
| 13th Amendment | ~24 | ~6 | ~30 |
| HOLC Redlining | ~32 | ~8 | ~40 |
| War on Drugs | ~24 | ~6 | ~30 |

Derive clause text from the paper sections: §4.3 (Virginia Slave Codes), §4.4 (13th Amendment), §4.5 (HOLC Redlining), §4.6 (War on Drugs). Architecture scores should reflect the paper's four components (AAR, SE, IJ, RSC). Proxy variable terms should match the \`proxyTerms\` already seeded in \`HistoricalPolicies.swift\` (e.g., \`\"property\"\`, \`\"servitude\"\`, \`\"risk\"\`, \`\"neighborhood\"\`, \`\"crime\"\`, \`\"narcotics\"\`).

#### 2. Create \`validation_clauses.jsonl\`

Create \`file:app/decodingOppression/decodingOppression/Data/validation_clauses.jsonl\` containing the held-out 20% rows extracted from the full dataset above (same schema, same stratification).

#### 3. Register both files in the Xcode project

In \`file:app/decodingOppression/decodingOppression.xcodeproj/project.pbxproj\`, add both \`.jsonl\` files to:
- The \`Data\` group (alongside \`KeywordTaxonomies.json\`)
- The app target's **Copy Bundle Resources** build phase

\`TrainingDataStore.bundledURL\` already looks for \`historical_clauses.jsonl\` in the \`Data\` subdirectory of the bundle, so no code change is needed there.

---

### Phase 2 — LoRA Training & Adapter Bundle

#### 4. Run LoRA training on macOS

Using the macOS app's **Training** view (backed by \`TrainingManager\` from T8):

1. Load \`historical_clauses.jsonl\` via \`TrainingDataStore.load()\`.
2. Start training with \`LoRAConfig.default\` (3 epochs, lr 0.0001, rank 8, alpha 16). Observe the \`TrainingProgress\` stream; iterate epochs until validation loss stabilises (target: val loss < 0.15 or plateau across 2 consecutive epochs).
3. On \`.complete\`, note the \`adapterPath\` URL.

#### 5. Replace the stub adapter

Copy the trained adapter files from the completed training run into \`file:app/decodingOppression/decodingOppression/Resources/Adapters/\`:
- Replace \`adapters.safetensors\` with the real trained weights.
- Update \`adapter_config.json\` to reflect the actual LoRA config used (rank, alpha, target modules: \`q_proj\`, \`v_proj\`, \`k_proj\`, \`o_proj\`).

\`MLXClauseClassifier.loadModel()\` already loads from this bundle path — no code change needed.

#### 6. Document calibration notes

After training, run \`ValidationRunner.runAll(scorer: DefaultPolicyScorer())\` on macOS. Record the \`ValidationResult\` deltas for all four policies. If any policy's \`|delta| > 0.10\`, document the drift in a comment block at the top of \`HistoricalPolicies.swift\` as a calibration note (e.g., \"War on Drugs actual COI: 0.74, delta: −0.04 — within tolerance\").

---

### Phase 3 — End-to-End Validation Test Suite

#### 7. Add \`GenerationOptions\` support to \`PolicyAnalysisSession\`

In \`file:app/decodingOppression/decodingOppression/AI/PolicyAnalysisSession.swift\`, add an optional \`GenerationOptions?\` parameter (defaulting to \`nil\`) to \`classifyClause(_:)\`, \`detectArchitecture(_:)\`, and \`detectProxy(_:)\`. Pass it through to \`session.respond(to:generating:options:)\` inside \`runWithRetry\`. The \`#else\` stub implementations should accept and ignore the parameter.

This is the minimal change needed to allow the test suite to inject \`GenerationOptions(sampling: .greedy)\` without altering production call sites (which pass \`nil\` and get default behaviour).

#### 8. Create \`HistoricalPolicyValidationTests.swift\`

Create \`file:app/decodingOppression/decodingOppressionTests/HistoricalPolicyValidationTests.swift\`.

Structure:

\`\`\`
@Suite(\"Historical Policy Validation — Informative\")
struct HistoricalPolicyValidationTests { ... }
\`\`\`

One \`@Test\` per policy. Each test follows this sequence:

\`\`\`mermaid
sequenceDiagram
    participant Test
    participant DataStore as TrainingDataStore
    participant Session as PolicyAnalysisSession
    participant Scorer as DefaultPolicyScorer
    participant Expect as #expect

    Test->>DataStore: load validation_clauses.jsonl
    DataStore-->>Test: [TrainingClause] filtered by sourcePolicy
    loop for each TrainingClause
        Test->>Session: classifyClause(Clause(text:), options: .greedy)
        Session-->>Test: ClauseClassification
        Test->>Session: detectArchitecture(Clause(text:), options: .greedy)
        Session-->>Test: ArchitectureDetection
        Test->>Session: detectProxy(Clause(text:), options: .greedy)
        Session-->>Test: ProxyDetection
        Note over Test: assemble TierClassification
    end
    Test->>Scorer: score(clauses: [TierClassification])
    Scorer-->>Test: ScoreResult
    Test->>Expect: abs(result.coi - policy.expectedCOI) ≤ 0.10
\`\`\`

Key implementation details:
- Load \`validation_clauses.jsonl\` from the test bundle using \`Bundle(for: HistoricalPolicyValidationTests.self)\` — add the file to the test target's **Copy Bundle Resources** phase as well.
- Convert each \`TrainingClause\` to a \`Clause\` (using \`Clause(id: UUID(), text: clause.text, sectionType: .operativeClauses, targetGroup: nil, effectDirection: nil)\`).
- Assemble \`TierClassification\` from the three \`PolicyAnalysisSession\` calls: map \`ClauseClassification.TargetGroup\` → \`TargetGroup\`, \`EffectDirection\` → \`EffectDirection\`; map \`ArchitectureDetection\` fields → \`ArchitectureScores\`; use \`ProxyDetection\` directly.
- Use \`DefaultPolicyScorer()\` (not \`StubPolicyScorer\`).
- Mark the suite as informative with a leading doc comment: \`// Informative — failures are calibration signals, not CI blockers.\` Use \`withKnownIssue\` (Swift Testing) if the platform does not have Foundation Models available, so the test degrades gracefully on CI machines without Apple Intelligence.
- The four \`@Test\` functions: \`testVirginiaSlaveCodes()\`, \`testThirteenthAmendment()\`, \`testHOLCRedlining()\`, \`testWarOnDrugs()\`.

#### 9. Register \`HistoricalPolicyValidationTests.swift\` in the Xcode project

In \`project.pbxproj\`, add the new file to:
- The \`decodingOppressionTests\` group.
- The \`decodingOppressionTests\` target's **Compile Sources** build phase.
- Also add \`validation_clauses.jsonl\` to the test target's **Copy Bundle Resources** phase (so \`Bundle(for:)\` can find it at test runtime).
=== END ORIGINAL TASK ===

=== REFERENCE CONTEXT ===

=== SWIFT DOCUMENTATION ===

--- FILE: generating-content-and-performing-tasks.md ---
# Generating content and performing tasks with Foundation Models

**Enhance the experience in your app by prompting an on-device large language model.**


## Overview

The Foundation Models framework lets you tap into the on-device large models at the core of Apple Intelligence. You can enhance your app by using generative models to create content or perform tasks. The framework supports language understanding and generation based on model capabilities.

For design guidance, see Human Interface Guidelines > Technologies > [https://developer.apple.com/design/human-interface-guidelines/generative-ai](https://developer.apple.com/design/human-interface-guidelines/generative-ai).


## Understand model capabilities

When considering features for your app, it helps to know what the on-device language model can do. The on-device model supports text generation and understanding that you can use to:

The on-device language model may not be suitable for handling all requests, like:

The model can complete complex generative tasks when you use guided generation or tool calling. For more on handling complex tasks, or tasks that require extensive world-knowledge, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation) and [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling).


## Check for availability

Before you use the on-device model in your app, check that the model is available by creating an instance of [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel) with the [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/default](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel/default) property.

Model availability depends on device factors like:

- The device must support Apple Intelligence.

- The device must have Apple Intelligence turned on in Settings.


> **NOTE**: It can take some time for the model to download and become available when a person turns on Apple Intelligence.


Always verify model availability first, and plan for a fallback experience in case the model is unavailable.

```swift
struct GenerativeView: View {
    // Create a reference to the system language model.
    private var model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            // Show your intelligence UI.
        case .unavailable(.deviceNotEligible):
            // Show an alternative UI.
        case .unavailable(.appleIntelligenceNotEnabled):
            // Ask the person to turn on Apple Intelligence.
        case .unavailable(.modelNotReady):
            // The model isn't ready because it's downloading or because of other system reasons.
        case .unavailable(let other):
            // The model is unavailable for an unknown reason.
        }
    }
}
```


## Create a session

After confirming that the model is available, create a [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession) object to call the model. For a single-turn interaction, create a new session each time you call the model:

```swift
// Create a session with the system model.
let session = LanguageModelSession()
```

For a multiturn interaction — where the model retains some knowledge of what it produced — reuse the same session each time you call the model.


## Provide a prompt to the model

A [doc://com.apple.foundationmodels/documentation/FoundationModels/Prompt](https://developer.apple.com/documentation/FoundationModels/Prompt) is an input that the model responds to. Prompt engineering is the art of designing high-quality prompts so that the model generates a best possible response for the request you make. A prompt can be as short as “hello”, or as long as multiple paragraphs. The process of designing a prompt involves a lot of exploration to discover the best prompt, and involves optimizing prompt length and writing style.

When thinking about the prompt you want to use in your app, consider using conversational language in the form of a question or command. For example, “What’s a good month to visit Paris?” or “Generate a food truck menu.”

Write prompts that focus on a single and specific task, like “Write a profile for the dog breed Siberian Husky”. When a prompt is long and complicated, the model takes longer to respond, and may respond in unpredictable ways. If you have a complex generation task in mind, break the task down into a series of specific prompts.

You can refine your prompt by telling the model exactly how much content it should generate. A prompt like, “Write a profile for the dog breed Siberian Husky” often takes a long time to process as the model generates a full multi-paragraph essay. If you specify “using three sentences”, it speeds up processing and generates a concise summary. Use phrases like “in a single sentence” or “in a few words” to shorten the generation time and produce shorter text.

```swift
// Generate a longer response for a specific command.
let simple = "Write me a story about pears."

// Quickly generate a concise response.
let quick = "Write the profile for the dog breed Siberian Husky using three sentences."
```


## Provide instructions to the model

[doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions](https://developer.apple.com/documentation/FoundationModels/Instructions) help steer the model in a way that fits the use case of your app. The model obeys prompts at a lower priority than the instructions you provide. When you provide instructions to the model, consider specifying details like:

- What the model’s role is; for example, “You are a mentor,” or “You are a movie critic”.

- What the model should do, like “Help the person extract calendar events,” or “Help the person by recommending search suggestions”.

- What the style preferences are, like “Respond as briefly as possible”.

- What the possible safety measures are, like “Respond with ‘I can’t help with that’ if you’re asked to do something dangerous”.

Use content you trust in instructions because the model follows them more closely than the prompt itself. When you initialize a session with instructions, it affects all prompts the model responds to in that session. Instructions can also include example responses to help steer the model. When you add examples to your prompt, you provide the model with a template that shows the model what a good response looks like.


## Generate a response

To call the model with a prompt, call [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re) on your session. The response call is asynchronous because it may take a few seconds for the on-device foundation model to generate the response.

```swift
let instructions = """
    Suggest five related topics. Keep them concise (three to seven words) and make sure they \
    build naturally from the person's topic.
    """

let session = LanguageModelSession(instructions: instructions)

let prompt = "Making homemade bread"
let response = try await session.respond(to: prompt)
```


> **NOTE**: A session can only handle a single request at a time, and causes a runtime error if you call it again before the previous request finishes. Check [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/isResponding](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/isResponding) to verify the session is done processing the previous request before sending a new one.


Instead of working with raw string output from the model, the framework offers guided generation to generate a custom Swift data structure you define. For more information about guided generation, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation).

When you make a request to the model, you can provide custom tools to help the model complete the request. If the model determines that a [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) can assist with the request, the framework calls your [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) to perform additional actions like retrieving content from your local database. For more information about tool calling, see [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling)


## Consider context size limits per session

The *context window size* is a limit on how much data the model can process for a session instance. A token is a chunk of text the model processes, and the system model supports up to 4,096 tokens. A single token corresponds to three or four characters in languages like English, Spanish, or German, and one token per character in languages like Japanese, Chinese, or Korean. In a single session, the sum of all tokens in the instructions, all prompts, and all outputs count toward the context window size.

If your session processes a large amount of tokens that exceed the context window, the framework throws the error [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)). When you encounter the error, start a new session and try shortening your prompts. If you need to process a large amount of data that won’t fit in a single context window limit, break your data into smaller chunks, process each chunk in a separate session, and then combine the results.


## Tune generation options and optimize performance

To get the best results for your prompt, experiment with different generation options. [doc://com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions](https://developer.apple.com/documentation/FoundationModels/GenerationOptions) affects the runtime parameters of the model, and you can customize them for every request you make.

```swift
// Customize the temperature to increase creativity.
let options = GenerationOptions(temperature: 2.0)

let session = LanguageModelSession()

let prompt = "Write me a story about coffee."
let response = try await session.respond(
    to: prompt,
    options: options
)
```

When you test apps that use the framework, use Xcode Instruments to understand more about the requests you make, like the time it takes to perform a request. When you make a request, you can access the [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript](https://developer.apple.com/documentation/FoundationModels/Transcript) entries that describe the actions the model takes during your [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession).

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models)*
--- END FILE ---

--- FILE: Deep_dive_into_Foundation_Models_Framework.md ---
Hi, I’m Louis. Today we’ll look at getting the most out of the Foundation Models framework.

As you may know, the Foundation Models framework gives you direct access to an on-device Large Language Model, with a convenient Swift API. It’s available on macOS, iPadOS, iOS, and visionOS. And because it runs on-device, using it in your project is just a simple import away. In this video, we will look at how sessions work with Foundation Models. How to use Generable to get structured output. How to get structured output with dynamic schemas defined at runtime, and using tool calling to let the model call into your custom functions.

Let’s start simple, by generating text with a session.

Now, I've been working on this pixel art game about a coffee shop, and I think it could be really fun to use Foundation Models to generate game dialog and other content to make it feel more alive! We can prompt the model to respond to a player's question, so our barista gives a unique dialog.

To do this, we'll create a LanguageModelSession with custom instructions. This let's us tell the model what its purpose is for this session and for the prompt we'll take the user's input. And that's really all it takes for a pretty fun new game element. Let's ask the Barista "How long have you worked here?", and let it respond to our question.

```swift
import FoundationModels

func respond(userInput: String) async throws -> String {
  let session = LanguageModelSession(instructions: """
    You are a friendly barista in a world full of pixels.
    Respond to the player's question.
    """
  )
  let response = try await session.respond(to: userInput)
  return response.content
}
```

That was generated entirely on-device. Pretty amazing. But how does this actually work? Let’s get a better sense of how Foundation Models generates text, and what to look out for. When you call respond(to:) on a session, it first takes your session’s instructions, and the prompt, in this case the user’s input, and it turns that text into tokens. Tokens are small substrings, sometimes a word but typically just a few characters. A large language model takes a sequence of tokens as input, and it then generates a new sequence of tokens as output. You don’t have to worry about the exact tokens that Foundation Models operates with, the API nicely abstracts that away for you. But it is important to understand that tokens are not free. Each token in your instructions and prompt adds extra latency. Before the model can start producing response tokens, it first needs to process all the input tokens. And generating tokens also has a computational cost, which is why longer outputs take longer to generate.

A LanguageModelSession is stateful. Each respond(to:) call is recorded in the transcript.

The transcript includes all prompts and responses for a given session.

This can be useful for debugging, or even showing it in your UI.

But a session has a limit for how large it can grow.

If you’re making a lot of requests, or if you’re giving a large prompt or getting large outputs, you can hit the context limit.

If your session exceeds the available context size, it will throw an error, which you should be prepared to catch.

Back in our game, when we're talking with a character and hit an error, the conversation just ends, which is unfortunate, I was just getting to know this character! Luckily there are ways to recover from this error.

You can catch the exceededContextWindowSize error.

```swift
var session = LanguageModelSession()

do {
  let answer = try await session.respond(to: prompt)
  print(answer.content)
} catch LanguageModelSession.GenerationError.exceededContextWindowSize {
  // New session, without any history from the previous session.
  session = LanguageModelSession()
}
```

And when you do, you can start a brand new session, without any history. But in my game that would mean the character suddenly forgets the whole conversation.

You can also choose some of the transcript from your current session to carry over into the new session.

You can take the entries from a session's transcript, and condense it into a new array of entries.

So for our game dialog, we could take the first entry of the session's transcript, which is the instructions. As well as the last entry, which is the last successful response.

```swift
var session = LanguageModelSession()

do {
  let answer = try await session.respond(to: prompt)
  print(answer.content)
} catch LanguageModelSession.GenerationError.exceededContextWindowSize {
  // New session, with some history from the previous session.
  session = newSession(previousSession: session)
}

private func newSession(previousSession: LanguageModelSession) -> LanguageModelSession {
  let allEntries = previousSession.transcript.entries
  var condensedEntries = [Transcript.Entry]()
  if let firstEntry = allEntries.first {
    condensedEntries.append(firstEntry)
    if allEntries.count > 1, let lastEntry = allEntries.last {
      condensedEntries.append(lastEntry)
    }
  }
  let condensedTranscript = Transcript(entries: condensedEntries)
  // Note: transcript includes instructions.
  return LanguageModelSession(transcript: condensedTranscript)
}
```

And when we pass that into a new session, our character is good to chat with for another while.

But keep in mind, the session's transcript includes the initial instructions as the first entry. When carrying over a transcript for our game character, we definitely want to include those instructions.

Including just a few relevant pieces from the transcript can be a simple, and effective, solution. But sometimes it’s not that simple.

Let’s imagine a transcript with more entries.

You definitely always want to start by carrying over the instructions. But a lot of entries in the transcript might be relevant, so for this use case you could consider summarizing the transcript.

You could do this with some external library, or perhaps even summarize parts of the transcript with Foundation Models itself.

So that’s what you can do with the transcript of a session.

Now let’s take a brief look at how the responses are actually generated.

In our game, when you walk up to the barista, the player can ask any question.

But if you start two new games, and ask the exact same question in each, you will probably get different output. So how does that work? Well that’s where sampling comes in.

When the model is generating its output, it does so one token at a time. And it does this by creating a distribution, for the likelihood of any given token By default, Foundation Models will pick tokens within some probability range. Sometimes it might start by saying "Ah", and other times it might pick "Well" for the first token. This happens for every token that's generated. Picking a token is what we call sampling. And the default behavior is random sampling. Getting varied output is great for use cases like a game. But sometimes you might want deterministic output, like when you're writing a demo that should be repeatable. The GenerationOptions API let's you control the sampling method. You can set it to greedy to get deterministic output. And when that's set, you will get the same output for the same prompt, assuming your session is also in the same state. Although note, this only holds true for a given version of the on-device model. When the model is updated as part of an OS update, your prompt can definitely give different output, even when using greedy sampling.

```swift
// Deterministic output
let response = try await session.respond(
  to: prompt,
  options: GenerationOptions(sampling: .greedy)
)
                
// Low-variance output
let response = try await session.respond(
  to: prompt,
  options: GenerationOptions(temperature: 0.5)
)
                
// High-variance output
let response = try await session.respond(
  to: prompt,
  options: GenerationOptions(temperature: 2.0)
)
```

You can also play with the temperature for the random sampling. For example, setting the temperature to 0.5 to get output that only varies a little. Or setting it to a higher value to get wildly different output for the same prompt.

Also, keep in mind, when taking user input in your prompt, the language might not be supported.

There is the dedicated unsupportedLanguageOrLocale error that you can catch for this case.

This can be a good way to show a custom message in your UI.

```swift
var session = LanguageModelSession()

do {
  let answer = try await session.respond(to: userInput)
  print(answer.content)
} catch LanguageModelSession.GenerationError.unsupportedLanguageOrLocale {
  // Unsupported language in prompt.
}

let supportedLanguages = SystemLanguageModel.default.supportedLanguages
guard supportedLanguages.contains(Locale.current.language) else {
  // Show message
  return
}
```

And there's also an API to check whether the model supports a certain language. For example to checkout if the user's current language is supported, and to show a disclaimer when it's not. So that's an overview of sessions. You can prompt it, which will store the history in the transcript. And you can optionally set the sampling parameter, to control the randomness of the session’s output. But let’s get fancier! When the player walks around, we can generate NPCs, Non Playable Characters, again using Foundation Models. However, this time, we want more complicated output. Instead of just plain text, we’d like a name and a coffee order from the NPC. Generable can help us here.

It can be a challenge to get structured output from a Large Language Model. You could prompt it with the specific fields you expect, and have some parsing code to extract that. But this is hard to maintain, and very fragile, it might not always give the valid keys, which would make the whole method fail.

Luckily, Foundation Models has a much better API, called Generable.

On your struct, you can apply the @Generable macro. So, what is Generable and is that even a word? Well, yes, it is.

```swift
@Generable
struct NPC {
  let name: String
  let coffeeOrder: String
}
```

Generable is an easy way to let the model generate structured data, using Swift types The macro generates a schema at compile time, which the model can use to produce the expected structure.

The macro also generates an initializer, which is automatically called for you when making a request to a session.

So then we can generate instances of our struct. Like before, we'll call the respond method on our session. But this time pass the generating argument telling the model which type to generate.

```swift
@Generable
struct NPC {
  let name: String
  let coffeeOrder: String
}

func makeNPC() async throws -> NPC {
  let session = LanguageModelSession(instructions: ...)
  let response = try await session.respond(generating: NPC.self) {
    "Generate a character that orders a coffee."
  }
  return response.content
}
```

Foundation Models will even automatically include details about your Generable type in the prompt, in a specific format that the model has been trained on. You don't have to tell it about what fields your Generable type has In our game, we'll now get some great generated NPC encounters! Generable is actually more powerful than it might seem. At a low level, this uses constrained decoding, which is a technique to let the model generate text that follows a specific schema.

Remember, that schema that the macro generates.

As we saw before, an LLM generates tokens, which are later transformed into text. And with Generable, that text is even automatically parsed for you in a type-safe way. The tokens are generated in a loop, often referred to as the decoding loop.

Without constrained decoding, the model might hallucinate some invalid field name.

Like `firstName`instead of a name. Which would then fail to be parsed into the NPC type.

But with constrained decoding, the model is prevented from making structural mistakes like this. For every token that’s generated, there’s a distribution of all the tokens in the model’s vocabulary.

And constrained decoding works by masking out the tokens that are not valid. So instead of just picking any token, the model is only allowed to pick valid tokens according to the schema.

And that's all without needing to worry about manually parsing the model's output. Which means you can spend your time on what truly matters, like talking to virtual guests in your coffee shop! Generable is truly the best way to get output from the on-device LLM. And it can do so much more. Not only can you use it on structs, but also on enums! So let's use that to make our encounters more dynamic! Here, I've added an Encounter enum, with two cases. The enum can even contain associated values in its cases, so let's use that to either generate a coffee order, or, to have someone that wants to speak to the manager.

```swift
@Generable
struct NPC {
  let name: String
  let encounter: Encounter

  @Generable
  enum Encounter {
    case orderCoffee(String)
    case wantToTalkToManager(complaint: String)
  }
}
```

Let's checkout what we encounter in our game now! Wow, someone really needs a coffee.

Clearly, not every guest is as easy to deal with, so let's level this up by adding levels to our NPCs.

Generable supports most common Swift types out of the box, including Int. So let's add a level property. But we don't want to generate any integer. If we want the level to be in a specific range, we can specify this using a Guide. We can use the Guide macro on our property, and pass a range.

Again, the model will use constrained decoding, to guarantee a value in this range.

While we're at it, let's also add an array of attributes to our NPC.

```swift
@Generable
struct NPC {
  @Guide(description: "A full name")
  let name: String
  @Guide(.range(1...10))
  let level: Int
  @Guide(.count(3))
  let attributes: [Attribute]
  let encounter: Encounter

  @Generable
  enum Attribute {
    case sassy
    case tired
    case hungry
  }
  @Generable
  enum Encounter {
    case orderCoffee(String)
    case wantToTalkToManager(complaint: String)
  }
}
```

We can again use a guide, this time to specify we want exactly three attributes for this array in our NPC. Keep in mind, the properties of your Generable type are generated in the order they are declared in the source code. Here, name will be generated first, followed by the level, then the attributes, and encounter last.

This order can be important, if you’re expecting the value of a property to be influenced by another property.

And you can even stream property-by-property, if you don’t want to wait until the full output is generated. The game is pretty fun now! Almost ready to share with my friends. But I notice the names of the NPCs aren’t exactly what I had in mind. I would prefer to have a first and last name.

We can use a guide for this, but this time just provide a natural language description.

We can say our name should be a “full name”.

And this is effectively another way of prompting. Instead of having to describe different properties in your prompt, you can do it directly in your Generable type. And it gives the model a stronger relation for what these descriptions are tied to.

If we walk around in our game now, we’ll checkout these new names in action.

Here’s an overview of all the guides you can apply to different types.

With common numerical types, like int, you can specify the minimum, maximum or a range. And with array, you can control the count, or specify guides on the array's element type.

For String, you can let the model pick from an array with anyOf, or even constrain to a regex pattern.

A regex pattern guide is especially powerful. You may be familiar with using a regex for matching against text. But with Foundation Models, you can use a regex pattern to define the structure of a string to generate. For example, you can constrain the name to a set of prefixes.

```swift
@Generable
struct NPC {
  @Guide(Regex {
    Capture {
      ChoiceOf {
        "Mr"
        "Mrs"
      }
    }
    ". "
    OneOrMore(.word)
  })
  let name: String
}

session.respond(to: "Generate a fun NPC", generating: NPC.self)
// > {name: "Mrs. Brewster"}
```

And you can even use the regex builder syntax! If this renews your excitement in regex, make sure to watch the timeless classic "Meet Swift Regex" from a few years ago.

To recap, Generable is a macro that you can apply to structs and enums, and it gives you a reliable way to get structured output from the model. You don't need to worry about any of the parsing, and to get even more specific output, you can apply guides to your properties.

So Generable is great when you know the structure at compile time.

The macro generates the schema for you, and you get an instance of your type as output. But sometimes you only know about a structure at runtime. That's where dynamic schemas can help.

I'm adding a level creator to my game, where players can dynamically define entities to encounter while walking around in the game. For example, a player could create a riddle structure. Where a riddle has a question, and multiple choice answers. If we knew this structure at compile time, we could simply define a Generable struct for it:

```swift
@Generable
struct Riddle {
  let question: String
  let answers: [Answer]

  @Generable
  struct Answer {
    let text: String
    let isCorrect: Bool
  }
}
```

But our level creator allows for creating any structure the player can think of.

We can use DynamicGenerationSchema to create a schema at runtime.

Just like a compile-time defined struct, a dynamic schema has a list of properties. We can add a level creator, that can take a player's input.

Each property has a name and its own schema, which defines its type. You can use the schema for any Generable type, including built-in types, such as String.

A dynamic schema can contain an array, where you then specify a schema for the element of the array. And importantly, a dynamic schema can have references to other dynamic schemas.

So here, our array can reference a custom schema that is also defined at runtime.

From the user's input, we can create a riddle schema, with two properties.

The first is the question, which is a string property. And secondly, an array property, of a custom type called Answer.

And we'll then create the answer. This has a string and boolean property.

Note that the riddle's answers property refers to the answer schema by its name.

```swift
struct LevelObjectCreator {
  var properties: [DynamicGenerationSchema.Property] = []

  mutating func addStringProperty(name: String) {
    let property = DynamicGenerationSchema.Property(
      name: name,
      schema: DynamicGenerationSchema(type: String.self)
    )
    properties.append(property)
  }

  mutating func addArrayProperty(name: String, customType: String) {
    let property = DynamicGenerationSchema.Property(
      name: name,
      schema: DynamicGenerationSchema(
        arrayOf: DynamicGenerationSchema(referenceTo: customType)
      )
    )
    properties.append(property)
  }
  
  var root: DynamicGenerationSchema {
    DynamicGenerationSchema(
      name: name,
      properties: properties
    )
  }
}

var riddleBuilder = LevelObjectCreator(name: "Riddle")
riddleBuilder.addStringProperty(name: "question")
riddleBuilder.addArrayProperty(name: "answers", customType: "Answer")

var answerBuilder = LevelObjectCreator(name: "Answer")
answerBuilder.addStringProperty(name: "text")
answerBuilder.addBoolProperty(name: "isCorrect")

let riddleDynamicSchema = riddleBuilder.root
let answerDynamicSchema = answerBuilder.root

let schema = try GenerationSchema(
  root: riddleDynamicSchema,
  dependencies: [answerDynamicSchema]
)

let session = LanguageModelSession()
let response = try await session.respond(
  to: "Generate a fun riddle about coffee",
  schema: schema
)
let generatedContent = response.content
let question = try generatedContent.value(String.self, forProperty: "question")
let answers = try generatedContent.value([GeneratedContent].self, forProperty: "answers")
```

Then we can create the DynamicGenerationSchema instances. Each dynamic schema is independent. Meaning the riddle dynamic schema doesn't actually contain the answer's dynamic schema. Before we can do inference, we first have to convert our dynamic schemas into a validated schema. This can throw errors if there are inconsistencies in the dynamic schemas, such as type references that don't exist.

And once we have a validated schema, we can prompt a session as usual. But this time, the output type is a GeneratedContent instance. Which holds the dynamic values.

You can query this with the property names from your dynamic schemas. Again, Foundation Models will use guided generation to make sure the output matches your schema. It will never make up an unexpected field! So even though it's dynamic, you still don't have to worry about manually parsing the output.

So now when the player encounters an NPC, the model can generate this dynamic content. Which we’ll show in a dynamic UI. Let’s checkout what we run into. I’m dark or light, bitter or sweet, I wake you up and bring the heat, what am I? Coffee or hot chocolate. I think the answer is coffee.

That's correct! I think my players will have a lot of fun creating all sorts of fun levels.

To recap, with the Generable macro, we can easily generate structured output from a Swift type that’s defined at compile time.

And under the hood, Foundation Models takes care of the schema, and converting the GeneratedContent into an instance of your own type. Dynamic schemas work very similar, but give you much more control. You control the schema entirely at runtime, and get direct access to the GeneratedContent. Next, let’s take a look at tool calling, which can let the model call your own functions. I’m thinking of creating a DLC, downloadable content, to make my game more personal. Using tool calling, I can let the model autonomously fetch information. I’m thinking that integrating the player’s contacts and calendar could be really fun.

I wouldn't normally do that with a server-based model, my players wouldn't appreciate it if the game uploaded such personal data. But since it's all on-device with Foundation Models, we can do this while preserving privacy.

Defining a tool is very easy, with the Tool protocol. You start by giving it a name, and a description. This is what will be put in the prompt, automatically by the API, to let the model decide when and how often to call your tool.

It's best to make your tool name short, but still readable as English text. Avoid abbreviations, and don't make your description too long, or explain any of the implementations. Because remember, these strings are put verbatim in your prompt. So longer strings means more tokens, which can increase the latency. Instead, consider using a verb in the name, such as findContact. And your description should be about one sentence. As always, it's important to try different variations to checkout what works best for your specific tool.

Next, we can define the input for our tool. I want the tool to get contacts from a certain age generation, like millennials. The model will be able to pick a funny case based on the game state, and I can add the Arguments struct, and make it Generable.

When the model decides to call this tool, it will generate the input arguments. By using Generable, this guarantees your tool always gets valid input arguments. So it won't make up a different generation, like gen alpha, which we don't support in our game.

Then I can implement the call function. The model will call this function when it decides to invoke the tool.

In this example, we'll then call out to the Contacts API. And return a contact's name for that query.

```swift
import FoundationModels
import Contacts

struct FindContactTool: Tool {
  let name = "findContact"
  let description = "Finds a contact from a specified age generation."
    
  @Generable
  struct Arguments {
    let generation: Generation
        
    @Generable
    enum Generation {
      case babyBoomers
      case genX
      case millennial
      case genZ            
    }
  }
  
  func call(arguments: Arguments) async throws -> ToolOutput {
    let store = CNContactStore()
        
    let keysToFetch = [CNContactGivenNameKey, CNContactBirthdayKey] as [CNKeyDescriptor]
    let request = CNContactFetchRequest(keysToFetch: keysToFetch)

    var contacts: [CNContact] = []
    try store.enumerateContacts(with: request) { contact, stop in
      if let year = contact.birthday?.year {
        if arguments.generation.yearRange.contains(year) {
          contacts.append(contact)
        }
      }
    }
    guard let pickedContact = contacts.randomElement() else {
      return ToolOutput("Could not find a contact.")
    }
    return ToolOutput(pickedContact.givenName)
  }
}
```

To use our tool, we'll pass it in the session initializer. The model will then call our tool when it wants that extra piece of information.

```swift
import FoundationModels

let session = LanguageModelSession(
  tools: [FindContactTool()],
  instructions: "Generate fun NPCs"
)
```

This is more powerful than just getting the contact ourselves, because the model will only call the tool when it needs for a certain NPC, and it can pick fun input arguments based on the game state. Like the age generation for the NPC.

Keep in mind, this is using the regular contacts API, which you might be familiar with. When our tool is first is invoked, it will ask the player for the usual permission. Even if the player doesn’t want to give access to their contacts, Foundation Models can still generate content like before, but if they do give access, we make it more personal.

Let’s walk around a bit in our game until we encounter another NPC. And this time, I’ll get a name from my contacts! Oh hi there Naomy! Let’s checkout what she has to say, I didn’t know you liked coffee.

Note that LanguageModelSession takes an instance of a tool. This means you control the lifecycle of the tool. The instance of this tool stays the same for the whole session.

Now, in this example, because we're just getting a random character with our FindContactsTool, it's possible we'll get the same contact sometimes. In our game, there are multiple Naomy's now. And that's not right, there can only be the one.

To fix this, we can keep track of the contacts the game has already used. We can add state to our FindContactTool. To do this, we will first convert our FindContactTool to be a class. So it can mutate its state from the call method.

```swift
import FoundationModels
import Contacts

class FindContactTool: Tool {
  let name = "findContact"
  let description = "Finds a contact from a specified age generation."
   
  var pickedContacts = Set<String>()
    
  ...

  func call(arguments: Arguments) async throws -> ToolOutput {
    contacts.removeAll(where: { pickedContacts.contains($0.givenName) })
    guard let pickedContact = contacts.randomElement() else {
      return ToolOutput("Could not find a contact.")
    }
    return ToolOutput(pickedContact.givenName)
  }
}
```

Then we can keep track of the picked contacts, and in our call method we don't pick the same one again.

The NPC names are now based on my contacts! But talking to them doesn't feel right yet. Let's round this off with another tool, this time for accessing my calendar.

```swift
import FoundationModels
import EventKit

struct GetContactEventTool: Tool {
  let name = "getContactEvent"
  let description = "Get an event with a contact."

  let contactName: String
    
  @Generable
  struct Arguments {
    let day: Int
    let month: Int
    let year: Int
  }
    
  func call(arguments: Arguments) async throws -> ToolOutput { ... }
}
```

For this tool, we’ll pass in the contact name from a dialog that’s going on in our game. And when the model calls this tool, we’ll let it generate a day, month and a year for which to fetch events with this contact. And we’ll pass this tool in the session for the NPC dialog.

So now, if we ask my friend Naomy’s NPC "What’s going on?", she can reply with real events we have planned together.

Wow, it's like talking to the real Naomy now.

Let’s take a closer look at how tool calling works.

We start by passing the tool at the start of the session, along with instructions. And for this example, we include information like today’s date.

Then, when the user prompts the session, the model can analyze the text. In this example, the model understands that the prompt is asking for events, so calling the calendar tool makes sense.

To call the tool, the model first generates the input arguments. In this case the model needs to generate the date to get events for. The model can relate information from the instructions and prompt, and understand how to fill in the tool arguments based on that.

So in this example it can infer what tomorrow means based on today’s date in the instructions. Once the input for your tool is generated, your call method is invoked.

This is your time to shine, your tool can do anything it wants. But note, the session waits for your tool to return, before it can generate any further output.

The output of your tool is then put in the transcript, just like output from the model. And based on your tool’s output, the model can generate a response to the prompt.

Note that a tool can be called multiple times for a single request.

And when that happens, your tool gets called in parallel. So keep that in mind when accessing data from your tool’s call method.

Alright, that was pretty fun! Our game now randomly generates content, based on my personal contacts and calendar. All without my data ever leaving my device. To recap, tool calling can let the model call your code to access external data during a request. This can be private information, like Contacts, or even external data from sources on the web. Keep in mind that a tool can be invoked multiple times, within a given request. The model determines this based on its context.

Tools can also be called in parallel, and they can store state.

That was quite a lot.

Perhaps get a coffee before doing anything else.

To learn more, you can check out the dedicated video about prompt engineering, including design and safety tips. And, if you want to meet the real Naomy, check out the code-along video. I hope you will have as much fun with Foundation Models as I've had. Thanks for watching.
--- END FILE ---

--- FILE: Meet_the_Foundation_Models_framework.md ---
# Meet the Foundation Models framework

Hi, I’m Erik. And I’m Yifei. And today, we are so excited to get the privilege of introducing you to the new Foundation Models framework! The Foundation Models framework gives you access to the on-device Large Language Model that powers Apple Intelligence, with a convenient and powerful Swift API. It is available on macOS, iOS, iPadOS, and visionOS! You can use it to enhance existing features in your apps, like providing personalized search suggestions. Or you can create completely new features, like generating an itinerary in a travel app, all on-device. You can even use it to create dialog on-the-fly for characters in a game.

It is optimized for generating content, summarizing text, analyzing user input and so much more.

All of this runs on-device, so all data going into and out of the model stays private. That also means it can run offline! And it’s built into the operating system, so it won’t increase your app size. It’s a huge year, so to help you get the most out of the FoundationModels framework, we’ve prepared a series of videos. In this first video, we’ll be giving you a high level overview of the framework in its entirety. Starting with the details of the model.

We will then introduce guided generation which allows you to get structured output in Swift, and the powerful streaming APIs that turn latency into delight.

We will also talk about tool calling, which allows the model to autonomously execute code you define in your app.

Finally, we will finish up with how we provide multi-turn support with stateful sessions, and how we seamlessly integrate the framework into the Apple developer ecosystem. The most important part of the framework, of course, is the model that powers it. And the best way to get started with prompting the model, is to jump into Xcode.

Testing out a variety of prompts to find what works best is an important part of building with large language models, and the new Playgrounds feature in Xcode is the best way to do that. With just a few lines of code, you can immediately start prompting the on-device model. Here I'll ask it to generate a title for a trip to Japan, and the model's output will appear in the canvas on the right. 

```swift
import FoundationModels
import Playgrounds

#Playground {
    let session = LanguageModelSession()
    let response = try await session.respond(to: "What's a good name for a trip to Japan? Respond only with a title")
}
```

Now, I want to see if this prompt works well for other destinations too. In a #Playground, you can access types defined in your app, so I'll create a for loop over the landmarks featured in mine. Now Xcode will show me the model's response for all of the landmarks.

```swift
import FoundationModels
import Playgrounds

#Playground {
    let session = LanguageModelSession()
    for landmark in ModelData.shared.landmarks {
        let response = try await session.respond(to: "What's a good name for a trip to \(landmark.name)? Respond only with a title")
    }
}
```

The on-device model we just used is a large language model with 3 billion parameters, each quantized to 2 bits. It is several orders of magnitude bigger than any other models that are part of the operating system.

But even so, it’s important to keep in mind that the on-device model is a device-scale model. It is optimized for use cases like summarization, extraction, classification, and many more. It’s not designed for world knowledge or advanced reasoning, which are tasks you might typically use server-scale LLMs for.

Device scale models require tasks to be broken down into smaller pieces. As you work with the model, you’ll develop an intuition for its strengths and weaknesses.

For certain common use cases, such as content tagging, we also provide specialized adapters that maximize the model’s capability in specific domains.

We will also continue to improve our models over time. Later in this video we’ll talk about how you can tell us how you use our models, which will help us to improve them in ways that matter to you.

Now that we've taken a look at the model, the first stop on our journey is Guided Generation. Guided Generation is what makes it possible to build features like the ones you just saw, and it is the beating heart of the FoundationModels framework. Let's take a look at a common problem and talk about how Guided Generation solves it.

By default, language models produce unstructured, natural language as output. It's easy for humans to read, but difficult to map onto views in your app.

A common solution is to prompt the model to produce something that's easy to parse, like JSON or CSV.

However, that quickly turns into a game of whack-a-mole. You have to add increasingly specific instructions about what it it is and isn't supposed to do… Often that doesn't quite work… So you end up writing hacks to extract and patch the content. This isn't reliable because the model is probabilistic and there is a non-zero chance of structural mistakes. Guided Generation offers a fundamental solution to this problem.

When you import FoundationModels, you get access to two new macros, @Generable and @Guide. Generable let's you describe a type that you want the model to generate an instance of.

```swift
// Creating a Generable struct

@Generable
struct SearchSuggestions {
    @Guide(description: "A list of suggested search terms", .count(4))
    var searchTerms: [String]
}
```

Additionally, Guides let you provide natural language descriptions of properties, and programmatically control the values that can be generated for those properties.

Once you've defined a Generable type, you can make the model respond to prompts by generating an instance of your type. This is really powerful.

```swift
// Responding with a Generable type

let prompt = """
    Generate a list of suggested search terms for an app about visiting famous landmarks.
    """

let response = try await session.respond(
    to: prompt,
    generating: SearchSuggestions.self
)

print(response.content)
```

Observe how our prompt no longer needs to specify the output format. The framework takes care of that for you.

The most important part, of course, is that we now get back a rich Swift object that we can easily map onto an engaging view.

Generable types can be constructed using primitives, like Strings, Integers, Doubles, Floats, and Decimals, and Booleans. Arrays are also generable. And Generable types can be composed as well. Generable even supports recursive types, which have powerful applications in domains like generative UIs.

```swift
// Composing Generable types

@Generable struct Itinerary {
    var destination: String
    var days: Int
    var budget: Float
    var rating: Double
    var requiresVisa: Bool
    var activities: [String]
    var emergencyContact: Person
    var relatedItineraries: [Itinerary]
}
```

The most important thing to understand about Guided Generation is that it fundamentally guarantees structural correctness using a technique called constrained decoding.

When using Guided Generation, your prompts can be simpler and focused on desired behavior instead of the format.

Additionally, Guided Generation tends to improve model accuracy. And, it allows us to perform optimizations that speed up inference at the same time. This is all made possible by carefully coordinated integration of Apple operating systems, developer tools, and the training of our foundation models. There is still a lot more to cover about guided generation, like how to create dynamic schemas at runtime, so please check out our deep dive video for more details. So that wraps up Guided Generation — we’ve seen how Swift’s powerful type system augments natural language prompts to enable reliable structured output. Our next topic is streaming, and it all builds on top of the @Generable macro you’re already familiar with.

If you’ve worked with large language models before, you may be aware that they generate text as short groups of characters called tokens.

Typically when streaming output, tokens are delivered in what’s called a delta, but the FoundationModels framework actually takes a different approach, and I want to show you why.

As deltas are produced, the responsibility for accumulating them usually falls on the developer.

You append each delta as they come in. And the response grows as you do.

But it gets tricky when the result has structure. If you want to show the greeting string after each delta, you have to parse it out of the accumulation, and that’s not trivial, especially for complicated structures. Delta streaming just isn’t the right formula when working with structured output.

And as you’ve learned, structured output is at the very core of the FoundationModels framework, which is why we’ve developed a different approach. Instead of raw deltas, we stream snapshots.

As the model produces deltas, the framework transforms them into snapshots. Snapshots represent partially generated responses. Their properties are all optional. And they get filled in as the model produces more of the response.

Snapshots are a robust and convenient representation for streaming structured output.

You're already familiar with the @Generable macro, and as it turns out, it's also where the definitions for partially generated types come from. If you expand the macro, you'll discover it produces a type named `PartiallyGenerated`. It is effectively a mirror of the outer structure, except every property is optional.

```swift
// PartiallyGenerated types

@Generable struct Itinerary {
    var name: String
    var days: [Day]
}
```

The partially generated type comes into play when you call the `streamResponse` method on your session.

```swift
// Streaming partial generations

let stream = session.streamResponse(
    to: "Craft a 3-day itinerary to Mt. Fuji.",
    generating: Itinerary.self
)

for try await partial in stream {
    print(partial)
}
```

Stream response returns an async sequence. And the elements of that sequence are instances of a partially generated type. Each element in the sequence will contain an updated snapshot.

These snapshots work great with declarative frameworks like SwiftUI. First, create state holding a partially generated type.

Then, just iterate over a response stream, store its elements, and watch as your UI comes to life.

```swift
ItineraryView: View {
    let session: LanguageModelSession
    let dayCount: Int
    let landmarkName: String
  
    @State
    private var itinerary: Itinerary.PartiallyGenerated?
  
    var body: some View {
        //...
        Button("Start") {
            Task {
                do {
                    let prompt = """
                        Generate a \(dayCount) itinerary \
                        to \(landmarkName).
                        """
                  
                    let stream = session.streamResponse(
                        to: prompt,
                        generating: Itinerary.self
                    )
                  
                    for try await partial in stream {
                        self.itinerary = partial
                    }
                } catch {
                    print(error)  
                }
            }
        }
    }
}
```

To wrap up, let's review some best practices for streaming.

First, get creative with SwiftUI animations and transitions to hide latency. You have an opportunity turn a moment of waiting into one of delight. Second, you'll need to think carefully about view identity in SwiftUI, especially when generating arrays. Finally, bear in mind that properties are generated in the order they are declared on your Swift struct. This matters both for animations and for the quality of the model's output. For example, you may find that the model produces the best summaries when they're the last property in the struct.

```swift
@Generable struct Itinerary {
  
  @Guide(description: "Plans for each day")
  var days: [DayPlan]
  
  @Guide(description: "A brief summary of plans")
  var summary: String
}
```

There is a lot to unpack here, so make sure to check out our video on integrating Foundation Models into your app for more details. So that wraps up streaming with Foundation Models. Next up, Yifei is going to teach you all about tool calling! Thanks Erik! Tool calling is another one of our key features. It lets the model execute code you define in your app. This feature is especially important for getting the most out of our model, since tool calling gives the model many additional capabilities. It allows the model to identify that a task may require additional information or actions and autonomously make decisions about what tool to use and when, when it’s difficult to decide programmatically.

The information you provide to the model can be world knowledge, recent events, or personal data. For example, in our travel app, it provides information about various locations from MapKit. This also gives the model the ability to cite sources of truth, which can suppress hallucinations and allow fact-checking the model output.

Finally, it allows the model to take actions, whether it’s in your app, on the system, or in the real world.

Integrating with various sources of information in your app is a winning strategy for building compelling experiences. Now that you know why tool calling is very useful, let’s take a look at how it works.

On the left we have a transcript which records everything that has happened so far. If you’ve provided tools to the session, the session will present these tools to the model along with the instructions. Next comes the prompt, where we tell the model which destination we want to visit.

Now, if the model deems that calling a tool can enhance the response, it will produce one or more tool calls. In this example, the model produces two tool calls — querying restaurants and hotels.

At this phase, the FoundationModels framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the transcript. Finally, the model will incorporate the tool output along with everything else in the transcript to furnish the final response.

Now that we have a high level understanding of tool calling, let's define a tool.

Here we're defining a simple weather tool, which conforms to the Tool protocol. The weather tool has kind of emerged as the de-facto 'hello world' of tool calling, and it's a great way to get started.

The protocol first requires you to specify a name and a natural language description of the tool.

The framework will automatically provide them for the model to help it understand when to call your tool.

When the model calls your tool, it will run the call method you define.

The argument to the call method can be any Generable type.

The reason your arguments need to be generable is because tool calling is built on guided generation to ensure that the model will never produce invalid tool names or arguments.

After defining your arguments type, you can now write anything you want in the body of your method. Here we're using CoreLocation and WeatherKit to find the temperature of a given city. The output is represented using the ToolOutput type, which can be created from GeneratedContent to represent structured data. Or from a string if your tool's output is natural language. 

```swift
// Defining a tool
import WeatherKit
import CoreLocation
import FoundationModels

struct GetWeatherTool: Tool {
    let name = "getWeather"
    let description = "Retrieve the latest weather information for a city"

    @Generable
    struct Arguments {
        @Guide(description: "The city to fetch the weather for")
        var city: String
    }

    func call(arguments: Arguments) async throws -> ToolOutput {
        let places = try await CLGeocoder().geocodeAddressString(arguments.city)
        let weather = try await WeatherService.shared.weather(for: places.first!.location!)
        let temperature = weather.currentWeather.temperature.value

        let content = GeneratedContent(properties: ["temperature": temperature])
        let output = ToolOutput(content)

        // Or if your tool's output is natural language:
        // let output = ToolOutput("\(arguments.city)'s temperature is \(temperature) degrees.")

        return output
    }
}
```

Now that we have defined a tool, we have to ensure that the model has access to it.

To do so, pass your tool into your session's initializer. Tools must be attached at session initialization, and will be available to the model for the session's lifetime.

```swift
// Attaching tools to a session

let session = LanguageModelSession(
    tools: [GetWeatherTool()],
    instructions: "Help the user with weather forecasts."
)

let response = try await session.respond(
    to: "What is the temperature in Cupertino?"
)

print(response.content)
// It's 71˚F in Cupertino!
```

After creating a session with tools, all you need to do is prompt the model as you would normally. Tool calls will happen transparently and autonomously, and the model will incorporate the tools' outputs into its final response. The examples I’ve shown here demonstrate how to define type-safe tools at compile time, which is great for the vast majority of use cases. But tools can also be dynamic in every way! For example, you can define the arguments and behaviors of a tool at runtime by using dynamic generation schemas. If you are curious about that, feel free to check out our deep dive video to learn more.

That wraps up tool calling. We learned why tool calling is useful and how to implement tools to extend the model's capabilities. Next, let's talk about stateful sessions. You've seen the word session pop up in this video many times already. The Foundation Models framework is built around the notion of a stateful session. By default, when you create a session, you will be prompting the on-device general-purpose model. And you can provide custom instructions.

Instructions are an opportunity for you to tell the model its role and provide guidance on how the model should respond. For example, you can specify things like style and verbosity.

```swift
// Supplying custom instructions

let session = LanguageModelSession(
    instructions: """
        You are a helpful assistant who always \
        responds in rhyme.
        """
)
```

Note that providing custom instructions is optional, and reasonable default instructions will be used if you don't specify any.

If you do choose to provide custom instructions, it is important to understand the difference between instructions and prompts. Instructions should come from you, the developer, while prompts can come from the user. This is because the model is trained to obey instructions over prompts. This helps protect against prompt injection attacks, but is by no means bullet proof.

As a general rule, instructions are mostly static, and it’s best not to interpolate untrusted user input into the instructions.

So this is a basic primer on how to best form your instructions and prompts. To discover even more best practices, check out our video on prompt design and safety.

Now that you have initialized a session, let's talk about multi-turn interactions! When using the respond or streamResponse methods we talked about earlier. Each interaction with the model is retained as context in a transcript, so the model will be able to refer to and understand past multi-turn interactions within a single session. For example, here the model is able to understand when we say "do another one", that we're referring back to writing a haiku.

```swift
// Multi-turn interactions

let session = LanguageModelSession()

let firstHaiku = try await session.respond(to: "Write a haiku about fishing")
print(firstHaiku.content)
// Silent waters gleam,
// Casting lines in morning mist—
// Hope in every cast.

let secondHaiku = try await session.respond(to: "Do another one about golf")
print(secondHaiku.content)
// Silent morning dew,
// Caddies guide with gentle words—
// Paths of patience tread.

print(session.transcript)// (Prompt) Write a haiku about fishing
// (Response) Silent waters gleam...
// (Prompt) Do another one about golf
// (Response) Silent morning dew...
```

And the `transcript` property on the session object will allow you to inspect previous interactions or draw UI views to represent them.

One more important thing to know is that while the model is producing output, its `isResponding` property will become `true`. You may need to observe this property and make sure not to submit another prompt until the model finishes responding.

```swift
import SwiftUI
import FoundationModels

struct HaikuView: View {

    @State
    private var session = LanguageModelSession()

    @State
    private var haiku: String?

    var body: some View {
        if let haiku {
            Text(haiku)
        }
        Button("Go!") {
            Task {
                haiku = try await session.respond(
                    to: "Write a haiku about something you haven't yet"
                ).content
            }
        }
        // Gate on `isResponding`
        .disabled(session.isResponding)
    }
}
```

Beyond the default model, we are also providing additional built-in specialized use cases that are backed by adapters.

If you find a built-in use case that fits your need, you can pass it to SystemLanguageModel's initializer. 

```swift
// Using a built-in use case

let session = LanguageModelSession(
    model: SystemLanguageModel(useCase: .contentTagging)
)
```

To understand what built-in use cases are available and how to best utilize them, check out our documentation on the developer website. One specialized adapter I want to talk more about today is the content tagging adapter. The content tagging adapter provides first class support for tag generation, entity extraction, and topic detection. By default, the adapter is trained to output topic tags, and it integrates with guided generation out of the box. So you can simply define a struct with our Generable macro, and pass the user input to extract topics from it.

```swift
// Content tagging use case

@Generable
struct Result {
    let topics: [String]
}

let session = LanguageModelSession(model: SystemLanguageModel(useCase: .contentTagging))
let response = try await session.respond(to: ..., generating: Result.self)
```

But there's more! By providing it with custom instructions and a custom Generable output type, you can even use it to detect things like actions and emotions.

```swift
// Content tagging use case

@Generable
struct Top3ActionEmotionResult {
    @Guide(.maximumCount(3))
    let actions: [String]
    @Guide(.maximumCount(3))
    let emotions: [String]
}

let session = LanguageModelSession(
    model: SystemLanguageModel(useCase: .contentTagging),
    instructions: "Tag the 3 most important actions and emotions in the given input text."
)
let response = try await session.respond(to: ..., generating: Top3ActionEmotionResult.self)
```

Before you create a session, you should also check for availability, since the model can only run on Apple Intelligence-enabled devices in supported regions. To check if the model is currently available, you can access the availability property on the SystemLanguageModel.

Availability is a two case enum that's either available or unavailable. If it's unavailable, you also receive a reason so you can adjust your UI accordingly.

```swift
// Availability checking

struct AvailabilityExample: View {
    private let model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            Text("Model is available").foregroundStyle(.green)
        case .unavailable(let reason):
            Text("Model is unavailable").foregroundStyle(.red)
            Text("Reason: \(reason)")
        }
    }
}
```

Lastly, you could encounter errors when you are calling into the model.

These errors might include guardrail violation, unsupported language, or context window exceeded. To provide the best user experience, you should handle them appropriately, and the deep-dive video will teach you more about them. That’s it for multi-turn stateful sessions! We learned how to create a session and use it, as well as how our model keeps track of your context. Now that you’ve seen all the cool features of the framework, let’s talk about developer tooling and experience. To start, you can go to any Swift file in your project and use the new playground macro to prompt the model.

Playgrounds are powerful because they let you quickly iterate on your prompts without having to rebuild and rerun your entire app.

In a playground, your code can access all the types in your project, such as the generable types that are already powering your UI.

Next, we know that when it comes to building app experiences powered by large language models, it is important to understand all the latency under the hood, because large language models take longer to run compared to traditional ML models. Understanding where latency goes can help you tweak the verbosity of your prompts, or determine when to call useful APIs such as prewarming.

And our new Instruments app profiling template is built exactly for that. You can profile the latency of a model request, observe areas of optimizations, and quantify improvements.

Now, as you develop your app, you may have feedback that can help us improve our models and our APIs.

We encourage you to provide your feedback through Feedback Assistant. We even provide an encodable feedback attachment data structure that you can attach as a file to your feedback.

```swift
let feedback = LanguageModelFeedbackAttachment(
  input: [
    // ...
  ],
  output: [
    // ...
  ],
  sentiment: .negative,
  issues: [
    LanguageModelFeedbackAttachment.Issue(
      category: .incorrect,
      explanation: "..."
    )
  ],
  desiredOutputExamples: [
    [
      // ...
    ]
  ]
)
let data = try JSONEncoder().encode(feedback)
```

Finally, if you are an ML practitioner with a highly specialized use case and a custom dataset, you can also train your custom adapters using our adapter training toolkit. But bear in mind, this comes with significant responsibilities because you need to retrain it as Apple improves the model over time. To learn more, you can visit the developer website. 

Now that you've learned many of the cool features provided by the new Foundation Models framework, we can't wait to see all the amazing things you build with it! To discover even more about how you can integrate generative AI into your app, how technologies like guided generation work under the hood, and how you can create the best prompts, we have a whole series of wonderful videos and articles for you.

Thank you so much for joining us today! Happy generating!
--- END FILE ---

--- FILE: GenerationOptions.md ---
# GenerationOptions

**Options that control how the model generates its response to a prompt.**

## Availability

- **iOS** 26.0+
- **iPadOS** 26.0+
- **Mac Catalyst** 26.0+
- **macOS** 26.0+
- **visionOS** 26.0+


## Overview

Create a [doc://com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions](https://developer.apple.com/documentation/FoundationModels/GenerationOptions) structure when you want to adjust the way the model chooses output tokens.

## Topics

### Creating options

- [init(sampling:temperature:maximumResponseTokens:)](https://developer.apple.com/documentation/foundationmodels/generationoptions/init(sampling:temperature:maximumresponsetokens:)) — Creates generation options that control token sampling behavior.
### Configuring the response tokens

- [maximumResponseTokens](https://developer.apple.com/documentation/foundationmodels/generationoptions/maximumresponsetokens) — The maximum number of tokens the model is allowed to produce in its response.
### Configuring the sampling mode

- [sampling](https://developer.apple.com/documentation/foundationmodels/generationoptions/sampling) — A sampling strategy for how the model picks tokens when generating a response.
- [GenerationOptions.SamplingMode](https://developer.apple.com/documentation/foundationmodels/generationoptions/samplingmode) — A type that defines how values are sampled from a probability distribution.
### Configuring the temperature

- [temperature](https://developer.apple.com/documentation/foundationmodels/generationoptions/temperature) — Temperature influences the confidence of the models response.

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions)*
--- END FILE ---

--- FILE: LanguageModelSession.md ---
# LanguageModelSession

**An object that represents a session that interacts with a language model.**

## Availability

- **iOS** 26.0+
- **iPadOS** 26.0+
- **Mac Catalyst** 26.0+
- **macOS** 26.0+
- **visionOS** 26.0+


## Overview

A session is a single context that you use to generate content with, and maintains state between requests. You can reuse the existing instance or create a new one each time you call the model. When you create a session you can provide instructions that tells the model what its role is and provides guidance on how to respond.

```swift
let session = LanguageModelSession(instructions: """
    You are a motivational workout coach that provides quotes to inspire \
    and motivate athletes.
    """
)
let prompt = "Generate a motivational quote for my next workout."
let response = try await session.respond(to: prompt)
```

The framework records each call to the model in a [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript](https://developer.apple.com/documentation/FoundationModels/Transcript) that includes all prompts and responses. If your session exceeds the available context size, it throws [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)).

## Topics

### Creating a session

- [init(model:tools:instructions:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/init(model:tools:instructions:)) — Start a new session in blank slate state with instructions builder.
- [SystemLanguageModel](https://developer.apple.com/documentation/foundationmodels/systemlanguagemodel) — An on-device large language model capable of text generation tasks.
- [Tool](https://developer.apple.com/documentation/foundationmodels/tool) — A tool that a model can call to gather information at runtime or perform side effects.
- [Instructions](https://developer.apple.com/documentation/foundationmodels/instructions) — Details you provide that define the model’s intended behavior on prompts.
### Creating a session from a transcript

- [init(model:tools:transcript:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/init(model:tools:transcript:)) — Start a session by rehydrating from a transcript.
- [Transcript](https://developer.apple.com/documentation/foundationmodels/transcript) — A transcript contains a linear history of [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript/Entry](https://developer.apple.com/documentation/FoundationModels/Transcript/Entry) entries.
### Preloading the model

- [prewarm(promptPrefix:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/prewarm(promptprefix:)) — Requests that the system eagerly load the resources required for this session into memory and optionally caches a prefix of your prompt.
### Inspecting session properties

- [isResponding](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/isresponding) — A Boolean value that indicates a response is being generated.
- [transcript](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/transcript) — A full history of interactions, including user inputs and model responses.
### Generating a request

- [respond(options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(options:prompt:)) — Produces a response to a prompt.
- [respond(generating:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(generating:includeschemainprompt:options:prompt:)) — Produces a generable object as a response to a prompt.
- [respond(schema:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(schema:includeschemainprompt:options:prompt:)) — Produces a generated content type as a response to a prompt and schema.
- [respond(to:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(to:options:)) — Produces a response to a prompt.
- [respond(to:generating:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(to:generating:includeschemainprompt:options:)) — Produces a generable object as a response to a prompt.
- [respond(to:schema:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/respond(to:schema:includeschemainprompt:options:)) — Produces a generated content type as a response to a prompt and schema.
- [Prompt](https://developer.apple.com/documentation/foundationmodels/prompt) — A prompt from a person to the model.
- [LanguageModelSession.Response](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/response) — A structure that stores the output of a response call.
- [GenerationOptions](https://developer.apple.com/documentation/foundationmodels/generationoptions) — Options that control how the model generates its response to a prompt.
### Streaming a response

- [streamResponse(to:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(to:options:)) — Produces a response stream to a prompt.
- [streamResponse(to:generating:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(to:generating:includeschemainprompt:options:)) — Produces a response stream to a prompt and schema.
- [streamResponse(to:schema:includeSchemaInPrompt:options:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(to:schema:includeschemainprompt:options:)) — Produces a response stream to a prompt and schema.
- [streamResponse(options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(options:prompt:)) — Produces a response stream to a prompt.
- [streamResponse(generating:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(generating:includeschemainprompt:options:prompt:)) — Produces a response stream for a type.
- [streamResponse(schema:includeSchemaInPrompt:options:prompt:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/streamresponse(schema:includeschemainprompt:options:prompt:)) — Produces a response stream to a prompt and schema.
- [LanguageModelSession.ResponseStream](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/responsestream) — An async sequence of snapshots of partially generated content.
- [GeneratedContent](https://developer.apple.com/documentation/foundationmodels/generatedcontent) — A type that represents structured, generated content.
- [ConvertibleFromGeneratedContent](https://developer.apple.com/documentation/foundationmodels/convertiblefromgeneratedcontent) — A type that can be initialized from generated content.
- [ConvertibleToGeneratedContent](https://developer.apple.com/documentation/foundationmodels/convertibletogeneratedcontent) — A type that can be converted to generated content.
### Generating feedback

- [logFeedbackAttachment(sentiment:issues:desiredOutput:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/logfeedbackattachment(sentiment:issues:desiredoutput:)) — Logs and serializes data that includes session information that you attach when reporting feedback to Apple.
### Getting the error types

- [LanguageModelSession.GenerationError](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/generationerror) — An error that may occur while generating a response.
- [LanguageModelSession.ToolCallError](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/toolcallerror) — An error that occurs while a system language model is calling a tool.
### Instance Methods

- [logFeedbackAttachment(sentiment:issues:desiredResponseContent:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/logfeedbackattachment(sentiment:issues:desiredresponsecontent:))
- [logFeedbackAttachment(sentiment:issues:desiredResponseText:)](https://developer.apple.com/documentation/foundationmodels/languagemodelsession/logfeedbackattachment(sentiment:issues:desiredresponsetext:))

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession)*
--- END FILE ---

--- FILE: Generating-content-and-performing-tasks-with-Foundation-Models.md ---
# Generating content and performing tasks with Foundation Models

**Enhance the experience in your app by prompting an on-device large language model.**


## Overview

The Foundation Models framework lets you tap into the on-device large models at the core of Apple Intelligence. You can enhance your app by using generative models to create content or perform tasks. The framework supports language understanding and generation based on model capabilities.

For design guidance, see Human Interface Guidelines > Technologies > [https://developer.apple.com/design/human-interface-guidelines/generative-ai](https://developer.apple.com/design/human-interface-guidelines/generative-ai).


## Understand model capabilities

When considering features for your app, it helps to know what the on-device language model can do. The on-device model supports text generation and understanding that you can use to:

The on-device language model may not be suitable for handling all requests, like:

The model can complete complex generative tasks when you use guided generation or tool calling. For more on handling complex tasks, or tasks that require extensive world-knowledge, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation) and [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling).


## Check for availability

Before you use the on-device model in your app, check that the model is available by creating an instance of [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel) with the [doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/default](https://developer.apple.com/documentation/FoundationModels/SystemLanguageModel/default) property.

Model availability depends on device factors like:

- The device must support Apple Intelligence.

- The device must have Apple Intelligence turned on in Settings.


> **NOTE**: It can take some time for the model to download and become available when a person turns on Apple Intelligence.


Always verify model availability first, and plan for a fallback experience in case the model is unavailable.

```swift
struct GenerativeView: View {
    // Create a reference to the system language model.
    private var model = SystemLanguageModel.default

    var body: some View {
        switch model.availability {
        case .available:
            // Show your intelligence UI.
        case .unavailable(.deviceNotEligible):
            // Show an alternative UI.
        case .unavailable(.appleIntelligenceNotEnabled):
            // Ask the person to turn on Apple Intelligence.
        case .unavailable(.modelNotReady):
            // The model isn't ready because it's downloading or because of other system reasons.
        case .unavailable(let other):
            // The model is unavailable for an unknown reason.
        }
    }
}
```


## Create a session

After confirming that the model is available, create a [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession) object to call the model. For a single-turn interaction, create a new session each time you call the model:

```swift
// Create a session with the system model.
let session = LanguageModelSession()
```

For a multiturn interaction — where the model retains some knowledge of what it produced — reuse the same session each time you call the model.


## Provide a prompt to the model

A [doc://com.apple.foundationmodels/documentation/FoundationModels/Prompt](https://developer.apple.com/documentation/FoundationModels/Prompt) is an input that the model responds to. Prompt engineering is the art of designing high-quality prompts so that the model generates a best possible response for the request you make. A prompt can be as short as “hello”, or as long as multiple paragraphs. The process of designing a prompt involves a lot of exploration to discover the best prompt, and involves optimizing prompt length and writing style.

When thinking about the prompt you want to use in your app, consider using conversational language in the form of a question or command. For example, “What’s a good month to visit Paris?” or “Generate a food truck menu.”

Write prompts that focus on a single and specific task, like “Write a profile for the dog breed Siberian Husky”. When a prompt is long and complicated, the model takes longer to respond, and may respond in unpredictable ways. If you have a complex generation task in mind, break the task down into a series of specific prompts.

You can refine your prompt by telling the model exactly how much content it should generate. A prompt like, “Write a profile for the dog breed Siberian Husky” often takes a long time to process as the model generates a full multi-paragraph essay. If you specify “using three sentences”, it speeds up processing and generates a concise summary. Use phrases like “in a single sentence” or “in a few words” to shorten the generation time and produce shorter text.

```swift
// Generate a longer response for a specific command.
let simple = "Write me a story about pears."

// Quickly generate a concise response.
let quick = "Write the profile for the dog breed Siberian Husky using three sentences."
```


## Provide instructions to the model

[doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions](https://developer.apple.com/documentation/FoundationModels/Instructions) help steer the model in a way that fits the use case of your app. The model obeys prompts at a lower priority than the instructions you provide. When you provide instructions to the model, consider specifying details like:

- What the model’s role is; for example, “You are a mentor,” or “You are a movie critic”.

- What the model should do, like “Help the person extract calendar events,” or “Help the person by recommending search suggestions”.

- What the style preferences are, like “Respond as briefly as possible”.

- What the possible safety measures are, like “Respond with ‘I can’t help with that’ if you’re asked to do something dangerous”.

Use content you trust in instructions because the model follows them more closely than the prompt itself. When you initialize a session with instructions, it affects all prompts the model responds to in that session. Instructions can also include example responses to help steer the model. When you add examples to your prompt, you provide the model with a template that shows the model what a good response looks like.


## Generate a response

To call the model with a prompt, call [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/respond(to:options:)-b2re) on your session. The response call is asynchronous because it may take a few seconds for the on-device foundation model to generate the response.

```swift
let instructions = """
    Suggest five related topics. Keep them concise (three to seven words) and make sure they \
    build naturally from the person's topic.
    """

let session = LanguageModelSession(instructions: instructions)

let prompt = "Making homemade bread"
let response = try await session.respond(to: prompt)
```


> **NOTE**: A session can only handle a single request at a time, and causes a runtime error if you call it again before the previous request finishes. Check [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/isResponding](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/isResponding) to verify the session is done processing the previous request before sending a new one.


Instead of working with raw string output from the model, the framework offers guided generation to generate a custom Swift data structure you define. For more information about guided generation, see [doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation](https://developer.apple.com/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation).

When you make a request to the model, you can provide custom tools to help the model complete the request. If the model determines that a [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) can assist with the request, the framework calls your [doc://com.apple.foundationmodels/documentation/FoundationModels/Tool](https://developer.apple.com/documentation/FoundationModels/Tool) to perform additional actions like retrieving content from your local database. For more information about tool calling, see [doc://com.apple.foundationmodels/documentation/FoundationModels/expanding-generation-with-tool-calling](https://developer.apple.com/documentation/FoundationModels/expanding-generation-with-tool-calling)


## Consider context size limits per session

The *context window size* is a limit on how much data the model can process for a session instance. A token is a chunk of text the model processes, and the system model supports up to 4,096 tokens. A single token corresponds to three or four characters in languages like English, Spanish, or German, and one token per character in languages like Japanese, Chinese, or Korean. In a single session, the sum of all tokens in the instructions, all prompts, and all outputs count toward the context window size.

If your session processes a large amount of tokens that exceed the context window, the framework throws the error [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession/GenerationError/exceededContextWindowSize(_:)). When you encounter the error, start a new session and try shortening your prompts. If you need to process a large amount of data that won’t fit in a single context window limit, break your data into smaller chunks, process each chunk in a separate session, and then combine the results.


## Tune generation options and optimize performance

To get the best results for your prompt, experiment with different generation options. [doc://com.apple.foundationmodels/documentation/FoundationModels/GenerationOptions](https://developer.apple.com/documentation/FoundationModels/GenerationOptions) affects the runtime parameters of the model, and you can customize them for every request you make.

```swift
// Customize the temperature to increase creativity.
let options = GenerationOptions(temperature: 2.0)

let session = LanguageModelSession()

let prompt = "Write me a story about coffee."
let response = try await session.respond(
    to: prompt,
    options: options
)
```

When you test apps that use the framework, use Xcode Instruments to understand more about the requests you make, like the time it takes to perform a request. When you make a request, you can access the [doc://com.apple.foundationmodels/documentation/FoundationModels/Transcript](https://developer.apple.com/documentation/FoundationModels/Transcript) entries that describe the actions the model takes during your [doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession](https://developer.apple.com/documentation/FoundationModels/LanguageModelSession).

---

*Source: [https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models](https://developer.apple.com/documentation/com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models)*
--- END FILE ---

--- FILE: Code-along-with-the-foundation-models-framework.md ---
Hello everyone and welcome to the Foundation Models Framework Code Along. My name is Shashank. I'm a technology evangelist here at Apple and today I'm excited to guide you through integrating on-device generative AI features directly into your app. We'll cover everything from basic prompting to generating structured output, streaming responses and more. we have an incredible team of experts in Slido. If you have any questions at any point, please ask there.

Let's start with a quick overview to get everyone on the same page. At WWDC24, we introduced Apple Intelligence, powered by large foundation models built into the core of our operating systems. This brought system level features like Writing Tools and Genmoji. Many of you have asked for access to the underlying models and at WWDC25, we delivered with the Foundation Models Framework. It gives you direct access to the same on-device large language model that powers Apple intelligence, all through a powerful Swift API. For developers, this on-device approach has major advantages. Because everything runs locally, user data remains private. Your features work entirely offline with no accounts to set up or API keys to manage. There's no cost to you or someone using the app for any of these requests. And since it's all part of the OS, there's no impact on your app size.

Today, we're gonna build an app together. We'll start with a simple static app that lists landmarks and transform it into a dynamic travel planner. You learn how to generate rich structure itineraries for a custom UI, stream the results in real time as they're created. You'll also learn how to give the model access to custom tools to find real points of interest. And finally, how to optimize your app for performance.

Let's do a quick tour of the final app you'll be building.

Here is a completed app running on my Mac. And this is what you'll have by the end of our session today. We'll start with a simple, clean list of famous landmarks built using SwiftUI. Let's pick a landmark. How about Serengeti? When we click into the detail view, you see a header image and description.

At the bottom is our generate itinerary button. When I click this, the app will call the on-device model to generate a complete three-day travel plan. Watch the screen closely as this happens.

The UI is building itself in real time. First the title, then the description, then the day-by-day plan. This is the streaming API we'll incorporate in chapter four, and it creates a fantastic dynamic user experience. And this here isn't just a block of code, it's rich structured response, which we learn about in chapter two. We have distinct sections for day with a title, subtitle, and a map. Notice the names here like Hotel 1 and Restaurant 1. These aren't random. Our app is using tool calling to get these names, which we'll cover in chapter five. The foundation model framework lets you create rich, structured, and intelligent experiences that feel seamlessly integrated into your app. This is what we're going to build together today. To get the most out of today's Code Along, you have three key resources. First is the Xcode startup project. It has all the boilerplate UI and assets ready to go. If you're watching on developer.apple.com or the developer app, You'll find this under resources on the bottom of the page. If you're watching this on YouTube, it's linked from the description.

Second is our step by step guide on the web page. This is your source of truth with all the instructions and code snippets. You can simply copy paste these to avoid typos. And finally, you have me on the live stream and team of experts behind the scenes to answer your questions. I'll be building this project right here with you, explaining the why behind each change.

Before we jump into the settings and set up the project, let's quickly go over the system requirements for today's session. Everyone is welcome to watch and follow along. However, if you plan on coding live with me, you'll need an Apple Silicon based Mac running macOS Tahoe and Xcode 26. You'll also need to make sure that Apple intelligence is turned on under settings. I'll be building and running the app directly on my Mac today, but you can also use Xcode 26 with a recent iPhone running iOS 26 as your target.

With that, let's move on to the prerequisite section in our Code Along Guide and get our startup project downloaded and configured.

Here in our guide, you'll see a prerequisite section.

First, please click the link to download the project file.

Here, once you've downloaded, you'll find a zip file that macOS may automatically unzip for you. Inside, you'll find a folder named Foundation Models Code Along. This is a startup project we'll be using today. It contains all the necessary views, models, and placeholder code to get us started. I have my project open here and ready to go.

The first thing we need to do is set the developer team. In the project navigator, select the project file.

Then select targets.

Click on signing and capabilities. And under team, select the dropdown and select your team.

To make sure everything is working correctly, select myMac as the run destination in the Xcode toolbar.

and then click on the Run button. This will build and run the project. Alternatively, you can use Command + R.

What I have here is the app that we're gonna be building and adding our generative AI features into. So this is our starting point and we'll be adding powerful features throughout this session. Now let's do a quick tour of our startup project.

First we have here our Playground.swift file under the playgrounds folder. This is where we'll iterate on our prompts and test out foundation models APIs in isolation without having to build and run our entire app. Once we're happy with a prompt here, we'll move this code into our app.

Next is our view models folder and the most important file for us here is itineraryGenerator.swift. All the core logic for creating and managing foundation model sessions, calling the framework APIs and processing the results will live right here. And finally, we have a views folder.

This is where all our SwiftUI code lives. For this code along, the UI is mostly pre-built to let us focus on the Foundation Models Framework. You'll notice that there are several files here and to make it easy to follow, the key files we'll be editing are numbered.

Our job will be to take the output of our itinerary generator and wire it up to these views to create the rich and interactive UI that you'll see in the app.

As you go through these files, you'll notice there are special comments formatted this way. Mark, Code Along, Chapter, and a number. Each number here corresponds directly to the chapter and section with the same number in your Code Along guide. You can use the Xcode Find Navigator to search for the chapter number to see all the outstanding code changes.

Enter the chapter number here, and you'll see all the code changes. As we complete each step, we'll keep deleting these comments so we can track progress throughout the Code Along.

So in summary, we'll follow three simple steps. First, experiment in the playground. Second, implement the core logic in the view model, and finally, display the results in the view. Let's take a closer look at each of these views.

The first screen is a starting point, the main list of landmarks. This is powered by LandmarksView.swift. We won't be touching this file today. It's all set up for us to let us browse and select a destination. When you tap a landmark, you land on the details screen. This view is controlled by the landmark DetailView.swift file. Its main job is to check if the Foundation Models Framework is available on device and decide what UI to show based on that.

Next is the landmark trip view. Its role is to present the generate itinerary button. And this is also where we'll first display the raw unstructured text that we get back from the model.

And finally, the itinerary view is our destination. This view renders the rich structured itinerary data we'll have towards the end of the code along.

We're now ready to dive into the agenda. We've structured the code along into six chapters. We'll start with the absolute basics where you learn how to start prompting the model to generate text. Then we'll move beyond simple text and see how to get structured Swift types back from the model, making it easy to map model output to your custom views. We'll then dive into prompting techniques. That lets you improve models accuracy by providing high quality examples directly in your prompts. Next, we'll learn how to stream the model's response to update the UI in real time for a great user experience. We'll then explore tool calling. Tools are powerful ways to give the model access to your own custom functions and data to extend its capabilities.

And finally, we'll cover performance optimizations to make our generative features feel faster and more responsive. With that, let's dive into the basics of Foundation Models Framework. You can use the Foundation Models Framework to send a prompt to the on-device large language model, or LLM for short. The LLM can then reason about your prompt and generate text. For example, you could ask it to generate a three-day itinerary to Paris, and the model will respond with a detailed plan.

To start prompting the model, you'll need to create a session. The framework is built around this idea of stateful language model session, which maintains a history of all prompts and responses.

In this chapter, we'll get familiar with foundation models prompts and sessions. First, we'll start in the playground to get a feel for the API. We'll create a language model session and get our first response from the model. Then, we'll add concise instructions to shape the tone and content. Next, we look at availability API to handle different states gracefully. Once we are comfortable, we'll switch to the app, update the itinerary generator in a view model, and display the raw text output in our views. So let's head on over to our Code Along Guide.

Our goal in chapter one is to make our very first request on-device language model. We'll use the Xcode Playground to send a simple text prompt and see what happens. This will help us understand the model's basic behavior.

Feel free to copy and paste this code block into Xcode Playground.swift file and you can use this handy copy button on the top right corner. I'll be adding these lines of code step by step and explaining what is going on. Let's head over to our Xcode. open up our Playground.swift file. To prompt a model, you need three simple steps. The first is to import the Foundation Models Framework, which we've already done. The next step is to create a playground.

As soon as you use a playground macro to create a playground, you'll see a canvas show up on the right. If it doesn't, you can always click on editor options and ensure that there's a check mark next to canvas. you can click the refresh button and what that does is run all the code contained within the playground block. Right now you don't see an output because we haven't added anything. Step two in prompting the model is to create a session.

What we have here is let a variable session equal to language model session and you'll see the playground canvas automatically shows what is in the session variable. So you see that there are tools which we'll discuss in a later chapter and then transcript which includes all the conversations that you have with the model.

Step three is to prompt the model.

We say let response equal to try await session dot respond to and provide a prompt generate a three day itinerary to Paris. This is an async request so we await its response.

As soon as we do that, on the right side on the canvas, you'll see we have a response variable which includes a few properties. First is prompt. The prompt shows generate a 3-day itinerary to Paris and then there's a property called content which is of type String.

Let's click on this and you'll see that there's a detailed 3-day itinerary to Paris. Certainly here's a 3-day itinerary for exploring Paris, highlighting some of the city's most iconic sites and experience, and you see day by day plans for day one, morning, afternoon, and so on.

Great. Let's go back to our guide here and discuss a key topic. When you make the very first call to Session.response, you might notice that there's a slight delay. This is because the on-device language model needs to be loaded into memory before it can process your request. Our first request triggers a system to load the model, which causes the initial latency. We'll see how to address this in a later chapter. And we also saw that the output was unstructured natural language text, which is easy for us to read, but hard to use in a custom Swift UI. In the next chapter, we'll see how to generate structured output using Swift types instead of raw text. Finally, it's important to note that the entire itinerary without any data ever leaving your device. It's completely private and works offline. So congratulations, you've successfully prompted the on-device foundation model using the Foundation Models Framework.

Oh, and one last thing, let me head back to our playground.

We are always interested in improving the model, and if you want to provide feedback, you can always use these buttons right here in Canvas to share your feedback with us. Let's head on over to our Code Along Guide to Section 1.2, Guiding the model with instructions.

Our goal now is to get more consistent and higher quality results. We can do this by providing the model with instructions. Think of instructions as permanent rules or persona for the entire conversation within a single session. Feel free again to copy this piece of code into Playground and run it. and I'm going to go and add these instructions.

Back in our Playground.swift file, I add a new variable called instructions, and I say, your job is to create an itinerary for the user. Each day needs an activity, hotel, and restaurant. Always include a title, a short description, and a day-by-day plan. We can pass these instructions into the language model session using the instruction argument. When you pass this, the canvas will automatically detect code changes and update our results. We see now that we have our content property under response and this will include the request that we made, which is include activity, hotel and restaurant, and you can see this here, activity, hotel, and restaurants.

A question you may have is, what is the difference between these instructions and prompts? Let's take a look.

Instructions can be used to define a persona, set rules, and specify desired format for the response. This should come from the developer. Prompts, on the other hand, can come from someone using the app. The model is trained to obey instructions over prompts, and this can help protect against prompt injection attacks where the user may ask the model to ignore guidance provided in the prompt. As a rule, keep the instructions static and avoid inserting user input into them.

Also note that instructions are maintained throughout the session's life. Every interaction is recorded in the session's transcript, and The initial instructions are always the first entry.

Great, we're able to successfully prompt a model and get responses. But it's important to consider that our app might run on devices where Apple intelligence isn't available and showing a non-functional feature can be a bad user experience.

For example, the device may not even support Apple intelligence or The device may support Apple intelligence, but the user has not enabled it. Or the model assets are still downloading and they're not ready for use yet. Let's take a close look at how to handle these cases. We'll head on back to our Code Along Guide. We are now in section 1.3 in our Code Along Guide, Handling Model Availability.

The model provides APIs for availability.

Let's head on over to Xcode and take a closer look at each of these cases in this switch block and what they mean for your app.

Back in our Playground.swift file, a neat feature of playground is you can add multiple of these in the same Swift file.

I added a new # playground block here that includes the availability code. All right. Let's take a look at these APIs. You can also check the output of the multiple playgrounds. The second playground will show up as a second tab here on our canvas. And you'll see my Mac does support Apple intelligence. So it says foundation model is available and ready to go. Let's take a closer look at these cases now.

The first case is available. This means you have a green light. the model is loaded and you're ready to make generation requests.

If it says unavailable and device not eligible, this means the model doesn't support Apple Intelligence. You should gracefully hide the generative UI and show an alternate experience.

For unavailable and Apple Intelligence not enabled, this means the device is capable, but Apple Intelligence is turned off in settings. This is your chance to prompt the user to enable it.

Unavailable and model not ready, this is a temporary state, likely because the model assets are still downloading. The best practice is to tell the user to try again. We're now ready to add these features into our app. Let's head on over to our Code Along Guide.

We're now in the app section of chapter one. In this section, we'll update our landmark DetailView.swift to check the model availability and display a message if it is unavailable.

Feel free to copy these code blocks. You can search for these marked comments to know exactly where to insert these code changes and I'll be doing this live with you. Let's head on over to our Xcode project and click on landmark detail view.swift in the views folder. Again, as a reminder, you can always use the find navigator to look for all the code changes that you need to make in this chapter. All right, the first thing to do is to add our model instance.

So we say private let model equal to system language model.default. This is exactly the same line of code we used in our playground, so it should look familiar to you. And since I've added this, I'm gonna delete this comment. So it disappears from our find navigator. The next code change we need to do is to delete this placeholder availability code I have here. This was purely for convenience, so I'm gonna delete this. And as soon as I do that, Xcode will promptly remind me that availability has not been defined yet, but that's an easy fix because we have our model now, model.availability.

And I'm gonna get rid of this line of code too. Okay? With that code change, we've made all the changes to this specific file. Now, we've added these availability checks, which is familiar to you because we use the same in the playground, but how do you test them? You may not have access to multiple test devices. Thankfully, there's an easy way. Right here in the scheme settings in the project, there's an option to simulate unavailability. Let's take a look. Click on foundation models code along, click edit scheme, and if you scroll down, you'll see an option that says simulated foundation models availability.

If you click this, there are a few different options, and these options should be familiar to you because these are the cases we covered in the playground. So I'm gonna click Apple Intelligence Not Enabled, close, and I'm going to build and run our app.

We have our app here, I'm gonna select Sahara Desert, and aha, I see a message here that says, Trip Planner is unavailable because Apple Intelligence has not been turned on.

And this is the same message we have in our unavailability view.

Great. Let me switch this back so we can keep adding additional features throughout the code along.

All right. Let's head on over to section 1.5 in our Code Along Guide.

Now we are ready to update the app's itinerary generator to initialize a language model session and define a function called generate itinerary to invoke the model from our views. The code again should look familiar to you because we already implemented this in the code along. Now we'll be migrating this into our app. So let's head on over to Xcode and open our itinerary Generator.swift file, which you will find in the view models folder.

We'll again use our find navigator to look for all the code changes that we need to make and track progress. All right. In itinerary Generator.swift file, the first change we'll need to make is to add a session property.

And I'm going to do that first. So we define a variable called session for language model session.

Next, Xcode will remind us that we have not initialized a session. So we are going to initialize this session right here in the init functions.

Okay, so here's what we added. We added an instructions variable where we use the same instructions we had in the playground. Your job is to create an itinerary for the user. Each day needs an activity hotel and restaurant. Always include a title, a short description, and a day-by-day plan and I have a session with language model session and we pass in the instructions.

Okay, the third and final change we need to do is to update our generate itinerary function. This is the function that we'll invoke from our views in order to send in the prompt and get back a response. Let's go make this code change.

Okay, here's what we added. First, we said let prompt equal to generate a day count day itinerary to landmark.name. Day count here defaults to three and then landmark.name is the name of the landmark that the user clicks on when they open the app. So we gather this name and we pass it to the prompt So we can generate a response for that specific landmark. Next, we have letResponse equal to tryAwaitSession.Response and pass in a prompt. Finally, the response variable has a property.content, which you can recall from our playground canvas that we observed, which had all the natural unstructured text, which is a string, and we assign it to itinerary content.

That includes all the code changes for our view model, which is now ready to be called from our views. Let's head on back to our Code Along Guide to section 1.6. This is our final section in chapter one. We will now update the landmark trip view to take the output from the itinerary generator and display it in the app. Again, feel free to make these code changes by following along these comments. I'm gonna head over to Xcode.

Click on Views.

And landmark trip view. Okay.

All right, the first code change we'll need to make is to add a local variable for the itinerary generator class in our view model.

All right, so we have itinerary generator of the type itinerary and I'm gonna delete this comment.

The next code we need to do, a code change we need to do is to create an instance of this when the view is loaded.

So here's what we introduce under task modify. We said let generator equal to itinerary generator, which is the ViewModel class, and we pass in the landmark, so that it has information about which landmark the user clicked on, which if you recall, we pass it to the prompt. And then we hold on to the itinerary generator here. I'm going to delete the code change that we just made.

The next change we need to do is to update our view itself. Let's take a closer look at the view. By default, we have a Boolean variable here called requested itinerary. It is set to false.

Because it's set to false, we load up the first view on top here, which is a text field that has the landmark name. We access the landmark.name, and then we access the short description using Landmark.shortDescription. This is what shows up when the user has not generated, asked the model to generate an itinerary. But when the requested itinerary is set to false, we need to load up a new view where we can populate it with the model's output, and that is what we are going to implement right now. So I'm gonna remove the else case here and introduce a new else case where I say, if let content equal to itinerary generator dot itinerary content. If you recall, itinerary content is the string variable has our model's output. And then we simply take that content and we update it in our text view. Since we made this change, I'm gonna get rid of this comment too. We're almost there. We have one final change in this view. If you scroll back down here, we've defined a button that will show up at the bottom of the screen and currently this button is hidden. So we'll need to make two minor code changes here. One, we want to show the button so you can either comment this out or straight out delete it like what I'm doing. And then we need to insert code here to generate the itinerary when the user taps on the button. So let's add that here. Okay, so we said await itinerary generator and we invoke the generate itinerary function. If you recall, this is the function that takes the prompt and then passes it to the model and gets the output. That concludes all the code changes in this chapter.

Okay, we are now ready to build and run this app. Click on the run button here, which will build and run the app.

Here is our app. I'm gonna click on Sahara Desert here and we see that we have our generate itinerary button and when I click on this generate itinerary button, the prompt and instructions are being sent to the on device LLM which is going to generate a response asynchronously, token by token, right here on device.

If you see this itinerary like I do, congratulations, you built your very first fully functional on-device generative AI feature using Foundation Models Framework. With just a few lines of Swift, you've tapped into the power of Apple Intelligence.

This is great, but what we have here is a wall of text. What if I wanted to pull out a hotel name and show it on a map? This isn't the rich experience that we want. We'll address this in chapter two with guided generation. We'll discuss how you can get outputs using Swift structs directly from the model. For now, let's quickly recap chapter one.

In this chapter, we learned how to create a session and prompt the model for a basic text response. We saw how to provide instructions to guide the model's output and we covered how to handle different availability states using the availability API. Finally, we integrated these features into our app by updating the view model and view.

That wraps up chapter one.

Now that we can generate raw text, let's see how we can get structured data from the model to build a much richer UI.

Let's start with a fundamental challenge when working with LLMs. By default, they give us unstructured text, like the itinerary we just generated. While a human can read it, for an app developer, this can be challenging to work with. For example, how would you reliably extract the hotel for day one to plot it on a map? You'd have to write complex string parsing code that could break if the model's output changed. What we want instead is structured data that maps directly to our apps logic.

We'd need a more advanced nested structure that can be implemented using Swift structs. This itinerary object should contain an array of objects which in turn should contain an array of activity objects and so on. This is where guided generation comes in. The Foundation Models Framework provides APIs that allow you to specify exactly what your output should look like. If you have a Swift struct, you can simply apply @Generable to it. And this lets the model generate structured data using native Swift types.

We'll start this chapter in the playground where we'll define a simple struct with a generable macro applied to it. We'll then build on it to create more complex nested data structures for the model to generate. Finally, we'll go back to the app, we'll refactor our itinerary generator to output our new structured itinerary type and update our views to display it in a rich UI.

Let's head on back to our Code Along Guide.

We're now in chapter two, generating structured outputs. Our goal is to move beyond simple strings and get structured type-safe Swift data directly from the model. This will allow us to build rich custom UI without any fragile string parsing.

Feel free to copy this piece of code again into Playground and take a look at its output. And I'll be explaining what is going on when we introduce this new struct called simple itinerary. Let's go to the Xcode Playground file and make these code changes.

I'm gonna get rid of the second playground we just added and right here I'm going to introduce this new struct called simple itinerary. Let me walk through what this looks like and how we can incorporate it in our foundation model code to generate this output. First, this struct has a few different properties. It has a title, which is of type String. It has a description, which is of type String. And it has days, which is an array of String.

We want the model to generate these fields and we can provide it additional information by providing guides. The guide has a description argument which says, an exciting name for the trip. This tells the model that it has to generate a title for this variable and similarly we have description, a short and engaging description for the trip and similarly for day count. What we can do now is provide this to the model and we can do that using the generating argument. So previously we had Session.response and just the prompt. So I'm going to add a new argument called generating and provide simple Itinerary.self.

And we can go and refresh our canvas.

This will run through the code and we'll take a look at the output.

Okay. We have our response here. And let's take a closer look at the content property here.

Previously, this content was a string. If you look carefully here, it says this is a struct simple itinerary. Let's open this up. And what you'll notice is the output one to one matches with the struct that we just defined here. So we have our title, which is Parisian Bliss. And that is our title property here. We have our description that is right here. And then we have our array of String. So you'll see days, which is an array of String with day by day activity plan.

Awesome. So let's go back to our Code Along Guide and the section 2.2. An itinerary doesn't just have to be string or array of Strings. It can have nested structs too. Now, let's take a look at a full itinerary struct that we'll be building in our app. So we're going to make a small code change here. All you have to do is replace simple itinerary with itinerary.self, and we are going to make this code chain and I'll explain what this itinerary struct looks like. Back to Xcode.

I'm going to delete this simple itinerary and replace simple itinerary with just itinerary. Okay, so what does itinerary look like? You can command and click on this to open the definition or head on over to the models folder, click on itinerary.swift file, and here you'll see a new struct called itinerary which has similar fields to what we saw in simple itinerary but more. Let's take a closer look. It also has a title which is of type String. It has a description, it has rationale. And if you take a closer look at days, you'll see that it's no longer an array of String. It is actually an array of day plan, which in turn is its own struct. It has its own title, its own subtitle, its own destination and an activity, which is an array of another struct called activity, which has a type, title, description, and type here happens to be an enum, which is also generable. The enum is a great way to have the model generate specific cases that are predefined. For example, here, the type can only be sightseeing, food and dining, shopping, hotel and lodging. If you scroll all the way up to the top, there is another way to constrain what the model can generate. We can use enums or here for destination name. We have a guide that says any of and we provide model data dot landmark. What this tells the model is that it has to generate a destination name that is one of the landmarks that we see when we open up the app. This includes the Serengeti, the Grand Canyon, Sahara Desert and so on. So the output must be one of these. So this is what the itinerary struct looks like. And this is what we actually use in the app. Let's head on back to our Swift playground. And because if you recall, we said the destination name should be one of the names from the list. Paris is not part of the list. So I'm going to change it to something that is actually on the list. How about Grand Canyon? Canvas will detect this code change and let's take a look at the output.

There we have our response. It includes a content and again, if you take a closer look, it is of struct itinerary, not simple itinerary because we updated it. Let's open this up. You'll see it has a title, destination name, description, rationale and days, which is an array of date plan struct. You open it up, you have multiple days, and activities is a type of an activity struct, and so on.

The key thing to note here is that when you apply @Generable, it is completely composable. The framework understands how to build this entire complex object from the top down, all while guaranteeing structural correctness. Now let's integrate this into our app. Let's head on over to our Code Along Guide. We're now in the app section of chapter two. In this section, we'll update our itinerary generator to use the itinerary generable struct that we just tested in the playground. Feel free to copy this code again and I'm going to be making these code changes with you. So let's head on over to itinerary generator under view models folder and bring up my find navigator, set it to chapter 2 so I can take a look at all the code changes I'll be making in this chapter. The first code change we need to make is way up top here where we have to update our itinerary content to not be a string anymore but be a type of type itinerary. So let's first change the name of this variable to just itinerary and update string to itinerary. We can delete this comment because we've made this code change. The next code change we need to do is if you scroll down to the generate itinerary function you see that Xcode is promptly reminding us that itinerary content no longer exists. So we can update this to itinerary because we just added this. And it is complaining because the content currently that is coming out of Session.respond is a string. So similar to last time in the playground, we're going to add the generating argument and provide itinerary.self.

So the model can now output a value that is of the type itinerary and because we made this code change I'm going to get rid of this comment right here. Okay, the final change we'll need to make is to remove additional structural guidance that we are providing in our instructions. Notice how we say each day needs an activity, hotel and restaurant, always include a title, short description, day by day. But all of this information is already in our itinerary generable struct. We don't need to provide it again in our instructions. So the benefit, another benefit of using generables is you can make your prompts much simpler, which can help improve performance as well. So I'm gonna get rid of this comment.

And that concludes all the code changes in this section. So we've updated our itinerary generator view model to be able to generate our generable structure. Let's head on over to section 2.4, updating our view to display the structure data. In this section, we'll update our landmark trip view to generate itinerary view instead of the raw text that we saw in the previous section. This is a very quick code change, so let's head on over to our landmark trip view, which you'll find as the second number to find the views folder.

And the code change we'll need to make is right here.

If you recall previously, we loaded this view when the model output was generated, but we are no longer generating a string. So we can no longer use a text view. So first we have to update this, but then we also need to update this with another view instead of text so that we can actually extract the fields from our itinerary and populate it in a rich UI. So let me replace this with an updated view and I'll talk about what that looks like.

Okay, so here is what I did. And you can also copy and paste this from our guide. Let's take a closer look. So I said itinerary equal to itinerary generator.itinerary. And instead of the text view, we have this itinerary view, which takes in a landmark and takes in the generated itinerary. Now this itinerary view exists in our views folder but we haven't looked at it so let's take a closer look. This should be the file number three of course you can also command and click on this to open it up. Alright we won't be making any code changes to this file in this chapter but you see comments here which means we'll likely be making changes will surely be making changes in a later chapter. What this view does is it can take an itinerary that was generated by the model, extract the fields and create the rich UI we saw in the initial demo. If you take a closer look at our body here we see it can extract the itinerary title, its description, populated and then it's if you scroll down you'll see that when it extracts the day-by-day activity, there is a dedicated view called day view that can show that and we use for each to loop through these and extract all the properties and lay it out. Notice this is so much simpler than being able to parse strings and update it.

All right, so let's head on over to our slides. So the key benefit of guided generation is that it fundamentally guarantees structural correctness. It uses a technique called constraint decoding to do that. What it does is give you control over what the model should generate, whether that be strings or numbers or arrays or even a custom data structure that you define.

This also means that our prompts can be a lot simpler and more focused on the desired behavior instead of prompting the model for specific output formats. This also tends to improve model accuracy allow for optimizations that speed up inference. So to recap, in this chapter we explored how to get structured data from the model. We use the generable macro to define our own Swift types and saw how to create complex data structures by nesting them. We then updated our app to generate and render this structured data in a rich user interface. Let's go build this model to take a look at all the changes we did. Here's our app. Let's click on Sahara Desert and generate itinerary. Similar to before, it's going to take our prompts and instructions and send it to the model and now instead of generating the wall of text, it generates the itinerary type, we extract all the fields, and then populate it in our app using the new view, which is itinerary view. All right, this concludes this chapter. And then, now we are getting that we're getting structured data as model outputs. We can now switch gears and focus on improving the quality and consistency of the output with additional prompting techniques.

While a good prompt tells the model what to do, sometimes it's more effective to just show it. We can include a high quality example as an instance of our generable type directly in a prompt.

This is great because it gives the model a better idea towards the type of responses I'm looking for. So in this chapter, we'll be focusing on improving the quality of our generated content. We'll start again in the playground by using the Prompt Builder API to create more dynamic prompts. Then we'll explore one-shot prompting by providing a high quality example in the prompt to improve the model's accuracy. Finally, we'll integrate what we learned into our apps itinerary generator.

Let's head on over to our Code Along Guide.

We're now in chapter three, prompting techniques. Our goal now is to improve the quality and reliability of a model's output. First, we'll explore how to introduce dynamic prompts using the Prompt Builder API.

Let's head on over to Playground and take a look. Again, feel free to copy this code block into your Playground.swift file.

We are in Xcode, Heading over to Playground.swift file.

Okay. The key code change we're gonna make here is to introduce a prompt using the prompt builder API. Previously, if you recall, under Session.respond, we provided the two argument with generate a three-day itinerary to Grand Canyon in the format of a string. But instead, we can define the prompt not as a string, but using the prompt builder API and passing the values to a closure. The key benefit is that it can now include things like Swift conditionals. So right up top here, we have a variable called which is a Boolean, which is currently set to true. And then within the Prompt Builder API, I use this Boolean to conditionally update my prompt. So if the kit-friendly Boolean is true, then we inject this additional information into the prompt, which is the itinerary must be kit-friendly. We can update our Session.response call to include this new prompt and refresh our canvas.

Let's take a look at our output.

We have our response variable, content.

I'm gonna open up rationale here and take a look. So it says, this itinerary provides a safe, engaging and educational experience for children, ensuring they enjoy the natural beauty of Grand Canyon while being supported by age appropriate activities and accommodation. So you'll see that the model is honoring our request and this came in as a conditional and the benefit of this again is that you can have these prompt speed dynamic. This could be something that the user selects on the app or it could be something that you learn as a developer from the user's preference and update a prompt.

Awesome. Let's go back to our Code Along Guide to section 3.2. Our goal now is to use a more advanced prompting technique called one-shot prompting to show the model exactly what a high quality response looks like. So let's head on over to our Code Along.

So right in my prompt filter API here inside disclosure, I'm going to add another line of code here. Here I say, here is an example of the desired format, but don't copy its content. And I introduce an example. Let's take a closer look. It says Itinerary.exampleTripToJapan. Now what is this? So you can command click on this or head over to models folder, click on itinerary and scroll down and you'll see that example trip to Japan is defined right here. The first thing that you'll notice that this is not a big string that includes an example. This is actually an instance of the itinerary generable with all its properties populated. You'll see that we have a title, a destination name, description, rationale, days, and all the properties manually populated for you. We can head back to our playground and you'll see that we do have an output here and this output will include the additional information that we provided as a one-shot example in order to guide the tone and quality of the response.

The most important part is that we are embedding this itinerary.exampleTripToJapan directly into the prompt. This is our golden example. We're also telling the model explicitly, don't copy its content. We wanted to learn from the style and structure and not just repeat the data. Let's head back now to our guide.

We're now in the app section of chapter 3. We'll now integrate this one-shot prompting approach into the app. The code change we'll need to make is to update the prompt in our itinerary generator in the ViewModels folder and include our example. Let's go make this code change. We are back in our Xcode. I'm going to click on View Models and Itinerary Generator. I'm going to pull up my Find Navigator and click Section 3. You'll see the code change that we do need to make right here.

So within our Generate Itinerary function, we obviously define our prompt here and we're going to replace this prompt.

and I'm going to delete the previous prompt.

Again, just like what we used in the playground, we say let prompt equal to use a prompt builder API, pass this closure. This includes the same string that we previously had, but we also include this additional information whereby introduce Itinerary.exampleTripToJapan, which is of the type Itinerary. So not only does it include all the guidance, but also the schema that's part of this prompt now.

And because we made this change, we can get rid of this comment. And you'll notice that we made all the changes in chapter 3, which means we are ready to build and run this app. and take a look at the build app.

We can choose Serenity here, click on Generate Itinerary. We can ensure that the model will take the prompt, the instructions and the additional example, pass it to the model and generate our final output. There you go.

Okay, our app is working great. Let's close this and head on over to our slides. So in this chapter, we focused on prompting techniques. We learned how to use a prompt builder to construct prompts dynamically and saw how you can use one-shot prompting to improve the quality and consistency of the model's output. We then applied this by updating our app to include a detailed example in our prompt. While @Generable enforces the structure, the one-shot example teaches the model about relationship and the style within the structure.

The model also uses the provided example for the desired tone of voice, ensuring that the generated text aligns with the tone you want to set for the app.

While the difference in output may not always be dramatic, it's an important way to significantly improve the quality of your generated content. And that wraps up our section on prompting techniques.

This is a great place to pause. Let's take a quick 10 minute break. Feel free to use this time to catch up on the code, grab a coffee or stretch your legs. When we get back, we have some really exciting topics ahead. We'll make a UI update in real time with streaming, extend the model's capabilities with tool calling and wrap up with performance optimizations. We'll be back in 10 minutes. See you soon.

Welcome back everyone. I hope you had a great break. Let's keep going. With our high quality prompts in place, let's enhance the user experience by streaming the response in real time. In this chapter, we'll focus on refactoring our itinerary generator to use the streaming API to improve the user experience by streaming the model's response. We'll see how to handle partially generated content as the model is generating the response. We'll then update our view to render the itinerary as it's being generated, providing for a much responsive feel. So let's head on over to our guide. We're now in chapter four, streaming responses. Our goal in this chapter is to dramatically improve the user experience by streaming responses and showing the itinerary as it is being generated. We'll start by updating the itinerary generator file. This section doesn't include a playground component because it's easy to appreciate the streaming responses directly in the app. So let's head on over to our Xcode and open up itinerary generator.

We'll again use our find navigator, update to chapter four and take a look at all the code changes we need to make starting with itinerary. The first change we'll need to make is update our itinerary variable to be of the type Itinerary.partiallyGenerated.

So what is partiallyGenerated? Think of this as a mirror version of our struct where every single property is an optional. @Generable defines this automatically for us. It's a perfect way to represent data that arrives over time. So that is the first code change. I'm going to remove the comment here.

And the next code change we'll need to make is down here. So recall, our generate itinerary function included this async call to Session.respond, we passed our prompt, and then we passed our generable, and then we received our output. What we want instead is the model to generate responses and stream the responses to us. So what we are going to do is replace this code with a new API called Session.streamResponse. Let's take a look.

So we replaced Session.response with Session.streamResponse and kept the rest of the argument same. So you still pass in a prompt, you still provide the generating argument with the itinerary. But we don't have an await here. What we get instead is an async sequence called stream, which means we can then loop over it and assign all the outputs to our itinerary, which includes all these options. So we say try await partial response in stream, and we can extract it using partialresponse.content where you'll get a snapshot every time of whatever has been generated at that point in time. Because we made this code change, I'm going to remove this comment as well.

Okay, that includes all the code changes we need to make to our itinerary generator. So let's head on over to our Code Along Guide and move on to section 4.2. Now we are ready to update our views. Since partially generated fields are optionals, we can use if let statements to safely unwrap these options. And that is what we are going to do in this section. So we'll update our itinerary view, which we previously just got a preview in an earlier chapter, but now we are going to actually go make code changes to this. So let's head on over to Xcode, click on the views folder and click on itinerary view.

Okay, at the very top, you'll notice that we have itinerary, so we should also update this with the partially generated type that we also defined in our view model. And we need to make this code change to all the generables that we have here. So not only itinerary, but all the nested generables too. So if you scroll all the way down, if you recall, we have our day view, which includes a day plan, which should also be partially generated. And each time I make this code change, I'm going to remove these comments and further down, you'll also remember we have our activity array, and we are going to do the same to that.

Okay, so that is the primary code change to the generables. Let's go back all the way up to the top and you'll see Xcode is complaining about a few other things. So the other code change we'll need to make is, if you recall, I said these are optional, so we have to unwrap them. So let's go and do that.

So here is what I did. I said if let title equal to itinerary.title, If let is a great way to deal with these optionals. And because I have a title here, I don't need to extract it from itinerary. So I remove that. So that takes care of title. Now I need to repeat the same step for our description.

I use if let and update the text view to include description.

And then I need to repeat this again for rationale.

And I need to do this again for the other fields, which is days.

Okay, so you get the gist. So we have to keep doing this for all the itinerary fields that properties that we are accessing to safely unwrap them. Now I'm going to do something that I've been asking all of you to do all this while, which is go back to our Code Along Guide here and copy the completely updated file and paste it here because we have to do this for every single property. If you scroll here, you see in step three, we says repeat this for all these properties. So we changed title description rationale, but you have to do this for all the day plan and the activity views too. So instead, what I'm going to do right now in this code along is click on the show updated views, which includes all the code changes. So what I'm going to do is click on this copy button on the top right and go back to our Xcode itineraryview.swift file and just replace all the code with the updated code. And you can see in our find navigator that we don't have any more comments so we've made all the code changes. So I showed you a few different code changes that we need to make but you have to do the same for every single property. So that concludes all the code changes for chapter 4. So to quickly recap, we spoke about the changes we need to make to view model, which is used partially generated, and we updated our views to unwrap these options. So we're now ready to run this app. Click on run, it will build and run this app. And we have our app right here. I'm going to click on Sahara Desert here and click generate itinerary. Unlike previously where it was an async call, now we are able to stream responses as it is being generated. This has great user experience because someone using the app can start consuming this content even before all of the itinerary has been loaded.

Awesome. In this chapter, we made a big leap in user experience. We refactored our app to use a streaming API and learn how to work with partially generated content in our view model. And finally, we updated our view to display the itinerary as it is being generated in real time. That wraps up chapter four on streaming responses. Now our app is looking great but let's make it even smarter by giving the model new capabilities with tool calling. First let me introduce the concept of tool calling. In addition to what you provide to the prompt the model brings its own core knowledge from its training data but remember the model is built into the OS and its knowledge is frozen in time. So For example, if you ask it about weather in Cupertino right now, there's no way for it to know what that information is. To handle cases where you need real time or dynamic data, the framework supports tool calling. Here's how it works. We have a session transcript.

If you provided tools to the session, the session will present the tool definition to the model along with the instructions. In our example, the prompt tells the model which destination we want to visit.

Now, if the model decides that calling a tool can enhance the response, it will produce one or more tool calls. In this example, the model produces two tool calls, querying restaurants and hotels.

At this phase, the Foundation Models Framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the session transcript.

Finally, the model will incorporate the tool output and everything else in the transcript into the final response.

As we've seen so far, the model can be very creative, often giving a slightly different itinerary each time we make a request. While this randomness is great for creativity, it can be a challenge when we need predictable For an advanced feature like tool calling, especially when testing and debugging, we need to ensure that the model behaves consistently. We want to guarantee that it will call our tool when we expect it to. To achieve this, we are going to make another small change to our request using generation options API to use greedy sampling. Greedy sampling tells the model to stop being creative and to always pick the most obvious next token. This makes the models output deterministic. For our app, this ensures that the model will reliably call our tool every single time.

In this chapter, we'll take a look at a tool that can find points of interest. We'll then provide this tool to our language model session and instruct the model how to use it.

Back in the app, we'll integrate this tool into our itinerary generator to get real world data into our itineraries. Let's head on over to our Code Along Guide. We are now in chapter five, tool calling.

Our itinerary contains model generated hotel and restaurant names, and these may not be up to date. Our goal is to give the model a tool it can use to call a Swift code and fetch hotel and restaurant names that we've provided.

Let's go make these code changes to first build a tool and later use this tool in our app. I'll head on over to Xcode and click on our ViewModels folder and you'll see a new file here that says Find Points of Interest Tool.

Click on that. So here we have a class called Find Points of Interest Tool that conforms to the tool protocol, which means we have to define a few properties here that will go through step by step. So let's start making these code changes and I'll explain what is going on. The first change we'll need to make is to add a name and description for our tool. So I'm going to do that here.

So we provide our tool with a name which is find points of interest and a description which is find points of interest for a landmark. This is critical for the model to understand when to invoke this tool. So it will use the name and the description to determine when to invoke this tool. The next change we'll need to make is down here where let me pull up our find navigator so we can see all the code changes that we need to be making. Next code change we need to make here is to define the categories that the tool can search for points of interest for and we'll do that by introducing this generable enum. So The category is an enum that includes hotels and restaurants. This can of course include other cases like museums or campgrounds and others. We're going to use this in our next code chain which is to update our arguments.

Here we have an argument struct here. Let's update this and I'll talk about what this does.

the argument struct, I have a property here that says let point of interest and it is of type category which is something we just defined. So this point of interest could be a hotel or a restaurant and we also provide a guide. The guide has a description that says this is the type of destination to look for. So this argument is the contract between the tool and the model. When the model wants to invoke the tool, it will pass this argument to the tool so that the tool has access to whether it's a hotel or a restaurant that it wants the response from the tool, the category that it wants the response from the tool.

We've updated the argument. And now we're going to update our call function right here.

This function is the heart of our tool. It receives the arguments, performs an action, and returns an output that gets added back into the session's transcript for the model to see and use. So let's make this change.

Okay. And I'll go through step by step what's going on here.

First, I say let results equal to await get suggestions. We have not defined this. We'll define this in a moment. Essentially, think of this as a function that the call method can invoke in order to get these specific points of interest. And then the results will be part of the output here, which you can then, as you see in the return statement, we can insert this result as a string output back to be provided back to the model. The model then uses that information along with the prompts and instructions to generate the final response. So, the last code change we need to make, of course, is to define this function. I have a placeholder function here called getSuggestions. Let's update this.

All right. So within getSuggestions, I have a switch block here which takes in a category and then if it's a restaurant, it can return restaurant1, restaurant2 or restaurant3. Similarly, if it's a hotel, it can return hotel one, hotel two, or hotel three. Now, these are, for this demo, we are using hardcoded data. In a real app, this is where you would call APIs like MapKit or a server-side API to fetch real live data.

Okay, so we made all the code changes to our tool, which means we have fully defined our tool. Let's head on over back to our Code Along Guide and move on to section 5.2.

So what we're going to do now is test this tool. So we'll head on over to our playground and provide this tool to the model and take a look at the results. Again, like before, feel free to copy paste this and I'm going to step through each of these lines of code and explain what exactly is happening. So back in our Xcode, I'm going to switch over to Playground.swift file. And for this section, I'm going to just clean up the previous code and start from scratch.

Okay, so we have our empty playground here.

First, I'm going to add instructions.

Here, a neat feature of playground is that it has access to all the data structures in your Xcode project. without having to build the app. So what I'm doing here is create a landmark variable that has access to the model data defined here under the models folder under model data dot Swift and I say model data dot landmark zero which means I'm going to access one of those landmarks that you see specifically we are going to access the first landmark and if you recall that is Sahara Desert. So you have access to the same list of landmarks that you get when you run the app. So we take that and then we just defined this Find Points of Interest tool right here in the ViewModels folder. So we are going to create an instance of this tool and we can pass it the landmark because it uses that information. And finally, we have our instructions just like before. There are two minor code changes if you look carefully. One, it's no longer a string but instruction builder similar to prompt builder wherein we pass in a closure and provide our instructions. And the second key change you'll notice, very important for tool calling, is we say always use the find points of interest tool to find hotels and restaurants in this landmark. Now this instruction is telling the model that it must invoke this tool in order to get the points of interest response. Now we'll create a language model Session. Similar to previous code change, we said language model session and pasta instruction, but we do introduce a new argument called tools. Here tools can be an array of tools. We have only one tool here which is point of interest tool. Since it's an array, you can provide multiple tools so the model can reason about your prompts and instructions and decide which tool to call when and get back the response. So we've included our tool in our session. Next, we define a prompt. There are no changes to the prompt itself here. And finally, We will invoke the model.

No code changes here too, except we do introduce options that we briefly discussed in the slide. This generation option with sampling set to greedy will ensure that we always get consistent, repeatable and deterministic output given that the rest of the prompts and instructions are consistent. Okay, let's take a look at the canvas here and take a look at the output. Okay, we have our response generated and we have our content here.

So we have our title, description, rationale, days. Let me pick one of these days, day 0, arrival and let me take a look at the activities.

I'll open up activity 0, activity 1, and activity 2.

Now if you look closely, you'll see here under activity 1 description it says, "Enjoy a traditional Moroccan dinner at restaurant 1." You'll also see this in the title, "Dine-in at restaurant 1." And similarly, you see title for activity 2 here that says, "Stay in hotel 1 and unwind at hotel 1." This is the output of the tool that is being inserted into the output of the model. So the model took in a prompt instructions, the landmark name, invoke the tool, got back the hotel and restaurant names and inserted it back to the transcript and generated this response.

Let's take a look at the transcript itself.

So what I'm doing here is just creating a temporary variable for the session itself and capturing it into inspect session. The reason I'm doing this is to take a closer look at the session and transcript and we can see the tool calls being placed. Okay. So we have our inspect session, which we just created.

Now we are going to take a look at these properties. you see tools. It has one tool that we provided. And if you look at transcript, it has six elements in this entries. And here we have our instructions, which is always the very first entry in the transcript. And then we have a prompt, which is our initial request. And then we have tool calls. The model autonomously decided that it needs to call our tool. Then we have our tool outputs. The framework executed our tool and inserted these tool outputs back into the transcript. And then finally we have our response. The model synthesized the original prompt, the tool output data to generate this final response. There are two tool calls here because we are requesting for both restaurants as well as hotels. And you'll see this under the tool calls. So there's a request for a restaurant and a hotel.

Awesome. Let's head back to our Code Along Guide.

So now that we know how a tool works, we defined the tool, we tested the tool in our playground, We're now ready to update our itineraryGenerator.swift file to incorporate our tool into the app. That's what we'll do in section 5.3. We'll make our code changes to itineraryGenerator.swift. Feel free to copy and paste this into your files. The key changes as you see here we'll be making is to update our instructions, create an instance of tool, and also pass it to our language model session. Let's head on over to Xcode and open up our itineraryGenerator.swift. I'll also bring up my find navigator, set this to chapter 5 and we'll start making code changes.

So the first change we need to do is of course update our instructions.

I'm going to delete the previous instructions because I have this new instructions which includes point of interest tool that we defined and this additional text that is asking the model to call this tool in order to get the points of interest. And we also of course need to update the language model session using the tools argument. And since it can accept multiple tools it is an array and and we'll pass in the tool.

Okay, so that is the two code changes that we need to make in our initializer. And we did that, so I'm gonna get rid of these comments so we can track our changes. Okay, the final change we need to make is in the generate itinerary method here.

Recall, we mentioned that if we want get deterministic outputs, we can use greedy sampling. By default, it does random sampling. So right here, after in this session.stream response, after we pass the prompt, after we pass the generating argument, we can pass our options. Let me clean this up so it's easy for everyone to read.

All right, so we have our Session.streamResponse, we have a prompt, we have our generating argument, and finally we have our options which includes generation options and we use sampling and set it to greedy. Okay, so that concludes all the code changes that we need to make. Let's ensure we get rid of this comment.

There you go. If you don't see anything for chapter 5 in your Find Navigator, that means we've made all the code changes, we are ready to build and run this app.

Click on the run button, this will build and run this app.

And here is our app. Let's go through the standard user flow, which is click on Sahara Desert. I see a generate itinerary button. I click on that. Now this includes a streaming API along with our tools and it takes our instructions, our prompts, sends it to the model along with the tool definition. And as you see here, you can see stay at hotel one and dine in at restaurant one. These were responses from the tool that were inserted back into the session transcript and the model used all the information from the instructions, the prompts, the tool calls, The tool responses, package all of it, synthesized it, and is able to generate the output in the format of the generable, itinerary generable.

Fantastic. All right, let's go back to our slides and recap. So in this chapter, we gave the model powers with tool calling. We discussed a custom tool with its own arguments and call function. We learned how to provide the tool to the language model session, and importantly, how to instruct the model on when and how to use the tool. Finally, we integrated our tool into the app to fetch points of interest and include them in the generated itinerary. That wraps up chapter five on tool calling.

Before we wrap up this code along, let's look at a couple of key techniques to optimize performance and make our generative features feel more responsive.

Let's head on over to a Code Along Guide and move on to chapter six, Performance and Optimization.

Our app is now feature complete, but to make our app performant, we first need to understand where the bottlenecks are. We can't optimize what we can't measure. For this, we will use a powerful developer tool called Instruments.

Let's head on over to Xcode.

We'll do something slightly different now. If you long press on the run button here, you'll see a few different options. You see run, test, profile and analyze. So I'm going to click profile. What this does is it'll build the app and then launch up Xcode instruments.

Let's wait for it to finish building and there it is. So this is Xcode Instruments. We'll choose the blank template and then once you have your instruments open, I'm going click on this plus symbol here and search for foundation models. Okay, we are now ready to profile our app. I'm gonna click on the record and this will launch our app and we will use this app like we usually do. So as a user, Sahara Desert looks interesting. I read the title description, looks good. I click on generate itinerary and I see this nice itinerary come up. The results are being streamed to me. I can read through this, take a look at all the different activities that plan. Okay, I'm going to stop recording. Now let's take a closer look at what we have here in the instruments. Okay, there are a few different tracks here and I'll explain what is going on in each of these to identify any potential bottlenecks that we can address. First track here is response. The blue bar here represents entire session. So this is ever since the user clicks on generate itinerary, we create a session and the model takes in the instructions, prompts and generates output. All of this is represented by this blue bar. The second row here is asset loading. Here if you take a closer look, you'll see that once the session starts, there is a little bit of a delay and then the models are loaded here, the model assets, which means all this time from the start of the session all the way to end of loading the model, the model is not generating any responses and roughly looks like this is about 700 milliseconds, which is almost a full second, right? And then if you look at the third track, this is where you see that the first token is generated, which means it waits for all the the models to be loaded and then it starts the token generation process, starting with the first token and continues to generate all the responses. So there is an opportunity for performance improvement here. If we could load these assets ahead of time, maybe we could start this generation process as soon as the session starts.

So that is one bottleneck we can try and address. The second bottleneck, if you look at the bottom here, I'm going to choose the inference section here. If you take a closer look, you will see here that there is max token count. And we see here that this currently amounts to 1044. And this token count includes everything we've added into the session. This includes your instructions, your prompts, your tools. It includes the generables with the itinerary, all of it. So it includes all of this here and we can see if there's an opportunity to reduce this because the number of tokens has an implication on the model's performance. So that is a second bottleneck that we can see if we can try and address.

Okay. If you recall, when we call Session.respond, the OS will load the model if it's not already in memory.

pre-warming can give your session a head start by loading the model before you even make a request. In our app, when someone taps on the landmark, it's pretty likely that they are going to make a request soon. We can pre-warm before they press the generate itinerary button to proactively load the model. By the time they finish reading the description, our model will be ready to go.

Let's also look at another optimization that can reduce request latency. Recall that generable structs provided to the model can help generate structured outputs, but this comes at the cost of increased token count, which affects initial processing time. Also recall that in Chapter 3, we passed an example itinerary called example trip to Japan. Since our instructions includes this full example of the generable schema, we can often exclude the schema definition itself from the front, which saves space and can speed up the model.

Thanks to Xcode instruments, we've identified the bottlenecks in our app. Now, we'll implement some optimizations directly in the app. First, we'll pre-warm the session by calling the pre-warm method when the user taps on the landmark. This does the framework to start loading the model before the user even asks for the itinerary. Second, because our one-shot example is quite detailed, the full schema definition in the prompt is redundant. We can remove it by setting include schema and prompt to false. In our stream response call, we'll make this change. This will significantly reduce our input token count. Let's head on over to our Code Along Guide and take a look at the code changes we'll be making. We're now in chapter 6, the app section. The first part is to pre-warm the model and the code changes will be reflected in the itinerary generator where we'll add a function to pre-warm and then in the view as well so that we can call the pre-warm method when the view is loaded. So let's go make these changes in itinerary generator and landmark trip view. Let's head on over to Xcode. I'll keep the instrument open because I do want to check the effect of these optimizations. So I'm going to go to Xcode, click on itinerary generator. This is already open for us. And I'm going to use the find navigator to open chapter 6.

Okay.

The first change we'll make to PREWARM is to add the PREWARM code here. We've defined this placeholder function called PREWARM model. So all I'm going to do here is call the pre-womp method in the session.

It's as easy as that.

Now we have a function that we can invoke from our view that will pre-womp the model. If you ahead of time know what the prompt is going to be, you can also use a prompt prefix to update a pre-womp method.

So inside a Session.prewarm function, there is an optional argument called prompt prefix where you can provide a prompt so the model has knowledge of the prompt that the user might provide and it can prewarm using this. So here we pass a prompt with a closure that says generate a three-day itinerary to landmark.name. This can further improve performance.

Okay, the next code change we need to make is actually in landmark trip view. So in our views folder, we have our landmark trip view.

Here, we need to update our task in order to call the pre-warm method when the model is actually loaded. So let's do this here.

Again, this is as simple as calling the generator.prewarmModel function that we just defined. So that includes all the code changes for prewarming the model. Let's head back to our Code Along Guide and take a look at the second optimization that we discussed, which is to reduce the max token count. So we're now in section 6.2, where we'll optimize the prompt.

The code change we'll need to make here is again in itinerary generator. So we'll include this additional argument called include schema in prompt and set it to false. Let's make this change and I'll briefly again explain what is going on.

So back to our itinerary generator.

Here we have our Session.stream response where we pass in the prompt, the generable, and options. So we'll also include our new argument called include schema in prompt and set it to false. Now what this tells the model is that we can exclude the schema of the itinerary that we pass because we are already passing the example trip to Japan in instruments, which includes the golden example along with the structure. So we can skip including the schema which will help in reducing the maximum token count. Because we made this change, I'm going to get rid of this comment too.

Okay, that concludes all the changes in chapter 6, which means we are now ready to profile the app once again. Okay, let's do this again. So I'm going to click on the profile option again and again this will build the app and launch up our profiler in a moment.

You see Xcode is building, launched up our profiler again. Now when I record it will relaunch the app and we'll go through the same process again of using the app. Click record. I have my app here. I'm going to follow the same exact steps. I'm going to click on Sahara Desert. I'll read the title. The description looks good to me. I want to generate the itinerary and I see the itinerary being generated. Looks good. We have our day-by-day plan, what restaurants to eat in, what hotels to stay. We'll let it finish executing and I'm going to stop profiling. Let's do the same thing we did previously and take a look at the output and see what effect our optimizations have had on the app. The very first thing you should notice is that asset loading happened well before the session started thanks to our pre-warm function. So we loaded this asset at this point when the user clicked on the detail view we called the pre-warm method by adding the pre-warm function in the task which means by the time the user used to read the title and description, the model was already loaded and ready. And if you take a closer look at the start of the session here, you'll see that the output starts generating almost as soon as the session started. So the session started because the model has already been loaded. It starts to prepare the vocabulary, it starts generating the tokens and your responses are now much quicker. Let's also take a look at the second optimization that we did and what effect it has had. So down here under inference you'll see the maximum token count has dropped to 700. Previously it was 1000 so we have dropped the maximum token count to 700 by excluding the schema from the prompt. Now this also means that the model is able to much quickly process the initial token and start generating responses a lot quicker. Awesome. So in this last chapter we looked at performance. We learned how to pre-warm the model to make our app feel more responsive and how to optimize a prompt by excluding the schema when it's not needed. These are two simple but effective ways to improve the performance of your generative features. Now let's take one final look at that app we've built together. Let's go back to Xcode and build and run. Okay, so this should look familiar to you as it's the app running on your machine. We started with this simple Swift list of landmarks and when we select Serengeti here we see this detail view. Now let's tap on generate itinerary one last time.

The UI builds itself in real time. That's the streaming API from chapter four using Session.stream response and partially generated content. In chapter two, we used add generable to get this rich structured response. And in chapter five, we use tool calls to find these points of interest, which the model intelligently decided to call to get this data.

Okay, we've covered a lot today from basic text generation to guided generation, streaming, tool calling, and performance optimizations, but there's still more to explore. We didn't have time to cover some advanced topics such as training custom model adapters, dynamic runtime schemas, or diving into guardrails and error handling. To learn more about these topics, I highly recommend watching other WWDC25 videos on the Foundation Models Framework.

Looking at Slido, there are a lot of great questions here and if we didn't get to yours, please bring them to the developer forums at developer.apple.com/forums where we can continue this discussion. The completed sample project from today, including few additional features is available for download in the Foundation Models Framework documentation. Finally, you'll receive a survey later today. We hope you enjoyed the session and we'd appreciate your feedback. With that, thank you so much for coding along with me. We'll see you again soon. Bye.
--- END FILE ---

--- FILE: known-issues.md ---
# Known issues

<!--
This source file is part of the Swift.org open source project

Copyright © 2023–2024 Apple Inc. and the Swift project authors
Licensed under Apache License v2.0 with Runtime Library Exception

See https://swift.org/LICENSE.txt for license information
See https://swift.org/CONTRIBUTORS.txt for Swift project authors
-->

Mark issues as known when running tests.

## Overview

The testing library provides several functions named `withKnownIssue()` that
you can use to mark issues as known. Use them to inform the testing library that
a test should not be marked as failing if only known issues are recorded.

### Mark an expectation failure as known

Consider a test function with a single expectation:

```swift
@Test func grillHeating() throws {
  var foodTruck = FoodTruck()
  try foodTruck.startGrill()
  #expect(foodTruck.grill.isHeating) // ❌ Expectation failed
}
```

If the value of the `isHeating` property is `false`, `#expect` will record an
issue. If you cannot fix the underlying problem, you can surround the failing
code in a closure passed to `withKnownIssue()`:

```swift
@Test func grillHeating() throws {
  var foodTruck = FoodTruck()
  try foodTruck.startGrill()
  withKnownIssue("Propane tank is empty") {
    #expect(foodTruck.grill.isHeating) // Known issue
  }
}
```

The issue recorded by `#expect` will then be considered "known" and the test
will not be marked as a failure. You may include an optional comment to explain
the problem or provide context.

### Mark a thrown error as known

If an `Error` is caught by the closure passed to `withKnownIssue()`, the issue
representing that caught error will be marked as known. Continuing the previous
example, suppose the problem is that the `startGrill()` function is throwing an
error. You can apply `withKnownIssue()` to this situation as well:

```swift
@Test func grillHeating() {
  var foodTruck = FoodTruck()
  withKnownIssue {
    try foodTruck.startGrill() // Known issue
    #expect(foodTruck.grill.isHeating)
  }
}
```

Because all errors thrown from the closure are caught and interpreted as known
issues, the `withKnownIssue()` function is not throwing. Consequently, any
subsequent code which depends on the throwing call having succeeded (such as the
`#expect` after `startGrill()`) must be included in the closure to avoid
additional issues.

- Note: `withKnownIssue()` is recommended instead of `#expect(throws:)` for any
  error which is considered a known issue so that the test status and results
  will reflect the situation more accurately.

### Match a specific issue

By default, `withKnownIssue()` considers all issues recorded while invoking the
body closure known. If multiple issues may be recorded, you can pass a trailing
closure labeled `matching:` which will be called once for each recorded issue
to determine whether it should be treated as known:

```swift
@Test func batteryLevel() throws {
  var foodTruck = FoodTruck()
  try withKnownIssue {
    let batteryLevel = try #require(foodTruck.batteryLevel) // Known
    #expect(batteryLevel >= 0.8) // Not considered known
  } matching: { issue in
    guard case .expectationFailed(let expectation) = issue.kind else {
      return false
    }
    return expectation.isRequired
  }
}
```

### Resolve a known issue

If there are no issues recorded while calling `function`, `withKnownIssue()`
will record a distinct issue about the lack of any issues having been recorded.
This notifies you that the underlying problem may have been resolved so that you
can investigate and consider removing `withKnownIssue()` if it's no longer
necessary.

### Handle a nondeterministic failure

If `withKnownIssue()` sometimes succeeds but other times records an issue
indicating there were no known issues, this may indicate a nondeterministic
failure or a "flaky" test.

The first step in resolving a nondeterministic test failure is to analyze the
code being tested and determine the source of the unpredictable behavior. If
you discover a bug such as a race condition, the ideal resolution is to fix
the underlying problem so that the code always behaves consistently even if
it continues to exhibit the known issue.

If the underlying problem only occurs in certain circumstances, consider
including a precondition. For example, if the grill only fails to heat when
there's no propane, you can pass a trailing closure labeled `when:` which
determines whether issues recorded in the body closure should be considered
known:

```swift
@Test func grillHeating() throws {
  var foodTruck = FoodTruck()
  try foodTruck.startGrill()
  withKnownIssue {
    // Only considered known when hasPropane == false
    #expect(foodTruck.grill.isHeating)
  } when: {
    !hasPropane
  }
}
```

If the underlying problem is unpredictable and fails at random, you can pass
`isIntermittent: true` to let the testing library know that it will not always
occur. Then, the testing library will not record an issue when zero known issues
are recorded:

```swift
@Test func grillHeating() throws {
  var foodTruck = FoodTruck()
  try foodTruck.startGrill()
  withKnownIssue(isIntermittent: true) {
    #expect(foodTruck.grill.isHeating)
  }
}
```

## Topics

### Recording known issues in tests

- ``withKnownIssue(_:isIntermittent:sourceLocation:_:)``
- ``withKnownIssue(_:isIntermittent:isolation:sourceLocation:_:)``
- ``withKnownIssue(_:isIntermittent:sourceLocation:_:when:matching:)``
- ``withKnownIssue(_:isIntermittent:isolation:sourceLocation:_:when:matching:)``
- ``KnownIssueMatcher``

### Describing a failure or warning

- ``Issue``
--- END FILE ---

--- FILE: OrganizingTests.md ---
# Organizing test functions with suite types

<!--
This source file is part of the Swift.org open source project

Copyright (c) 2023-2024 Apple Inc. and the Swift project authors
Licensed under Apache License v2.0 with Runtime Library Exception

See https://swift.org/LICENSE.txt for license information
See https://swift.org/CONTRIBUTORS.txt for Swift project authors
-->

Organize tests into test suites.

## Overview

When working with a large selection of test functions, it can be helpful to
organize them into test suites.

A test function can be added to a test suite in one of two ways:

@Comment{ * By placing it in the same file as other test functions. }
* By placing it in a Swift type.
* By placing it in a Swift type and annotating that type with the `@Suite`
   attribute.

The `@Suite` attribute isn't required for the testing library to recognize that
a type contains test functions, but adding it allows customization of a test
suite's appearance in the IDE and at the command line. If a trait such as
``Trait/tags(_:)`` or ``Trait/disabled(_:sourceLocation:)`` is applied to a test
suite, it's automatically inherited by the tests contained in the suite.

In addition to containing test functions and any other members that a Swift type
might contain, test suite types can also contain additional test suites nested
within them. To add a nested test suite type, simply declare an additional type
within the scope of the outer test suite type.

By default, tests contained within a suite run in parallel with each other.
For more information about test parallelization, see <doc:Parallelization>.

### Customize a suite's name

To customize a test suite's name, supply a string literal as an argument to the
`@Suite` attribute:

```swift
@Suite("Food truck tests") struct FoodTruckTests {
  @Test func foodTruckExists() { ... }
}
```

To further customize the appearance and behavior of a test function, use
[traits](doc:Traits) such as ``Trait/tags(_:)``.

## Test functions in test suite types

If a type contains a test function declared as an instance method (that is,
without either the `static` or `class` keyword), the testing library calls
that test function at runtime by initializing an instance of the type, then
calling the test function on that instance. If a test suite type contains
multiple test functions declared as instance methods, each one is called on a
distinct instance of the type. Therefore, the following test suite and test
function:

```swift
@Suite struct FoodTruckTests {
  @Test func foodTruckExists() { ... }
}
```

Are equivalent to:

```swift
@Suite struct FoodTruckTests {
  func foodTruckExists() { ... }

  @Test static func staticFoodTruckExists() {
    let instance = FoodTruckTests()
    instance.foodTruckExists()
  }
}
```

### Constraints on test suite types

When using a type as a test suite, it's subject to some constraints that are
not otherwise applied to Swift types.

#### An initializer may be required

If a type contains test functions declared as instance methods, it must be
possible to initialize an instance of the type with a zero-argument initializer.
The initializer may be any combination of:

- implicit or explicit
- synchronous or asynchronous
- throwing or non-throwing
- `private`, `fileprivate`, `internal`, `package`, or `public`

For example:

```swift
@Suite struct FoodTruckTests {
  var batteryLevel = 100

  @Test func foodTruckExists() { ... } // ✅ OK: The type has an implicit init().
}

@Suite struct CashRegisterTests {
  private init(cashOnHand: Decimal = 0.0) async throws { ... }

  @Test func calculateSalesTax() { ... } // ✅ OK: The type has a callable init().
}

struct MenuTests {
  var foods: [Food]
  var prices: [Food: Decimal]

  @Test static func specialOfTheDay() { ... } // ✅ OK: The function is static.
  @Test func orderAllFoods() { ... } // ❌ ERROR: The suite type requires init().
}
```

The compiler emits an error when presented with a test suite that doesn't
meet this requirement.

#### Test suite types must always be available

Although `@available` can be applied to a test function to limit its
availability at runtime, a test suite type (and any types that contain it) must
_not_ be annotated with the `@available` attribute:

```swift
@Suite struct FoodTruckTests { ... } // ✅ OK: The type is always available.

@available(macOS 11.0, *) // ❌ ERROR: The suite type must always be available.
@Suite struct CashRegisterTests { ... }

@available(macOS 11.0, *) struct MenuItemTests { // ❌ ERROR: The suite type's
                                                 // containing type must always
                                                 // be available too.
  @Suite struct BurgerTests { ... }
}
```

The compiler emits an error when presented with a test suite that doesn't
meet this requirement.
--- END FILE ---

--- FILE: MigratingFromXCTest.md ---
# Migrating a test from XCTest

<!--
This source file is part of the Swift.org open source project

Copyright (c) 2023-2024 Apple Inc. and the Swift project authors
Licensed under Apache License v2.0 with Runtime Library Exception

See https://swift.org/LICENSE.txt for license information
See https://swift.org/CONTRIBUTORS.txt for Swift project authors
-->

<!-- NOTE: The voice of this document is directed at the second person ("you")
because it provides instructions the reader must follow directly. -->

Migrate an existing test method or test class written using XCTest.

## Overview

The testing library provides much of the same functionality of XCTest, but uses
its own syntax to declare test functions and types. Here, you'll learn how to
convert XCTest-based content to use the testing library instead.

### Import the testing library

XCTest and the testing library are available from different modules. Instead of 
importing the XCTest module, import the Testing module:

@Row {
  @Column {
    ```swift
    // Before
    import XCTest
    ```
  }
  @Column {
    ```swift
    // After
    import Testing
    ```
  }
}

A single source file can contain tests written with XCTest as well as other 
tests written with the testing library. Import both XCTest and Testing if a 
source file contains mixed test content.

### Convert test classes

XCTest groups related sets of test methods in test classes: classes that inherit
from the [`XCTestCase`](https://developer.apple.com/documentation/xctest/xctestcase)
class provided by the [XCTest](https://developer.apple.com/documentation/xctest) framework. The testing library doesn't require
that test functions be instance members of types. Instead, they can be _free_ or
_global_ functions, or can be `static` or `class` members of a type.

If you want to group your test functions together, you can do so by placing them
in a Swift type. The testing library refers to such a type as a _suite_. These
types do _not_ need to be classes, and they don't inherit from `XCTestCase`.

To convert a subclass of `XCTestCase` to a suite, remove the `XCTestCase`
conformance. It's also generally recommended that a Swift structure or actor be
used instead of a class because it allows the Swift compiler to better-enforce
concurrency safety:

@Row {
  @Column {
    ```swift
    // Before
    class FoodTruckTests: XCTestCase {
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    struct FoodTruckTests {
      ...
    }
    ```
  }
}

For more information about suites and how to declare and customize them, see
<doc:OrganizingTests>.

### Convert setup and teardown functions

In XCTest, code can be scheduled to run before and after a test using the
[`setUp()`](https://developer.apple.com/documentation/xctest/xctest/3856481-setup)
and [`tearDown()`](https://developer.apple.com/documentation/xctest/xctest/3856482-teardown)
family of functions. When writing tests using the testing library, implement
`init()` and/or `deinit` instead:

@Row {
  @Column {
    ```swift
    // Before
    class FoodTruckTests: XCTestCase {
      var batteryLevel: NSNumber!
      override func setUp() async throws {
        batteryLevel = 100
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    struct FoodTruckTests {
      var batteryLevel: NSNumber
      init() async throws {
        batteryLevel = 100
      }
      ...
    }
    ```
  }
}

The use of `async` and `throws` is optional. If teardown is needed, declare your
test suite as a class or as an actor rather than as a structure and implement
`deinit`:

@Row {
  @Column {
    ```swift
    // Before
    class FoodTruckTests: XCTestCase {
      var batteryLevel: NSNumber!
      override func setUp() async throws {
        batteryLevel = 100
      }
      override func tearDown() {
        batteryLevel = 0 // drain the battery
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    final class FoodTruckTests {
      var batteryLevel: NSNumber
      init() async throws {
        batteryLevel = 100
      }
      deinit {
        batteryLevel = 0 // drain the battery
      }
      ...
    }
    ```
  }
}

<!--
- Bug: `deinit` cannot be asynchronous or throwing, unlike `tearDown()`.
  ((103616215)[rdar://103616215])
-->

### Convert test methods

The testing library represents individual tests as functions, similar to how
they are represented in XCTest. However, the syntax for declaring a test
function is different. In XCTest, a test method must be a member of a test class
and its name must start with `test`. The testing library doesn't require a test
function to have any particular name. Instead, it identifies a test function by
the presence of the `@Test` attribute:

@Row {
  @Column {
    ```swift
    // Before
    class FoodTruckTests: XCTestCase {
      func testEngineWorks() { ... }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    struct FoodTruckTests {
      @Test func engineWorks() { ... }
      ...
    }
    ```
  }
}

As with XCTest, the testing library allows test functions to be marked `async`,
`throws`, or `async`-`throws`, and to be isolated to a global actor (for example, by using the
`@MainActor` attribute.)

- Note: XCTest runs synchronous test methods on the main actor by default, while
  the testing library runs all test functions on an arbitrary task. If a test
  function must run on the main thread, isolate it to the main actor with
  `@MainActor`, or run the thread-sensitive code inside a call to
  [`MainActor.run(resultType:body:)`](https://developer.apple.com/documentation/swift/mainactor/run(resulttype:body:)).

For more information about test functions and how to declare and customize them,
see <doc:DefiningTests>.

### Check for expected values and outcomes 

XCTest uses a family of approximately 40 functions to assert test requirements.
These functions are collectively referred to as
[`XCTAssert()`](https://developer.apple.com/documentation/xctest/1500669-xctassert).
The testing library has two replacements, ``expect(_:_:sourceLocation:)`` and
``require(_:_:sourceLocation:)-5l63q``. They both behave similarly to
`XCTAssert()` except that ``require(_:_:sourceLocation:)-5l63q`` throws an
error if its condition isn't met:

@Row {
  @Column {
    ```swift
    // Before
    func testEngineWorks() throws {
      let engine = FoodTruck.shared.engine
      XCTAssertNotNil(engine.parts.first)
      XCTAssertGreaterThan(engine.batteryLevel, 0)
      try engine.start()
      XCTAssertTrue(engine.isRunning)
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func engineWorks() throws {
      let engine = FoodTruck.shared.engine
      try #require(engine.parts.first != nil)
      #expect(engine.batteryLevel > 0)
      try engine.start()
      #expect(engine.isRunning)
    }
    ```
  }
}

### Check for optional values

XCTest also has a function, [`XCTUnwrap()`](https://developer.apple.com/documentation/xctest/3380195-xctunwrap),
that tests if an optional value is `nil` and throws an error if it is. When
using the testing library, you can use ``require(_:_:sourceLocation:)-6w9oo``
with optional expressions to unwrap them:

@Row {
  @Column {
    ```swift
    // Before
    func testEngineWorks() throws {
      let engine = FoodTruck.shared.engine
      let part = try XCTUnwrap(engine.parts.first)
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func engineWorks() throws {
      let engine = FoodTruck.shared.engine
      let part = try #require(engine.parts.first)
      ...
    }
    ```
  }
}

### Record issues

XCTest has a function, [`XCTFail()`](https://developer.apple.com/documentation/xctest/1500970-xctfail),
that causes a test to fail immediately and unconditionally. This function is
useful when the syntax of the language prevents the use of an `XCTAssert()`
function. To record an unconditional issue using the testing library, use the
``Issue/record(_:severity:sourceLocation:)`` function:

@Row {
  @Column {
    ```swift
    // Before
    func testEngineWorks() {
      let engine = FoodTruck.shared.engine
      guard case .electric = engine else {
        XCTFail("Engine is not electric")
        return
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func engineWorks() {
      let engine = FoodTruck.shared.engine
      guard case .electric = engine else {
        Issue.record("Engine is not electric")
        return
      }
      ...
    }
    ```
  }
}

The following table includes a list of the various `XCTAssert()` functions and
their equivalents in the testing library:

| XCTest | Swift Testing |
|-|-|
| `XCTAssert(x)`, `XCTAssertTrue(x)` | `#expect(x)` |
| `XCTAssertFalse(x)` | `#expect(!x)` |
| `XCTAssertNil(x)` | `#expect(x == nil)` |
| `XCTAssertNotNil(x)` | `#expect(x != nil)` |
| `XCTAssertEqual(x, y)` | `#expect(x == y)` |
| `XCTAssertNotEqual(x, y)` | `#expect(x != y)` |
| `XCTAssertIdentical(x, y)` | `#expect(x === y)` |
| `XCTAssertNotIdentical(x, y)` | `#expect(x !== y)` |
| `XCTAssertGreaterThan(x, y)` | `#expect(x > y)` |
| `XCTAssertGreaterThanOrEqual(x, y)` | `#expect(x >= y)` |
| `XCTAssertLessThanOrEqual(x, y)` | `#expect(x <= y)` |
| `XCTAssertLessThan(x, y)` | `#expect(x < y)` |
| `XCTAssertThrowsError(try f())` | `#expect(throws: (any Error).self) { try f() }` |
| `XCTAssertThrowsError(try f()) { error in … }` | `let error = #expect(throws: (any Error).self) { try f() }` |
| `XCTAssertNoThrow(try f())` | `#expect(throws: Never.self) { try f() }` |
| `try XCTUnwrap(x)` | `try #require(x)` |
| `XCTFail("…")` | `Issue.record("…")` |

The testing library doesn’t provide an equivalent of
[`XCTAssertEqual(_:_:accuracy:_:file:line:)`](https://developer.apple.com/documentation/xctest/3551607-xctassertequal).
To compare two numeric values within a specified accuracy, 
use `isApproximatelyEqual()` from [swift-numerics](https://github.com/apple/swift-numerics).

### Continue or halt after test failures

An instance of an `XCTestCase` subclass can set its
[`continueAfterFailure`](https://developer.apple.com/documentation/xctest/xctestcase/1496260-continueafterfailure)
property to `false` to cause a test to stop running after a failure occurs.
XCTest stops an affected test by throwing an Objective-C exception at the
time the failure occurs.

- Note: `continueAfterFailure` isn't fully supported when using the
  [swift-corelibs-xctest](https://github.com/swiftlang/swift-corelibs-xctest)
  library on non-Apple platforms.

The behavior of an exception thrown through a Swift stack frame is undefined. If
an exception is thrown through an `async` Swift function, it typically causes
the process to terminate abnormally, preventing other tests from running.

The testing library doesn't use exceptions to stop test functions. Instead, use
the ``require(_:_:sourceLocation:)-5l63q`` macro, which throws a Swift error on
failure:

@Row {
  @Column {
    ```swift
    // Before
    func testTruck() async {
      continueAfterFailure = false
      XCTAssertTrue(FoodTruck.shared.isLicensed)
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func truck() throws {
      try #require(FoodTruck.shared.isLicensed)
      ...
    }
    ```
  }
}

When using either `continueAfterFailure` or
``require(_:_:sourceLocation:)-5l63q``, other tests will continue to run after
the failed test method or test function.

### Validate asynchronous behaviors

XCTest has a class, [`XCTestExpectation`](https://developer.apple.com/documentation/xctest/xctestexpectation),
that represents some asynchronous condition. You create an instance of
this class (or a subclass like [`XCTKeyPathExpectation`](https://developer.apple.com/documentation/xctest/xctkeypathexpectation))
using an initializer or a convenience method on `XCTestCase`. When the condition
represented by an expectation occurs, the developer _fulfills_ the expectation.
Concurrently, the developer _waits for_ the expectation to be fulfilled using an
instance of [`XCTWaiter`](https://developer.apple.com/documentation/xctest/xctwaiter)
or using a convenience method on `XCTestCase`.

Wherever possible, prefer to use Swift concurrency to validate asynchronous
conditions. For example, if it's necessary to determine the result of an
asynchronous Swift function, it can be awaited with `await`. For a function that
takes a completion handler but which doesn't use `await`, a Swift
[continuation](https://developer.apple.com/documentation/swift/withcheckedcontinuation(isolation:function:_:))
can be used to convert the call into an `async`-compatible one.

Some tests, especially those that test asynchronously-delivered events, cannot
be readily converted to use Swift concurrency. The testing library offers
functionality called _confirmations_ which can be used to implement these tests.
Instances of ``Confirmation`` are created and used within the scope of the
functions ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-5mqz2``
and ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-l3il``.

Confirmations function similarly to the expectations API of XCTest, however,
they don't block or suspend the caller while waiting for a condition to be
fulfilled. Instead, the requirement is expected to be _confirmed_ (the
equivalent of _fulfilling_ an expectation) before `confirmation()` returns, and
records an issue otherwise:

@Row {
  @Column {
    ```swift
    // Before
    func testTruckEvents() async {
      let soldFood = expectation(description: "…")
      FoodTruck.shared.eventHandler = { event in
        if case .soldFood = event {
          soldFood.fulfill()
        }
      }
      await Customer().buy(.soup)
      await fulfillment(of: [soldFood])
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func truckEvents() async {
      await confirmation("…") { soldFood in
        FoodTruck.shared.eventHandler = { event in
          if case .soldFood = event {
            soldFood()
          }
        }
        await Customer().buy(.soup)
      }
      ...
    }
    ```
  }
}

By default, `XCTestExpectation` expects to be fulfilled exactly once, and will
record an issue in the current test if it is not fulfilled or if it is fulfilled
more than once. `Confirmation` behaves the same way and expects to be confirmed
exactly once by default. You can configure the number of times an expectation
should be fulfilled by setting its [`expectedFulfillmentCount`](https://developer.apple.com/documentation/xctest/xctestexpectation/2806572-expectedfulfillmentcount)
property, and you can pass a value for the `expectedCount` argument of
``confirmation(_:expectedCount:isolation:sourceLocation:_:)-5mqz2`` for the same
purpose.

`XCTestExpectation` has a property, [`assertForOverFulfill`](https://developer.apple.com/documentation/xctest/xctestexpectation/2806575-assertforoverfulfill),
which when set to `false` allows an expectation to be fulfilled more times than
expected without causing a test failure. When using a confirmation, you can pass
a range to ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-l3il`` as
its expected count to indicate that it must be confirmed _at least_ some number
of times:

@Row {
  @Column {
    ```swift
    // Before
    func testRegularCustomerOrders() async {
      let soldFood = expectation(description: "…")
      soldFood.expectedFulfillmentCount = 10
      soldFood.assertForOverFulfill = false
      FoodTruck.shared.eventHandler = { event in
        if case .soldFood = event {
          soldFood.fulfill()
        }
      }
      for customer in regularCustomers() {
        await customer.buy(customer.regularOrder)
      }
      await fulfillment(of: [soldFood])
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func regularCustomerOrders() async {
      await confirmation(
        "…",
        expectedCount: 10...
      ) { soldFood in
        FoodTruck.shared.eventHandler = { event in
          if case .soldFood = event {
            soldFood()
          }
        }
        for customer in regularCustomers() {
          await customer.buy(customer.regularOrder)
        }
      }
      ...
    }
    ```
  }
}

Any range expression with a lower bound (that is, whose type conforms to
both [`RangeExpression<Int>`](https://developer.apple.com/documentation/swift/rangeexpression)
and [`Sequence<Int>`](https://developer.apple.com/documentation/swift/sequence))
can be used with ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-l3il``.
You must specify a lower bound for the number of confirmations because, without
one, the testing library cannot tell if an issue should be recorded when there
have been zero confirmations. 

### Control whether a test runs

When using XCTest, the [`XCTSkip`](https://developer.apple.com/documentation/xctest/xctskip)
error type can be thrown to bypass the remainder of a test function. As well,
the [`XCTSkipIf()`](https://developer.apple.com/documentation/xctest/3521325-xctskipif)
and [`XCTSkipUnless()`](https://developer.apple.com/documentation/xctest/3521326-xctskipunless)
functions can be used to conditionalize the same action. The testing library
allows developers to skip a test function or an entire test suite before it
starts running using the ``ConditionTrait`` trait type. Annotate a test suite or
test function with an instance of this trait type to control whether it runs:

@Row {
  @Column {
    ```swift
    // Before
    class FoodTruckTests: XCTestCase {
      func testArepasAreTasty() throws {
        try XCTSkipIf(CashRegister.isEmpty)
        try XCTSkipUnless(FoodTruck.sells(.arepas))
        ...
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Suite(.disabled(if: CashRegister.isEmpty))
    struct FoodTruckTests {
      @Test(.enabled(if: FoodTruck.sells(.arepas)))
      func arepasAreTasty() {
        ...
      }
      ...
    }
    ```
  }
}

<!-- TODO: document Test.cancel() and Test.Case.cancel() here, and update
     relevant links to use proper DocC symbol references.

If a test has already started running and you determine it cannot complete and
should end early without failing, use `Test/cancel(_:sourceLocation:)` instead
of [`XCTSkip`](https://developer.apple.com/documentation/xctest/xctskip) to
cancel the task associated with the current test:

@Row {
  @Column {
    ```swift
    // Before
    func testCashRegister() throws {
      let cashRegister = CashRegister()
      let drawer = cashRegister.open()
      if drawer.isEmpty {
        throw XCTSkip("Cash register is empty")
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func cashRegister() throws {
      let cashRegister = CashRegister()
      let drawer = cashRegister.open()
      if drawer.isEmpty {
        try Test.cancel("Cash register is empty")
      }
      ...
    }
    ```
  }
}

If the test is parameterized and you only want to cancel the current test case
rather than the entire test, use `Test/Case/cancel(_:sourceLocation:)`.
-->

### Annotate known issues

A test may have a known issue that sometimes or always prevents it from passing.
When written using XCTest, such tests can call
[`XCTExpectFailure(_:options:failingBlock:)`](https://developer.apple.com/documentation/xctest/3727246-xctexpectfailure)
to tell XCTest and its infrastructure that the issue shouldn't cause the test
to fail. The testing library has an equivalent function with synchronous and
asynchronous variants:

- ``withKnownIssue(_:isIntermittent:sourceLocation:_:)``
- ``withKnownIssue(_:isIntermittent:isolation:sourceLocation:_:)``

This function can be used to annotate a section of a test as having a known
issue:

@Row {
  @Column {
    ```swift
    // Before
    func testGrillWorks() async {
      XCTExpectFailure("Grill is out of fuel") {
        try FoodTruck.shared.grill.start()
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func grillWorks() async {
      withKnownIssue("Grill is out of fuel") {
        try FoodTruck.shared.grill.start()
      }
      ...
    }
    ```
  }
}

- Note: The XCTest function [`XCTExpectFailure(_:options:)`](https://developer.apple.com/documentation/xctest/3727245-xctexpectfailure),
  which doesn't take a closure and which affects the remainder of the test,
  doesn't have a direct equivalent in the testing library. To mark an entire
  test as having a known issue, wrap its body in a call to `withKnownIssue()`. 

If a test may fail intermittently, the call to
`XCTExpectFailure(_:options:failingBlock:)` can be marked _non-strict_. When
using the testing library, specify that the known issue is _intermittent_
instead:

@Row {
  @Column {
    ```swift
    // Before
    func testGrillWorks() async {
      XCTExpectFailure(
        "Grill may need fuel",
        options: .nonStrict()
      ) {
        try FoodTruck.shared.grill.start()
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func grillWorks() async {
      withKnownIssue(
        "Grill may need fuel", 
        isIntermittent: true
      ) {
        try FoodTruck.shared.grill.start()
      }
      ...
    }
    ```
  }
}

Additional options can be specified when calling `XCTExpectFailure()`:

- [`isEnabled`](https://developer.apple.com/documentation/xctest/xctexpectedfailure/options/3726085-isenabled)
  can be set to `false` to skip known-issue matching (for instance, if a
  particular issue only occurs under certain conditions)
- [`issueMatcher`](https://developer.apple.com/documentation/xctest/xctexpectedfailure/options/3726086-issuematcher)
  can be set to a closure to allow marking only certain issues as known and to
  allow other issues to be recorded as test failures

The testing library includes overloads of `withKnownIssue()` that take
additional arguments with similar behavior:

- ``withKnownIssue(_:isIntermittent:sourceLocation:_:when:matching:)``
- ``withKnownIssue(_:isIntermittent:isolation:sourceLocation:_:when:matching:)``

To conditionally enable known-issue matching or to match only certain kinds
of issues:

@Row {
  @Column {
    ```swift
    // Before
    func testGrillWorks() async {
      let options = XCTExpectedFailure.Options()
      options.isEnabled = FoodTruck.shared.hasGrill
      options.issueMatcher = { issue in
        issue.type == thrownError
      }
      XCTExpectFailure(
        "Grill is out of fuel",
        options: options
      ) {
        try FoodTruck.shared.grill.start()
      }
      ...
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Test func grillWorks() async {
      withKnownIssue("Grill is out of fuel") {
        try FoodTruck.shared.grill.start()
      } when: {
        FoodTruck.shared.hasGrill
      } matching: { issue in
        issue.error != nil 
      }
      ...
    }
    ```
  }
}

### Run tests sequentially

By default, the testing library runs all tests in a suite in parallel. The
default behavior of XCTest is to run each test in a suite sequentially. If your
tests use shared state such as global variables, you may see unexpected
behavior including unreliable test outcomes when you run tests in parallel.

Annotate your test suite with ``Trait/serialized`` to run tests within that
suite serially:

@Row {
  @Column {
    ```swift
    // Before
    class RefrigeratorTests : XCTestCase {
      func testLightComesOn() throws {
        try FoodTruck.shared.refrigerator.openDoor()
        XCTAssertEqual(FoodTruck.shared.refrigerator.lightState, .on)
      }
      
      func testLightGoesOut() throws {
        try FoodTruck.shared.refrigerator.openDoor()
        try FoodTruck.shared.refrigerator.closeDoor()
        XCTAssertEqual(FoodTruck.shared.refrigerator.lightState, .off)
      }
    }
    ```
  }
  @Column {
    ```swift
    // After
    @Suite(.serialized)
    class RefrigeratorTests {
      @Test func lightComesOn() throws {
        try FoodTruck.shared.refrigerator.openDoor()
        #expect(FoodTruck.shared.refrigerator.lightState == .on)
      }
      
      @Test func lightGoesOut() throws {
        try FoodTruck.shared.refrigerator.openDoor()
        try FoodTruck.shared.refrigerator.closeDoor()
        #expect(FoodTruck.shared.refrigerator.lightState == .off)
      }
    }
    ```
  }
}

For more information, see <doc:Parallelization>.

### Attach values

In XCTest, you can create an instance of [`XCTAttachment`](https://developer.apple.com/documentation/xctest/xctattachment)
representing arbitrary data, files, property lists, encodable objects, images,
and other types of information that would be useful to have available if a test
fails. Swift Testing has an ``Attachment`` type that serves much the same
purpose.

To attach a value from a test to the output of a test run, that value must
conform to the ``Attachable`` protocol. The testing library provides default
conformances for various standard library and Foundation types.

If you want to attach a value of another type, and that type already conforms to
[`Encodable`](https://developer.apple.com/documentation/swift/encodable) or to
[`NSSecureCoding`](https://developer.apple.com/documentation/foundation/nssecurecoding),
the testing library automatically provides a default implementation when you
import Foundation:

@Row {
  @Column {
    ```swift
    // Before
    import Foundation

    class Tortilla: NSSecureCoding { /* ... */ }

    func testTortillaIntegrity() async {
      let tortilla = Tortilla(diameter: .large)
      ...
      let attachment = XCTAttachment(
        archivableObject: tortilla
      )
      self.add(attachment)
    }
    ```
  }
  @Column {
    ```swift
    // After
    import Foundation

    struct Tortilla: Codable, Attachable { /* ... */ }

    @Test func tortillaIntegrity() async {
      let tortilla = Tortilla(diameter: .large)
      ...
      Attachment.record(tortilla)
    }
    ```
  }
}

If you have a type that does not (or cannot) conform to `Encodable` or
`NSSecureCoding`, or if you want fine-grained control over how it is serialized
when attaching it to a test, you can provide your own implementation of
``Attachable/withUnsafeBytes(for:_:)``.

<!-- NOTE: not discussing attaching to activities here since there is not yet an
equivalent interface in Swift Testing. -->

## See Also

- <doc:DefiningTests>
- <doc:OrganizingTests>
- <doc:Expectations>
- <doc:known-issues>
--- END FILE ---

--- FILE: DefiningTests.md ---
# Defining test functions

<!--
This source file is part of the Swift.org open source project

Copyright (c) 2023 Apple Inc. and the Swift project authors
Licensed under Apache License v2.0 with Runtime Library Exception

See https://swift.org/LICENSE.txt for license information
See https://swift.org/CONTRIBUTORS.txt for Swift project authors
-->

Define a test function to validate that code is working correctly.

## Overview

Defining a test function for a Swift package or project is straightforward.

### Import the testing library

To import the testing library, add the following to the Swift source file that
contains the test:

```swift
import Testing
```

- Note: Only import the testing library into a test target or library meant for
  test targets. Importing the testing library into a target intended for
  distribution such as an application, app library, or executable target isn't
  supported or recommended. Test functions aren't stripped from binaries when
  building for release, so logic and fixtures of a test may be visible to anyone
  who inspects a build product that contains a test function.

### Declare a test function

To declare a test function, write a Swift function declaration that doesn't
take any arguments, then prefix its name with the `@Test` attribute:

```swift
@Test func foodTruckExists() {
  // Test logic goes here.
}
```

This test function can be present at file scope or within a type. A type
containing test functions is automatically a _test suite_ and can be optionally
annotated with the `@Suite` attribute. For more information about suites, see
<doc:OrganizingTests>.

Note that, while this function is a valid test function, it doesn't actually
perform any action or test any code. To check for expected values and outcomes
in test functions, add [expectations](doc:Expectations) to the test function.

### Customize a test's name

To customize a test function's name as presented in an IDE or at the command
line, supply a string literal as an argument to the `@Test` attribute:

```swift
@Test("Food truck exists") func foodTruckExists() { ... }
```

To further customize the appearance and behavior of a test function, use
[traits](doc:Traits) such as ``Trait/tags(_:)``.

### Write concurrent or throwing tests

As with other Swift functions, test functions can be marked `async` and `throws`
to annotate them as concurrent or throwing, respectively. If a test is only safe
to run in the main actor's execution context (that is, from the main thread of
the process), it can be annotated `@MainActor`:

```swift
@Test @MainActor func foodTruckExists() async throws { ... }
```

### Limit the availability of a test

If a test function can only run on newer versions of an operating system or of
the Swift language, use the `@available` attribute when declaring it. Use the
`message` argument of the `@available` attribute to specify a message to log if
a test is unable to run due to limited availability:

```swift
@available(macOS 11.0, *)
@available(swift, introduced: 8.0, message: "Requires Swift 8.0 features to run")
@Test func foodTruckExists() { ... }
```
--- END FILE ---

--- FILE: Expectations.md ---
# Expectations and confirmations

<!--
This source file is part of the Swift.org open source project

Copyright (c) 2023–2024 Apple Inc. and the Swift project authors
Licensed under Apache License v2.0 with Runtime Library Exception

See https://swift.org/LICENSE.txt for license information
See https://swift.org/CONTRIBUTORS.txt for Swift project authors
-->

Check for expected values, outcomes, and asynchronous events in tests.

## Overview

Use ``expect(_:_:sourceLocation:)`` and
``require(_:_:sourceLocation:)-5l63q`` macros to validate expected
outcomes. To validate that an error is thrown, or _not_ thrown, the
testing library provides several overloads of the macros that you can
use. For more information, see <doc:testing-for-errors-in-swift-code>.

Use a ``Confirmation`` to confirm the occurrence of an
asynchronous event that you can't check directly using an expectation.
For more information, see <doc:testing-asynchronous-code>.

### Validate your code's result

To validate that your code produces an expected value, use
``expect(_:_:sourceLocation:)``. This macro captures the
expression you pass, and provides detailed information when the code doesn't
satisfy the expectation.

```swift
@Test func calculatingOrderTotal() {
  let calculator = OrderCalculator()
  #expect(calculator.total(of: [3, 3]) == 7)
  // Prints "Expectation failed: (calculator.total(of: [3, 3]) → 6) == 7"
}
```

Your test keeps running after ``expect(_:_:sourceLocation:)`` fails. To stop
the test when the code doesn't satisfy a requirement, use
``require(_:_:sourceLocation:)-5l63q`` instead:

```swift
@Test func returningCustomerRemembersUsualOrder() throws {
  let customer = try #require(Customer(id: 123))
  // The test runner doesn't reach this line if the customer is nil.
  #expect(customer.usualOrder.countOfItems == 2)
}
```

``require(_:_:sourceLocation:)-5l63q`` throws an instance of
``ExpectationFailedError`` when your code fails to satisfy the requirement.

## Topics

### Checking expectations

- ``expect(_:_:sourceLocation:)``
- ``require(_:_:sourceLocation:)-5l63q``
- ``require(_:_:sourceLocation:)-6w9oo``

### Checking that errors are thrown

- <doc:testing-for-errors-in-swift-code>
- ``expect(throws:_:sourceLocation:performing:)-1hfms``
- ``expect(throws:_:sourceLocation:performing:)-7du1h``
- ``expect(_:sourceLocation:performing:throws:)``
- ``require(throws:_:sourceLocation:performing:)-7n34r``
- ``require(throws:_:sourceLocation:performing:)-4djuw``
- ``require(_:sourceLocation:performing:throws:)``

### Checking how processes exit

- <doc:exit-testing>
- ``expect(processExitsWith:observing:_:sourceLocation:performing:)``
- ``require(processExitsWith:observing:_:sourceLocation:performing:)``
- ``ExitStatus``
- ``ExitTest``

### Confirming that asynchronous events occur

- <doc:testing-asynchronous-code>
- ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-5mqz2``
- ``confirmation(_:expectedCount:isolation:sourceLocation:_:)-l3il``
- ``Confirmation``

### Retrieving information about checked expectations

- ``Expectation``
- ``ExpectationFailedError``
- ``CustomTestStringConvertible``

### Representing source locations

- ``SourceLocation``
<!-- - ``_sourceLocation()`` -->
<!-- - ``SourceContext`` -->
<!-- - ``Backtrace`` -->
--- END FILE ---

--- FILE: Adding-Tests-to-Your-Xcode-Project.md ---
# Adding Tests to Your Xcode Project

This comprehensive guide covers how to add and configure tests in your Xcode project, including both XCTest and Swift Testing frameworks.

## Overview

Testing is a crucial part of software development that helps ensure your code works correctly and continues to work as you make changes. Xcode provides powerful testing tools that integrate seamlessly with your development workflow.

## Creating Test Targets

### Adding a Test Target

1. **Open your Xcode project**
2. **Select your project** in the navigator
3. **Click the "+" button** at the bottom of the target list
4. **Choose "Unit Testing Bundle"** from the template list
5. **Configure the target**:
   - **Product Name**: Choose a descriptive name (e.g., "MyAppTests")
   - **Target to be Tested**: Select your main app target
   - **Language**: Choose Swift or Objective-C
   - **Use Core Data**: Check if your app uses Core Data

### Test Target Configuration

```swift
// Package.swift example for Swift Package
let package = Package(
    name: "MyPackage",
    products: [
        .library(name: "MyPackage", targets: ["MyPackage"]),
    ],
    targets: [
        .target(name: "MyPackage"),
        .testTarget(
            name: "MyPackageTests",
            dependencies: ["MyPackage"]
        ),
    ]
)
```

## Test Types

### Unit Tests

Unit tests verify that individual components of your app work correctly in isolation.

```swift
import XCTest
@testable import MyApp

class UserManagerTests: XCTestCase {
    var userManager: UserManager!
    
    override func setUp() {
        super.setUp()
        userManager = UserManager()
    }
    
    override func tearDown() {
        userManager = nil
        super.tearDown()
    }
    
    func testUserCreation() {
        // Test user creation logic
        let user = userManager.createUser(name: "John", email: "john@example.com")
        XCTAssertNotNil(user)
        XCTAssertEqual(user.name, "John")
    }
}
```

### Swift Testing Unit Tests

```swift
import Testing
@testable import MyApp

struct UserManagerTests {
    let userManager = UserManager()
    
    @Test func userCreation() {
        let user = userManager.createUser(name: "John", email: "john@example.com")
        #expect(user != nil)
        #expect(user?.name == "John")
    }
}
```

### UI Tests

UI tests verify that your app's user interface works correctly by simulating user interactions.

```swift
import XCTest

class MyAppUITests: XCTestCase {
    var app: XCUIApplication!
    
    override func setUp() {
        super.setUp()
        continueAfterFailure = false
        app = XCUIApplication()
        app.launch()
    }
    
    func testLoginFlow() {
        // Test the login flow
        let emailTextField = app.textFields["email"]
        emailTextField.tap()
        emailTextField.typeText("user@example.com")
        
        let passwordTextField = app.secureTextFields["password"]
        passwordTextField.tap()
        passwordTextField.typeText("password123")
        
        let loginButton = app.buttons["login"]
        loginButton.tap()
        
        // Verify login success
        XCTAssertTrue(app.staticTexts["Welcome"].exists)
    }
}
```

## Test Organization

### File Structure

Organize your tests logically:

```
MyAppTests/
├── UnitTests/
│   ├── Models/
│   │   ├── UserTests.swift
│   │   └── ProductTests.swift
│   ├── Services/
│   │   ├── NetworkServiceTests.swift
│   │   └── DataServiceTests.swift
│   └── ViewModels/
│       └── HomeViewModelTests.swift
├── UITests/
│   ├── LoginFlowTests.swift
│   ├── NavigationTests.swift
│   └── SettingsTests.swift
└── TestUtilities/
    ├── MockData.swift
    └── TestHelpers.swift
```

### Test Classes and Suites

#### XCTest Organization

```swift
class AuthenticationTests: XCTestCase {
    // Authentication-related tests
}

class DataModelTests: XCTestCase {
    // Data model tests
}

class NetworkTests: XCTestCase {
    // Network-related tests
}
```

#### Swift Testing Organization

```swift
struct AuthenticationTests {
    @Test func login() { }
    @Test func logout() { }
    @Test func passwordReset() { }
}

struct DataModelTests {
    @Test func userCreation() { }
    @Test func userValidation() { }
}

@Suite(.tags(.network))
struct NetworkTests {
    @Test func apiCall() { }
    @Test func errorHandling() { }
}
```

## Test Configuration

### Build Settings

Configure your test target's build settings:

1. **Deployment Target**: Match your app's deployment target
2. **Swift Version**: Use the same Swift version as your app
3. **Code Signing**: Configure for your development team
4. **Bundle Identifier**: Use a unique identifier (e.g., `com.yourcompany.MyAppTests`)

### Scheme Configuration

1. **Edit Scheme**: Product > Scheme > Edit Scheme
2. **Test Action**: Configure test settings
3. **Arguments**: Add launch arguments if needed
4. **Environment Variables**: Set up test environment

```swift
// Example of using launch arguments in tests
func testWithLaunchArguments() {
    let app = XCUIApplication()
    app.launchArguments = ["--uitesting", "--reset-data"]
    app.launch()
    
    // Test with specific configuration
}
```

## Test Data and Mocking

### Test Data Setup

```swift
struct TestData {
    static let sampleUser = User(
        id: "123",
        name: "Test User",
        email: "test@example.com"
    )
    
    static let sampleProducts = [
        Product(id: "1", name: "Product 1", price: 9.99),
        Product(id: "2", name: "Product 2", price: 19.99)
    ]
}
```

### Mock Objects

```swift
class MockNetworkService: NetworkServiceProtocol {
    var shouldSucceed = true
    var mockData: Data?
    var mockError: Error?
    
    func fetchData(completion: @escaping (Result<Data, Error>) -> Void) {
        if shouldSucceed {
            completion(.success(mockData ?? Data()))
        } else {
            completion(.failure(mockError ?? NetworkError.unknown))
        }
    }
}

// Using mocks in tests
func testDataFetching() {
    let mockService = MockNetworkService()
    mockService.shouldSucceed = true
    mockService.mockData = "test data".data(using: .utf8)
    
    let viewModel = MyViewModel(networkService: mockService)
    // Test with mock
}
```

## Async Testing

### XCTest Async Testing

```swift
func testAsyncOperation() async throws {
    let expectation = XCTestExpectation(description: "Async operation")
    
    Task {
        let result = await performAsyncOperation()
        XCTAssertNotNil(result)
        expectation.fulfill()
    }
    
    await fulfillment(of: [expectation], timeout: 5.0)
}
```

### Swift Testing Async

```swift
@Test func asyncOperation() async throws {
    let result = try await performAsyncOperation()
    #expect(result != nil)
}
```

## Performance Testing

### XCTest Performance Tests

```swift
func testPerformance() {
    measure(metrics: [XCTClockMetric(), XCTMemoryMetric()]) {
        // Code to measure
        let array = Array(0..<1000000)
        let sorted = array.sorted()
    }
}
```

### Custom Performance Metrics

```swift
func testCustomPerformance() {
    measure(metrics: [XCTClockMetric()]) {
        // Measure specific operation
        let result = expensiveOperation()
        XCTAssertNotNil(result)
    }
}
```

## Test Coverage

### Enabling Code Coverage

1. **Edit Scheme**: Product > Scheme > Edit Scheme
2. **Test Action**: Check "Gather coverage data"
3. **Run Tests**: Execute your test suite
4. **View Coverage**: Report Navigator > Coverage

### Coverage Analysis

```swift
// Example of testing edge cases for better coverage
func testEdgeCases() {
    // Test with empty input
    let emptyResult = processData([])
    XCTAssertTrue(emptyResult.isEmpty)
    
    // Test with nil input
    let nilResult = processData(nil)
    XCTAssertNil(nilResult)
    
    // Test with maximum values
    let maxResult = processData(Array(repeating: 1, count: 10000))
    XCTAssertNotNil(maxResult)
}
```

## Continuous Integration

### Xcode Cloud

Configure tests for Xcode Cloud:

1. **Create Workflow**: Add test action
2. **Configure Environment**: Set up test environment
3. **Add Test Plans**: Include test plans in workflow
4. **Monitor Results**: View test results in Xcode Cloud

### Command Line Testing

```bash
# Run all tests
xcodebuild test -scheme MyApp -destination 'platform=iOS Simulator,name=iPhone 15'

# Run specific test class
xcodebuild test -scheme MyApp -destination 'platform=iOS Simulator,name=iPhone 15' -only-testing:MyAppTests/UserManagerTests

# Run with coverage
xcodebuild test -scheme MyApp -destination 'platform=iOS Simulator,name=iPhone 15' -enableCodeCoverage YES
```

## Best Practices

### Test Naming

- **Descriptive Names**: Use clear, descriptive test names
- **Consistent Format**: Follow a consistent naming convention
- **Include Context**: Include what you're testing and expected outcome

```swift
// Good test names
func testUserCreationWithValidData() { }
func testUserCreationWithInvalidEmailThrowsError() { }
func testUserDeletionRemovesUserFromDatabase() { }

// Swift Testing
@Test func userCreationWithValidData() { }
@Test func userCreationWithInvalidEmailThrowsError() { }
@Test func userDeletionRemovesUserFromDatabase() { }
```

### Test Structure

Follow the Arrange-Act-Assert pattern:

```swift
func testUserValidation() {
    // Arrange
    let validEmail = "user@example.com"
    let validPassword = "SecurePassword123"
    
    // Act
    let isValid = UserValidator.validate(email: validEmail, password: validPassword)
    
    // Assert
    XCTAssertTrue(isValid)
}
```

### Test Independence

```swift
class IndependentTests: XCTestCase {
    override func setUp() {
        super.setUp()
        // Set up fresh state for each test
        UserDefaults.standard.removePersistentDomain(forName: Bundle.main.bundleIdentifier!)
    }
    
    override func tearDown() {
        // Clean up after each test
        super.tearDown()
    }
}
```

## Troubleshooting

### Common Issues

1. **Tests Not Running**
   - Check target membership
   - Verify build settings
   - Ensure proper imports

2. **Import Errors**
   - Use `@testable import` for internal types
   - Check module names
   - Verify target dependencies

3. **UI Test Failures**
   - Check element accessibility
   - Verify timing issues
   - Use proper waits

### Debugging Tests

```swift
func testWithDebugging() {
    print("Starting test...")
    
    // Add breakpoints
    let result = performOperation()
    
    print("Result: \(result)")
    XCTAssertNotNil(result)
}
```

## Migration from XCTest to Swift Testing

### Gradual Migration

You can run both XCTest and Swift Testing side by side:

```swift
// Keep existing XCTest
class LegacyTests: XCTestCase {
    func testLegacyFunctionality() { }
}

// Add new Swift Testing
struct NewTests {
    @Test func newFunctionality() { }
}
```

### Converting Tests

```swift
// XCTest
func testUserCreation() {
    let user = User(name: "John")
    XCTAssertEqual(user.name, "John")
}

// Swift Testing
@Test func userCreation() {
    let user = User(name: "John")
    #expect(user.name == "John")
}
```

## References

- [Apple Developer Documentation - Adding Tests](https://developer.apple.com/documentation/xcode/adding-tests-to-your-xcode-project)
- [XCTest Framework Documentation](https://developer.apple.com/documentation/xctest)
- [Swift Testing GitHub Repository](https://github.com/swiftlang/swift-testing)
- [WWDC Testing Sessions](https://developer.apple.com/videos/wwdc2024)

---

*This comprehensive guide covers all aspects of adding and configuring tests in your Xcode project, from basic setup to advanced testing strategies.*
--- END FILE ---

--- FILE: Meet_swift_testing.md ---
Hi! I’m Stuart Montgomery, and I’m thrilled to introduce Swift Testing to you. Quality and reliability are crucial for delivering a great user experience. Automated testing is a proven way to achieve and maintain software quality over time. That’s why this year we’re introducing a brand new set of tools which make testing Swift code easier and more powerful than ever. Meet Swift Testing, a new open source package for testing your code using Swift. It includes powerful capabilities for describing and organizing your tests; it gives actionable details when a failure occurs; and it scales to large codebases elegantly. Swift Testing was designed for Swift, embracing modern features like concurrency and macros. It supports all major platforms, including Linux and Windows. And it has an open development process, providing opportunities for you and the rest of the community to shape its evolution. In this session we’ll start by talking about the building blocks of Swift Testing, the core concepts you need to know. Then, we’ll discuss several common workflows, including ways to customize tests or repeat them with different arguments. We’ll cover how Swift Testing and XCTest relate to each other. And we’ll finish by talking about this new project’s role in the open source community. Let’s get started by taking a tour of the building blocks of Swift Testing. If you’ve never written tests for your app before, the first step is to add a test bundle target to your project. Choose File > New > Target.

Then search for Unit Testing Bundle in the Test section.

Swift Testing is now the default choice of testing system for this template in Xcode 16. Just choose a name for your new target and click Finish. This app already has a test target though, so let’s write our first test there. We'll start by importing the Testing module.

Then, we'll write a global function.

```swift
import Testing

@Test func videoMetadata() {
    // ...
}
```

And we'll add the @Test attribute to it.

The @Test attribute is the first building block. It indicates that a function is a test. Once we add that, Xcode recognizes it and shows a Run button alongside it. Test functions are just ordinary Swift functions that have the @Test attribute. They can be global functions or methods in a type. They can be marked async or throws, or be isolated to a global actor if needed. Next, let's make our test actually validate something by filling out the body of the function. We'll make this test check that the metadata for a video file are what we expect. We'll start by initializing the video we want to check and its expected metadata. Now, we're getting an error because these types are declared this app's module so we need to import that first.

```swift
import Testing
@testable import DestinationVideo

@Test func videoMetadata() {
    let video = Video(fileName: "By the Lake.mov")
    let expectedMetadata = Metadata(duration: .seconds(90))
    #expect(video.metadata == expectedMetadata)
}
```

Note that we use the lowercase @testable attribute on this import.

This is a general language feature, not part of Swift Testing, but it allows us to reference types like these whose access level is internal. Next, we'll use the #expect macro to check that the video metadata are correct.

The #expect macro performs an expectation, and this is the second building block of Swift Testing. You can use an expectation like the #expect macro to validate that an expected condition is true. It accepts ordinary expressions and language operators. And it captures the source code and the values of subexpressions if it fails. Let's run our test for the first time and see how it goes.

It looks like it failed, indicated by the red X icon. We can click the test failure message and choose Show to see more about what went wrong on this line.

This results view shows details about the expression that was passed to the #expect macro, including its sub-values. If we expand the metadata, we can compare their properties.

It looks like both the duration and resolution fields are non-equal. Looking at this gives me an idea: I think the Video type might not be loading the metadata after it's initialized. We can fix this by going to the Video initializer in a split editor, and ensure that property is assigned.

```swift
// In `Video.init(...)`
self.metadata = Metadata(forContentsOfUrl: url)
```

Now, let's re-run the test.

And it succeeded! Great. The #expect macro is really flexible. You can pass any expression, including operators or method calls, and it will show you detailed results if it fails. Here are a few examples. You can use the == operator, and the left and right hand sides will be captured and shown if there’s a failure. You can access properties like .isEmpty. And you can even call methods like .contains on an array. Notice how the error shows you the contents of the numbers array automatically. You don’t need to learn specialized APIs to do any of this — just use the #expect macro. Sometimes, you may want to end a test early if an expectation fails. For this, you can use the #require macro. Required expectations are similar to regular expectations. But they have the try keyword and throw an error if the expression is false, causing the test to fail and not proceed any further. Another way you can use the #require macro is to try unwrapping an optional value safely, and stop the test if it’s nil. This example shows using the #require macro to access the .first property of a collection, and afterward it checks a property on the element. The “first” property is optional, but the second line of our test relies on that value, so this test stops early because it doesn’t make sense to continue if the unwrapped value is nil. Required expectations are a great tool for this pattern. Before we commit this test to the project, let's make its purpose more clear. We can do that by passing a custom display name in the @Test attribute. That name will then be shown in the Test Navigator and other places in Xcode.

```swift
@Test("Check video metadata") func videoMetadata() {
    let video = Video(fileName: "By the Lake.mov")
    let expectedMetadata = Metadata(duration: .seconds(90))
    #expect(video.metadata == expectedMetadata)
}
```

A display name is an example of a trait, which is the third building block. Traits can do several things: they can add descriptive information about a test; they can customize when or whether a test runs; or they can modify how a test behaves.

Here are some examples. In addition to adding information with the display name, you can also reference related bugs or add custom tags. When you only want to run a test in certain conditions, you can use traits to control that. And some traits influence how a test actually behaves, such as imposing a time limit or executing one at a time. We've finished writing our first test, and now let's add a second one to validate another aspect of the Video type. This time, let's use the built-in test snippet in Xcode 16 to quickly add an empty test function.

Let's name this test rating.

```swift
@Test func rating() async throws {
    let video = Video(fileName: "By the Lake.mov")
    #expect(video.contentRating == "G")
}
```

And in the body, we'll create a video just like before, and #expect that its contentRating is the default value. It would be nice to group these two tests together so we can find them more easily in the project. We can do that by wrapping them into a struct, which we'll call VideoTests.

```swift
struct VideoTests {
    @Test("Check video metadata") func videoMetadata() {
        let video = Video(fileName: "By the Lake.mov")
        let expectedMetadata = Metadata(duration: .seconds(90))
        #expect(video.metadata == expectedMetadata)
    }

    @Test func rating() async throws {
        let video = Video(fileName: "By the Lake.mov")
        #expect(video.contentRating == "G")
    }
}
```

As soon as we do that, the hierarchy is reflected in the Test Navigator, and we can even click to run them as a group. A type like this which contains tests is called a test suite, and that's the fourth and final building block. Suites are used to group related test functions or other suites. They can be annotated explicitly using the @Suite attribute, although any type which contains @Test functions or @Suites is considered a @Suite itself, implicitly. Suites can have stored instance properties. They can use init or deinit to perform logic before or after each test. And a separate @Suite instance is created for every instance @Test function it contains to avoid unintentional state sharing. These two tests start the same way: their first lines of code for creating a video are identical. Now that these tests are in a suite, we can reduce repetition by factoring out that line into a stored property like this.

```swift
struct VideoTests {
    let video = Video(fileName: "By the Lake.mov")

    @Test("Check video metadata") func videoMetadata() {
        let expectedMetadata = Metadata(duration: .seconds(90))
        #expect(video.metadata == expectedMetadata)
    }

    @Test func rating() async throws {
        #expect(video.contentRating == "G")
    }
}
```

And now we can delete that line from the second test.

Since each test function is called on a new instance of its containing suite type, each one will get its own video instance and they can never share state accidentally. So let’s review the building blocks. We talked about test functions, expectations, traits, and suites. They were designed to feel right at home in Swift, in several ways: Test functions integrate seamlessly with Swift concurrency by supporting async/await and actor isolation. Expectations can use async/await too, and they accept all the built-in language operators. Both expectations and traits leverage Swift macros, allowing you to see detailed failure results and specify per-test information directly in code. And suites embrace value semantics, encouraging the use of structs to isolate state.

Let's now apply those building blocks to some common problems in testing and discuss workflows for addressing them. We'll discuss controlling when tests run; associating tests which have things in common; and repeating tests more than once with different arguments each time. First, tests with conditions. Some tests should only be run in certain circumstances — such as on specific devices or environments. For those, you can apply a condition trait such as .enabled(if: ...). You pass it a condition to be evaluated before the test runs, and if the condition is false, the test will be marked as skipped.

```swift
@Test(.enabled(if: AppFeatures.isCommentingEnabled))
func videoCommenting() {
    // ...
}
```

Other times, you might want a test to never run. For this, you can use the .disabled(...) trait. Disabling a test is preferable over other techniques, like commenting out the test function, since it verifies the code inside the test still compiled. The .disabled(...) trait accepts a comment, which you can use to explain the reason why the test is disabled. And comments always appear in the structured results, so they can be shown in your CI system for visibility.

```swift
@Test(.disabled("Due to a known crash"))
func example() {
    // ...
}
```

Oftentimes, the reason a test is disabled is because of an issue which is tracked in a bug-tracking system. In addition to a comment, you can include a .bug(...) trait along with any other trait to reference related issues with a URL. Then, you can see that bug trait in the Test Report in Xcode 16 and click to open its URL.

```swift
@Test(.disabled("Due to a known crash"),
     .bug("example.org/bugs/1234", "Program crashes at <symbol>"))
func example() {
    // ...
}
```

When the entire body of a test can only run on certain OS versions, you can place the @available(...) attribute on that test to control which versions it will run on. Use the @available(...) attribute rather than checking at runtime using #available. The @available(...) attribute allows the testing library to know that a test has an OS version condition, so this can be reflected in the results more accurately.

```swift
@Test
@available(macOS 15, *)
func usesNewAPIs() {
    // ...
}
```

**✗ Avoid checking availability at runtime using #available**
```swift
@Test func hasRuntimeVersionCheck() {
    guard #available(macOS 15, *) else { return }
    // ...
}
```

**✓ Prefer @available attribute on test function**
```swift
@Test
@available(macOS 15, *)
func usesNewAPIs() {
    // ...
}
```

Next, let’s talk about how you can associate tests which have characteristics in common, even if they’re in different suites or files. Swift Testing supports assigning custom tags to tests. I've already begun using tags in this project. The Test Navigator shows all the tags at the bottom. To view the tests which each of these tags have been applied to, we can switch to the new Group By: Tag mode.

Let's apply a tag to one of the tests we wrote earlier. To do this, we'll add a tags trait to the test via the @Test attribute. This test is validating some data formatting logic. There's already another test in this project which relates to formatting, so let's add the formatting tag to this test too.

```swift
@Test(.tags(.formatting)) func rating() async throws {
    #expect(video.contentRating == "G")
}
```

Once we do that, it shows in the Test Navigator under that tag.

I wrote another test which also validates data formatting, which I'll add here.

```swift
@Test(.tags(.formatting)) func formattedDuration() async throws {
    let videoLibrary = try await VideoLibrary()
    let video = try #require(await videoLibrary.video(named: "By the Lake"))
    #expect(video.formattedDuration == "0m 19s")
}
```

Since these two tests are about the formatting of Video information, let's group them into a sub-suite.

```swift
struct MetadataPresentation {
    let video = Video(fileName: "By the Lake.mov")

    @Test(.tags(.formatting)) func rating() async throws {
        #expect(video.contentRating == "G")
    }

    @Test(.tags(.formatting)) func formattedDuration() async throws {
        let videoLibrary = try await VideoLibrary()
        let video = try #require(await videoLibrary.video(named: "By the Lake"))
        #expect(video.formattedDuration == "0m 19s")
    }
}
```

Now, we can move the formatting tag up to the @Suite, so it will be inherited by all the tests it contains. Finally, we can delete the tags from each individual @Test function, since they're now inherited.

```swift
@Suite(.tags(.formatting))
struct MetadataPresentation {
    let video = Video(fileName: "By the Lake.mov")

    @Test func rating() async throws {
        #expect(video.contentRating == "G")
    }

    @Test func formattedDuration() async throws {
        let videoLibrary = try await VideoLibrary()
        let video = try #require(await videoLibrary.video(named: "By the Lake"))
        #expect(video.formattedDuration == "0m 19s")
    }
}
```

You can associate tags with tests which have things in common. As an example, you might apply a common tag to all the tests which validate a particular feature or subsystem. This lets you run all the tests with a particular tag. It also lets you filter them in the Test Report, and even see insights there such as when multiple tests with the same tag begin failing. Tags themselves can be applied to tests in different files, suites, or targets. They can even be shared among multiple projects.

When using Swift Testing, prefer tags over specific names of tests when including or excluding them from a test plan. For best results, always use the most appropriate type of trait for each circumstance. Not every scenario should use tags. For example, if you’re trying to express a runtime condition, use .enabled(if ...) as we discussed earlier.

To learn more about using test tags in Xcode, see "Go further with Swift Testing".

The last workflow I'd like to show is my favorite. Repeating tests with different arguments each time. Here's an example of why that can be useful. In this project there are several tests which check the number of continents that various videos mention. Each of them follows a similar pattern: it creates a fresh videoLibrary, looks up a video by name, and then uses the #expect macro to check how many continents it mentions.

```swift
struct VideoContinentsTests {
    @Test func mentionsFor_A_Beach() async throws {
        let videoLibrary = try await VideoLibrary()
        let video = try #require(await videoLibrary.video(named: "A Beach"))
        #expect(!video.mentionedContinents.isEmpty)
        #expect(video.mentionedContinents.count <= 3)
    }

    @Test func mentionsFor_By_the_Lake() async throws {
        let videoLibrary = try await VideoLibrary()
        let video = try #require(await videoLibrary.video(named: "By the Lake"))
        #expect(!video.mentionedContinents.isEmpty)
        #expect(video.mentionedContinents.count <= 3)
    }

    @Test func mentionsFor_Camping_in_the_Woods() async throws {
        let videoLibrary = try await VideoLibrary()
        let video = try #require(await videoLibrary.video(named: "Camping in the Woods"))
        #expect(!video.mentionedContinents.isEmpty)
        #expect(video.mentionedContinents.count <= 3)
    }

    // ...and more, similar test functions
}
```

These tests work, but they're very repetitive and the more videos we add a test for, the harder it will be to maintain them due to all the duplicated code. Also, when using this pattern we're forced to give each test a unique function name, but these names are hard to read and they might get out-of-sync with the name of the video they're testing. Instead, we can write all of these as a single test using a feature called parameterized testing. Let's transform this first test into a parameterized one. The first step is to add a parameter to its signature.

As soon as we do this, an error appears telling us that we must specify the arguments to pass to this test, so let’s fix that.

For now, let's include the names of just three videos.

```swift
struct VideoContinentsTests {
    @Test("Number of mentioned continents", arguments: [
        "A Beach",
        "By the Lake",
        "Camping in the Woods",
        "The Rolling Hills",
        "Ocean Breeze",
        "Patagonia Lake",
        "Scotland Coast",
        "China Paddy Field",
    ])
    func mentionedContinentCounts(videoName: String) async throws {
        let videoLibrary = try await VideoLibrary()
        let video = try #require(await videoLibrary.video(named: videoName))
        #expect(!video.mentionedContinents.isEmpty)
        #expect(video.mentionedContinents.count <= 3)
    }
}
```

I like to split arguments over multiple lines so they're easier to read, but you can format them however you like. The last step is to replace the name of the video being looked up with the passed-in argument.

Since this test now covers multiple videos, let’s generalize its name.

The full name of this test now includes its parameter label.

But we can still give it a display name or other traits if we want, by passing them before the arguments.

Now let's run the test and see how it goes.

Great! It succeeded, and the Test Navigator shows each individual video below it as if it were a separate test. This structure makes it really easy to add more arguments and expand test coverage. Let’s add all the remaining videos to this list — and even a couple new ones.

At this point, we can delete the old @Test functions since they're no longer necessary.

Let's run the test one more time and make sure it's still passing.

Hm, it looks like one of the new videos we added near the end is causing a failure now. By clicking the argument, we can see details about it, and the expectation which failed. To investigate this problem, it would help to re-run it with the debugger, but I'd prefer to only re-run the argument that failed to save some time. In Xcode 16 we can now run an individual argument by clicking its run button in the Test Navigator. But before we do this, let’s add a breakpoint to the beginning of the test.

And now let’s re-run it.

The videoName shown in the debugger is "Scotland Coast”, so we know we’re running this test with exactly the argument we're interested in. From here, we could continue debugging further and identify the reason for the failure. Conceptually, a parameterized test is similar to a single test that is repeated multiple times using a for…in loop. Here's an example: it has an array of videoNames that it loops over to perform the test. However, using a for...in loop like this has some downsides.

```swift
// Using a for...in loop to repeat a test (not recommended)
@Test func mentionedContinentCounts() async throws {
    let videoNames = [
        "A Beach",
        "By the Lake",
        "Camping in the Woods",
    ]
    
    let videoLibrary = try await VideoLibrary()
    for videoName in videoNames {
        let video = try #require(await videoLibrary.video(named: videoName))
        #expect(!video.mentionedContinents.isEmpty)
        #expect(video.mentionedContinents.count <= 3)
    }
}
```

Parameterized testing allows you see the details of each individual argument clearly in the results. The arguments can be re-run independently for fine-grained debugging. And they can be executed more efficiently by running each argument in parallel, so you can get results more quickly.

Parameterized tests can be used in even more advanced ways than we saw here, such as by testing all combinations of two sets of inputs. Check out "Go further with Swift Testing" to learn more.

Whenever you see a test using this pattern, it's best to transform it into a parameterized test function. Just add a parameter to the function, get rid of the for...in loop, move the arguments up to the @Test attribute, and you're done!

```swift
@Test(arguments: [
    "A Beach",
    "By the Lake",
    "Camping in the Woods",
])
func mentionedContinentCounts(videoName: String) async throws {
    let videoLibrary = try await VideoLibrary()
    let video = try #require(await videoLibrary.video(named: videoName))
    #expect(!video.mentionedContinents.isEmpty)
    #expect(video.mentionedContinents.count <= 3)
}
```

Next, let's talk about how Swift Testing and XCTest relate to one another. If you've already written some XCTests, you might be wondering how this new testing system compares, or how to migrate your tests. Swift Testing has some similarities to XCTest, but it also some important differences to be aware of. Let's compare three of the building blocks from earlier, test functions, expectations, and suites.

Tests in XCTest are any method whose name begins with “test”. But Swift Testing uses the @Test attribute to denote them explicitly, so there’s no ambiguity.

Swift Testing supports more kinds of functions, so you can still use instance methods in a type, but also static or global functions if you prefer. Unlike XCTest, Swift Testing supports traits for specifying information either per-test or per-suite. And Swift Testing takes a different approach to parallelization: it runs in-process using Swift Concurrency, and supports physical devices like iPhone and Apple Watch.

Expectations are very different between these two systems. XCTest refers to this concept as assertions, and it uses many functions beginning with XCTAssert to denote them. Swift Testing takes a different approach: it has just two basic macros — #expect and #require. Instead of needing many specialized functions, you can pass in ordinary expressions and language operators to #expect or #require. For example, you can use double-equals to check equality, or the greater-than operator to compare two values.

And you can easily use the opposite operator to negate any expectation.

Halting a test after a test failure occurs is also handled differently. In XCTest, you assign the continueAfterFailure property to false, and then any subsequent assertion which fails will cause the test to halt. In Swift Testing, any expectation can be made into a required one by using #require instead of #expect, and it will throw an error upon failure. This lets you choose which expectations should halt the test, and even alternate as the test progresses.

When it comes to suite types, XCTest only supports classes and they must inherit from XCTestCase. In Swift Testing, you can use a struct, actor, or class, and a struct is encouraged since it uses value semantics and avoids bugs due to unintentional state sharing.

Suites may be denoted explicitly by the @Suite attribute, although it’s implicit for any type which contains test functions or nested suites. It is only required when specifying a display name or other trait.

To perform logic before each test runs, XCTest offers several setUp methods, but Swift Testing uses the type’s initializer for this purpose, and it can be async or throws.

If you need to perform logic after each test, you can include a de-initializer. Deinitializers can only be used when the suite type is an actor or class, and that’s the most common reason to use a reference type instead of a struct for a suite.

Finally, in Swift Testing, you can group tests into subgroups via nested types.

XCTest and Swift Testing tests can co-exist in a single target, so if you choose to migrate, you can do so incrementally and you don’t need to create a new target first. When migrating multiple XCTest methods which have a similar structure, you can consolidate them into one parameterized test as we discussed earlier. For any XCTest classes with only one test method, consider migrating them to a global @Test function. And when naming tests, the word “test” is no longer necessary at the beginning.

Please continue using XCTest for any tests which use UI automation APIs like XCUIApplication or use performance testing APIs like XCTMetric as these are not supported in Swift Testing. You must also use XCTest for any tests which can only be written in Objective-C. You can use Swift Testing to write tests in Swift that validate code written in another language, however. Finally, avoid calling XCTest assertion functions from Swift Testing tests, or the opposite — the #expect macro from XCTests.

Check out “Migrating a test from XCTest” in our documentation. It has lots of details about how to translate assertions, handle asynchronous waiting conditions, and more.

We’ve gone over the features of Swift Testing and shown several ways to use it. This is just the beginning for this new package, and I’m so excited that it will continue to evolve in the community. Swift Testing is open source and hosted on GitHub. Soon it will transition to the recently announced swiftlang organization. It works on all Apple operating systems which support Swift Concurrency, as well as on Linux and Windows. And importantly, it has a common codebase across all these platforms! This is a significant improvement over XCTest which had multiple implementations. It means your tests behave much more consistently when moving between platforms, and there will be better functional parity between them.

Swift Testing is integrated into major tools and IDEs in the Swift ecosystem, including Swift Package Manager on the command-line, as well as Xcode 16 and VS Code with recent versions of the Swift extension. Let's take a look at Swift Testing's command-line experience. Here's a simple package I created using the New Package template included in Xcode 16.

```swift
import Testing
@testable import MyLibrary

@Test func example() throws {
    #expect("abc" == "abc")
}

@Test func failingExample() throws {
    #expect(123 == 456)
}
```

We can run the tests for this package from the Terminal by typing swift test.

```bash
> swift test
```

This causes both XCTest and Swift Testing tests to run. The console shows pass and fail results using colorful output, and includes detailed failure messages similar to the ones shown in Xcode. Swift Testing has an open feature proposal process and we discuss its evolution on the Swift Forums. We invite you to get involved by writing or participating in feature proposals, improving the documentation, or even filing bug reports. All contributions are welcome! So that's Swift Testing. Use its powerful features like expectations and parameterized testing to improve the quality of your code; customize your tests using traits; and join us on GitHub and the Forums to shape this package's future. Don't forget to check out "Go further with Swift Testing" to learn even more ways you can improve your tests. Thank you so much for watching!
--- END FILE ---

--- FILE: Resources.md ---
# Resources

**Access assets and other data bundled with your app.**


## Topics

### Bundle Resources

- [Bundle](https://developer.apple.com/documentation/foundation/bundle) — A representation of the code and resources stored in a bundle directory on disk.
- [bundle()](https://developer.apple.com/documentation/foundation/bundle()) — Returns the bundle most likely to contain resources for the calling code.
### On-Demand Resources

- [NSBundleResourceRequest](https://developer.apple.com/documentation/foundation/nsbundleresourcerequest) — A resource manager you use to download content hosted on the App Store at the time your app needs it.

---

*Source: [https://developer.apple.com/documentation/com.apple.foundation/documentation/Foundation/resources](https://developer.apple.com/documentation/com.apple.foundation/documentation/Foundation/resources)*
--- END FILE ---
=== END SWIFT DOCUMENTATION ===
=== END CONTEXT ===

=== BUILD ATTEMPT 4 FAILED ===
The previous implementation failed with the following errors:

copilot CLI invocation failed. Please try again.

=== INSTRUCTIONS FOR ATTEMPT 5 ===
1. Analyze what went wrong in the previous approach
2. Try a DIFFERENT approach or fix the specific issues
3. Do NOT repeat the same mistakes
4. If the same approach keeps failing, consider an alternative implementation strategy
5. Write the corrected code directly - do not explain, just implement

Please fix the issues and provide the corrected implementation.